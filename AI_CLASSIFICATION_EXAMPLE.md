# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

## üìù –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from app.services.ai_document_classifier import classify_document_with_ai

# –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
doc_type = await classify_document_with_ai(
    text_content="–§–ï–î–ï–†–ê–õ–¨–ù–´–ô –ó–ê–ö–û–ù –æ—Ç 01.05.2019 N 51-–§–ó...",
    file_name="law.pdf",
    document_id="doc_123"
)
# –í–µ—Ä–Ω–µ—Ç: "federal_law"
```

## üéØ –î–µ—Ç–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

```python
from app.services.ai_document_classifier import ai_document_classifier

result = await ai_document_classifier.classify_document_ai(
    text_content="–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ü–ª–µ–Ω—É–º–∞ –í–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ –°—É–¥–∞ –†–§...",
    file_name="resolution.pdf"
)

print(result)
# {
#     "type": "supreme_court_resolution",
#     "confidence": 0.95,
#     "reason": "–≠—Ç–æ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ü–ª–µ–Ω—É–º–∞ –í–° –†–§",
#     "method": "ai"
# }
```

## üîÑ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ vector_store_service

```python
# –í vector_store_service.py, –º–µ—Ç–æ–¥ add_document:

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
if "document_type" not in sanitized_metadata:
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–∞–≤–∏–ª–∞ (–±—ã—Å—Ç—Ä–æ)
    file_name = sanitized_metadata.get("file_name", "")
    doc_type = determine_document_type(file_name, document_id, content)
    
    # –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º AI
    if doc_type == 'other' and self.use_ai_classification:
        from .ai_document_classifier import classify_document_with_ai
        doc_type = await classify_document_with_ai(
            text_content=content,
            file_name=file_name,
            document_id=document_id
        )
    
    sanitized_metadata["document_type"] = doc_type
```

## ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def get_cached_type(text_hash: str) -> str:
    """–ö—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    # ...

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
text_hash = hashlib.md5(text_content[:500].encode()).hexdigest()
doc_type = get_cached_type(text_hash)
```

### –ë–∞—Ç—á–∏–Ω–≥ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏

```python
# –í load_documents_optimized.py

# –°–æ–±–∏—Ä–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
uncertain_docs = []
for chunk in chunks:
    rule_type = determine_document_type(...)
    if rule_type == 'other':
        uncertain_docs.append({
            'text': chunk.get('text', ''),
            'file_name': file_name,
            'document_id': document_id
        })

# –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –±–∞—Ç—á–µ–º
if uncertain_docs:
    ai_results = await ai_document_classifier.classify_batch(uncertain_docs)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤

| –°–∏—Ç—É–∞—Ü–∏—è | Rule-Based | AI-Based | –ì–∏–±—Ä–∏–¥–Ω—ã–π |
|----------|-----------|----------|-----------|
| –ö–æ–¥–µ–∫—Å—ã | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 100% |
| –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã | ‚úÖ 90% | ‚úÖ 95% | ‚úÖ 92% |
| –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã | ‚ùå 60% | ‚úÖ 95% | ‚úÖ 90% |
| –°–∫–æ—Ä–æ—Å—Ç—å | ‚ö° 1 –º—Å | üêå 500-2000 –º—Å | ‚ö° 1-500 –º—Å |

## üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞

```python
# –û—Ç–∫–ª—é—á–∏—Ç—å AI (—Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª–∞)
ai_document_classifier.use_ai = False

# –í–∫–ª—é—á–∏—Ç—å AI –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
ai_document_classifier.use_ai = True
```

