#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB
- Batch-–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ (–±—ã—Å—Ç—Ä–æ)
- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è)
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
"""

import sys
import json
from pathlib import Path

sys.path.insert(0, 'backend')
from app.services.vector_store_service import vector_store_service, determine_document_type

print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB...")
vector_store_service.initialize()

if not vector_store_service.is_ready():
    print("‚ùå ChromaDB –Ω–µ –≥–æ—Ç–æ–≤")
    exit(1)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
processed_dir = Path('/root/advakod/unified_codexes/rag_integration/processed_documents')
processed_files = sorted(list(processed_dir.glob('*.json')))

print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(processed_files)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á–∏–Ω–≥–∞
BATCH_SIZE = 500  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
added_total = 0
doc_types_count = {}

for i, processed_file in enumerate(processed_files, 1):
    try:
        with open(processed_file, 'r', encoding='utf-8') as f:
            doc_data = json.load(f)
        
        chunks = doc_data.get('chunks', [])
        document_id = doc_data.get('document_id', '')
        file_name = doc_data.get('metadata', {}).get('file_name', 'unknown')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_type = determine_document_type(
            file_name=file_name,
            document_id=document_id,
            text_content=chunks[0].get('text', '') if chunks else ''
        )
        doc_types_count[doc_type] = doc_types_count.get(doc_type, 0) + 1
        
        print(f"[{i}/{len(processed_files)}] {file_name}")
        print(f"   –¢–∏–ø: {doc_type}, –ß–∞–Ω–∫–æ–≤: {len(chunks)}", end=' ... ')
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –±–∞—Ç—á–∏
        batch_documents = []
        batch_metadatas = []
        batch_ids = []
        
        for chunk in chunks:
            chunk_metadata = {
                **chunk.get('metadata', {}),
                'document_id': document_id,
                'document_type': doc_type,  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                'added_at': chunk.get('metadata', {}).get('processing_timestamp', '')
            }
            
            batch_documents.append(chunk.get('text', ''))
            batch_metadatas.append(chunk_metadata)
            batch_ids.append(chunk.get('id', ''))
            
            # –ö–æ–≥–¥–∞ –±–∞—Ç—á –∑–∞–ø–æ–ª–Ω–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
            if len(batch_documents) >= BATCH_SIZE:
                try:
                    vector_store_service.collection.add(
                        documents=batch_documents,
                        metadatas=batch_metadatas,
                        ids=batch_ids
                    )
                    added_total += len(batch_documents)
                    print(f"‚úÖ +{len(batch_documents)}", end=' ', flush=True)
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞: {e}")
                
                # –û—á–∏—â–∞–µ–º –±–∞—Ç—á
                batch_documents = []
                batch_metadatas = []
                batch_ids = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —á–∞–Ω–∫–∏
        if batch_documents:
            try:
                vector_store_service.collection.add(
                    documents=batch_documents,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                added_total += len(batch_documents)
                print(f"‚úÖ +{len(batch_documents)}", end=' ', flush=True)
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±–∞—Ç—á–∞: {e}")
        
        print(f" ‚úÖ –í—Å–µ–≥–æ: {added_total}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {processed_file.name}: {e}")
        import traceback
        traceback.print_exc()

print(f"\n{'='*60}")
print(f"‚úÖ –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {added_total:,} —á–∞–Ω–∫–æ–≤")
count = vector_store_service.collection.count()
print(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB: {count:,}")

print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
for doc_type, count in sorted(doc_types_count.items()):
    print(f"   {doc_type}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

print(f"{'='*60}")

