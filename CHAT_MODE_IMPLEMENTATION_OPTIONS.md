# Варианты реализации режимов чата (Базовый/Эксперт)

## Анализ текущей архитектуры

### Текущее состояние:
1. **Frontend**: UI для выбора режима уже реализован в `Chat.js` (строки 32-33, 890-936)
   - Состояние `chatMode` хранится локально
   - TODO заглушка для логики переключения режима
   
2. **Backend**: 
   - Единый промпт в `unified_llm_service.create_legal_prompt()` (строка 593)
   - Параметры генерации: `temperature=0.3`, `top_p=0.8`, `max_tokens=4000`
   - Стриминг работает через `generate_response()` с `stream=True`
   - Кеширование ответов для ускорения

3. **База данных**:
   - `ChatSession` модель не содержит поле для режима
   - Можно добавить поле `chat_mode` (String, default='basic')

---

## Варианты реализации

### ВАРИАНТ 1: Параметр в промпте (Самый простой, без изменений БД)

**Суть**: Передавать режим в каждом запросе и модифицировать только промпт.

**Преимущества**:
- ✅ Минимальные изменения кода
- ✅ Нет миграций БД
- ✅ Можно переключать режим в любой момент
- ✅ Не влияет на скорость (промпт формируется быстро)
- ✅ Работает с кешированием (разные режимы = разные кеши)

**Недостатки**:
- ⚠️ Режим не сохраняется между сессиями
- ⚠️ Нужно передавать режим в каждом запросе

**Реализация**:
```python
# В ChatRequest добавить поле:
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[int] = None
    chat_mode: Optional[str] = "basic"  # "basic" или "expert"
    
# В create_legal_prompt добавить параметр:
def create_legal_prompt(self, question: str, context: Optional[str] = None, 
                        chat_mode: str = "basic") -> str:
    if chat_mode == "expert":
        prompt = f"""Ты опытный юрист-консультант по российскому законодательству. 
        Отвечай профессионально, используя юридические термины и ссылки на конкретные нормы права.
        Вопрос: {question}
        Ответ:"""
    else:  # basic
        prompt = f"""Ты юрист-консультант по российскому законодательству. 
        Отвечай простым и понятным языком, избегая сложных юридических терминов. 
        Объясняй как обычному человеку.
        Вопрос: {question}
        Ответ:"""
```

**Оценка скорости**: ⚡⚡⚡⚡⚡ (5/5) - не влияет на скорость

---

### ВАРИАНТ 2: Режим в сессии (Рекомендуемый)

**Суть**: Хранить режим в `ChatSession`, использовать при генерации.

**Преимущества**:
- ✅ Режим сохраняется для всей сессии
- ✅ Удобно для пользователя (не нужно выбирать каждый раз)
- ✅ Можно анализировать использование режимов
- ✅ Не влияет на скорость генерации
- ✅ Работает с кешированием

**Недостатки**:
- ⚠️ Нужна миграция БД (добавить поле `chat_mode`)
- ⚠️ Нужно обновлять API для изменения режима

**Реализация**:
```python
# 1. Миграция БД - добавить поле в ChatSession:
chat_mode = Column(String(20), nullable=False, default='basic')

# 2. В ChatRequest добавить опциональное поле для установки режима:
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[int] = None
    set_chat_mode: Optional[str] = None  # Для изменения режима

# 3. В chat.py использовать режим из сессии:
session = db.query(ChatSession).filter(...).first()
chat_mode = session.chat_mode if hasattr(session, 'chat_mode') else 'basic'

# 4. Передавать в create_legal_prompt:
prompt = unified_llm_service.create_legal_prompt(
    question=chat_request.message,
    context=chat_history,
    chat_mode=chat_mode
)
```

**Оценка скорости**: ⚡⚡⚡⚡⚡ (5/5) - не влияет на скорость

---

### ВАРИАНТ 3: Разные параметры генерации

**Суть**: Использовать разные `temperature` и `top_p` для разных режимов.

**Преимущества**:
- ✅ Базовый режим может быть более детерминированным (ниже temperature)
- ✅ Эксперт режим может быть более креативным (выше temperature)
- ✅ Не требует изменений в промпте

**Недостатки**:
- ⚠️ Может влиять на скорость (но минимально)
- ⚠️ Нужно тестировать оптимальные значения

**Реализация**:
```python
# В chat.py:
if chat_mode == "expert":
    temperature = 0.5  # Более креативные ответы
    top_p = 0.9
    max_tokens = 5000  # Более длинные ответы
else:  # basic
    temperature = 0.3  # Более детерминированные ответы
    top_p = 0.8
    max_tokens = 3000  # Более короткие ответы

response_text = await unified_llm_service._generate_response_internal(
    prompt=prompt,
    max_tokens=max_tokens,
    temperature=temperature,
    top_p=top_p
)
```

**Оценка скорости**: ⚡⚡⚡⚡ (4/5) - минимальное влияние на скорость

---

### ВАРИАНТ 4: Комбинированный подход (Оптимальный)

**Суть**: Комбинация вариантов 2 и 3 + улучшенные промпты.

**Преимущества**:
- ✅ Максимальная гибкость
- ✅ Режим сохраняется в сессии
- ✅ Разные промпты для разных режимов
- ✅ Оптимизированные параметры генерации
- ✅ Полный контроль над поведением

**Недостатки**:
- ⚠️ Больше кода для поддержки
- ⚠️ Нужна миграция БД

**Реализация**:
```python
# 1. Миграция БД
chat_mode = Column(String(20), nullable=False, default='basic')

# 2. Улучшенные промпты в unified_llm_service:
def create_legal_prompt(self, question: str, context: Optional[str] = None, 
                        chat_mode: str = "basic") -> str:
    if chat_mode == "expert":
        system_prompt = """Ты опытный юрист-консультант по российскому законодательству.
        Отвечай профессионально, используя:
        - Юридические термины и понятия
        - Ссылки на конкретные статьи законов (ГК РФ, ТК РФ, УК РФ и т.д.)
        - Номера статей и пункты
        - Судебную практику при необходимости
        - Профессиональную терминологию"""
    else:  # basic
        system_prompt = """Ты юрист-консультант по российскому законодательству.
        Отвечай простым и понятным языком:
        - Избегай сложных юридических терминов
        - Объясняй как обычному человеку
        - Используй примеры из жизни
        - Пиши короткими предложениями"""
    
    prompt = f"""{system_prompt}

Вопрос: {question}

Ответ:"""
    
    if context:
        context_section = f"\n\n=== ДОПОЛНИТЕЛЬНЫЙ КОНТЕКСТ ===\n{context}\n=== КОНЕЦ КОНТЕКСТА ===\n\n"
        prompt = prompt.replace(f"Вопрос: {question}", f"{context_section}Вопрос: {question}")
    
    return prompt

# 3. Параметры генерации в chat.py:
CHAT_MODE_CONFIG = {
    "basic": {
        "temperature": 0.3,
        "top_p": 0.8,
        "max_tokens": 3000,
        "description": "Простой и понятный режим"
    },
    "expert": {
        "temperature": 0.5,
        "top_p": 0.9,
        "max_tokens": 5000,
        "description": "Профессиональный режим с терминологией"
    }
}

config = CHAT_MODE_CONFIG.get(chat_mode, CHAT_MODE_CONFIG["basic"])
response_text = await unified_llm_service._generate_response_internal(
    prompt=prompt,
    max_tokens=config["max_tokens"],
    temperature=config["temperature"],
    top_p=config["top_p"]
)
```

**Оценка скорости**: ⚡⚡⚡⚡⚡ (5/5) - не влияет на скорость

---

### ВАРИАНТ 5: Кеширование с учетом режима

**Суть**: Раздельное кеширование для разных режимов.

**Преимущества**:
- ✅ Быстрые ответы для часто задаваемых вопросов в обоих режимах
- ✅ Не смешивает ответы базового и экспертного режимов

**Реализация**:
```python
# В chat.py при кешировании:
cache_key = f"{chat_mode}:{normalized_question}"
cached_response = await ChatCache.get_cached_ai_response(cache_key)
```

**Оценка скорости**: ⚡⚡⚡⚡⚡ (5/5) - ускоряет повторные запросы

---

## Сравнительная таблица

| Критерий | Вариант 1 | Вариант 2 | Вариант 3 | Вариант 4 | Вариант 5 |
|----------|-----------|-----------|-----------|-----------|-----------|
| Скорость генерации | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ |
| Скорость стриминга | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ | ⚡⚡⚡⚡⚡ |
| Сложность реализации | ⭐ | ⭐⭐ | ⭐ | ⭐⭐⭐ | ⭐⭐ |
| Сохранение режима | ❌ | ✅ | ✅ | ✅ | ✅ |
| Гибкость | ⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| Требует миграцию БД | ❌ | ✅ | ✅ | ✅ | ❌ |

---

## Рекомендация

**Рекомендуется ВАРИАНТ 4 (Комбинированный подход)** по следующим причинам:

1. ✅ **Максимальная гибкость** - можно настраивать промпты и параметры отдельно
2. ✅ **Сохранение режима** - пользователь не теряет выбор при перезагрузке
3. ✅ **Не влияет на скорость** - все изменения на уровне промпта и параметров
4. ✅ **Работает со стримингом** - режим передается в промпт до начала генерации
5. ✅ **Кеширование** - можно раздельно кешировать ответы для разных режимов
6. ✅ **Расширяемость** - легко добавить новые режимы в будущем

### План реализации (Вариант 4):

1. **Миграция БД**: Добавить поле `chat_mode` в `ChatSession`
2. **Backend API**: 
   - Обновить `ChatRequest` для установки режима
   - Обновить `create_legal_prompt` для поддержки режимов
   - Добавить конфигурацию параметров генерации
3. **Frontend**: 
   - Отправлять режим в API при создании/изменении сессии
   - Сохранять выбранный режим в состоянии
4. **Тестирование**: Проверить скорость генерации и стриминг

---

## Влияние на производительность

### Анализ узких мест:

1. **Формирование промпта**: 
   - Текущее время: ~0.001-0.01 сек
   - С режимами: ~0.001-0.01 сек (без изменений)
   - ✅ Не влияет

2. **Генерация ответа**:
   - Текущее время: зависит от длины ответа
   - С режимами: то же самое (параметры не влияют на скорость первого токена)
   - ✅ Не влияет

3. **Стриминг**:
   - Текущее время: первый токен ~0.5-2 сек
   - С режимами: то же самое (режим определяется до начала генерации)
   - ✅ Не влияет

4. **Кеширование**:
   - Текущее время: проверка кеша ~0.001 сек
   - С режимами: то же самое (ключ кеша включает режим)
   - ✅ Не влияет

**Вывод**: Реализация режимов НЕ влияет на скорость генерации и стриминг, так как:
- Режим определяется ДО начала генерации
- Изменения только в промпте (формируется быстро)
- Параметры генерации не влияют на скорость первого токена
- Кеширование работает независимо для каждого режима

---

## Дополнительные оптимизации

1. **Предзагрузка промптов**: Можно кешировать шаблоны промптов
2. **Асинхронная установка режима**: Изменение режима не блокирует генерацию
3. **Умное кеширование**: Кешировать ответы с учетом режима и контекста

---

## Заключение

Все варианты реализации **не влияют на скорость генерации и стриминг**, так как режим определяется до начала генерации ответа. Рекомендуется использовать **Вариант 4 (Комбинированный подход)** для максимальной гибкости и удобства пользователей.

