#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ –∫–æ–¥–µ–∫—Å–æ–≤ —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
"""

import urllib.request
import urllib.error
import urllib.parse
import re
import json
import os
import time
from datetime import datetime
from pathlib import Path

class AdvancedCodexParser:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä –∫–æ–¥–µ–∫—Å–æ–≤"""
    
    def __init__(self, output_dir="/root/advakod/parsed_codexes"):
        self.base_url = "http://pravo.gov.ru"
        self.codex_url = "http://pravo.gov.ru/codex/"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        (self.output_dir / "raw_html").mkdir(exist_ok=True)
        (self.output_dir / "parsed_data").mkdir(exist_ok=True)
        (self.output_dir / "codex_links").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        self.download_delay = 1
        self.parsed_data = []
        self.found_codexes = []
        
        print(f"‚úÖ AdvancedCodexParser –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.output_dir}")
    
    def log(self, message, level="INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        log_file = self.output_dir / "logs" / f"parser_{datetime.now().strftime('%Y%m%d')}.log"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + "\n")
    
    def download_and_save(self, url, filename, description=""):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª"""
        try:
            self.log(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º: {description} - {url}")
            
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read()
                
                filepath = self.output_dir / "raw_html" / filename
                
                with open(filepath, 'wb') as f:
                    f.write(content)
                
                self.log(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath} ({len(content)} –±–∞–π—Ç)")
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            self.log(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}", "ERROR")
            return None
    
    def extract_codex_links_advanced(self, html_content, base_url):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–¥–µ–∫—Å—ã"""
        # –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–æ–¥–µ–∫—Å–æ–≤ –†–§
        known_codexes = [
            '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å', '–Ω–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å', '—Ç—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å',
            '—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å', '—Å–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–∂–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–∑–µ–º–µ–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–≤–æ–¥–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–ª–µ—Å–Ω–æ–π –∫–æ–¥–µ–∫—Å', '–≤–æ–∑–¥—É—à–Ω—ã–π –∫–æ–¥–µ–∫—Å', '—Ç–∞–º–æ–∂–µ–Ω–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–±—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å', '—É–≥–æ–ª–æ–≤–Ω–æ-–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å',
            '—É–≥–æ–ª–æ–≤–Ω–æ-–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å', '–∫–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö'
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
        link_patterns = [
            r'<a[^>]*href=["\']([^"\']*)["\'][^>]*>([^<]*)</a>',
            r'href=["\']([^"\']*)["\']',
            r'<link[^>]*href=["\']([^"\']*)["\'][^>]*>'
        ]
        
        found_links = []
        
        for pattern in link_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            
            for match in matches:
                if isinstance(match, tuple):
                    url, text = match
                else:
                    url = match
                    text = ""
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–æ–¥–µ–∫—Å–æ–≤
                url_lower = url.lower()
                text_lower = text.lower()
                
                for codex in known_codexes:
                    if (codex in url_lower or codex in text_lower or 
                        any(word in url_lower for word in codex.split())):
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if url.startswith('/'):
                            full_url = self.base_url + url
                        elif url.startswith('http'):
                            full_url = url
                        else:
                            full_url = base_url.rstrip('/') + '/' + url
                        
                        found_links.append({
                            'url': full_url,
                            'text': text.strip(),
                            'codex_type': codex,
                            'confidence': self.calculate_confidence(url, text, codex)
                        })
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        unique_links = {}
        for link in found_links:
            if link['url'] not in unique_links or link['confidence'] > unique_links[link['url']]['confidence']:
                unique_links[link['url']] = link
        
        return sorted(unique_links.values(), key=lambda x: x['confidence'], reverse=True)
    
    def calculate_confidence(self, url, text, codex_type):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–æ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –Ω–∞ –∫–æ–¥–µ–∫—Å"""
        confidence = 0
        
        url_lower = url.lower()
        text_lower = text.lower()
        
        # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä—è–º—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        if codex_type in url_lower:
            confidence += 50
        if codex_type in text_lower:
            confidence += 40
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        codex_words = codex_type.split()
        for word in codex_words:
            if word in url_lower:
                confidence += 10
            if word in text_lower:
                confidence += 8
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if '–∫–æ–¥–µ–∫—Å' in url_lower or '–∫–æ–¥–µ–∫—Å' in text_lower:
            confidence += 20
        
        if any(word in url_lower for word in ['–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤', '–Ω–æ—Ä–º']):
            confidence += 5
        
        return min(confidence, 100)  # –ú–∞–∫—Å–∏–º—É–º 100
    
    def analyze_codex_page(self, url, html_content):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–¥–µ–∫—Å–∞"""
        analysis = {
            'url': url,
            'title': self.extract_title(html_content),
            'content_length': len(html_content),
            'has_pdf_links': bool(re.search(r'\.pdf', html_content, re.IGNORECASE)),
            'has_document_links': bool(re.search(r'–¥–æ–∫—É–º–µ–Ω—Ç|–∞–∫—Ç|–∑–∞–∫–æ–Ω', html_content, re.IGNORECASE)),
            'codex_mentions': len(re.findall(r'–∫–æ–¥–µ–∫—Å', html_content, re.IGNORECASE)),
            'article_mentions': len(re.findall(r'—Å—Ç–∞—Ç—å—è|—Å—Ç\.', html_content, re.IGNORECASE)),
            'chapter_mentions': len(re.findall(r'–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª', html_content, re.IGNORECASE))
        }
        
        return analysis
    
    def extract_title(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        title_patterns = [
            r'<title[^>]*>([^<]*)</title>',
            r'<h1[^>]*>([^<]*)</h1>',
            r'<h2[^>]*>([^<]*)</h2>'
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"
    
    def find_publication_links(self, html_content):
        """–ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∏—Å—Ç–µ–º—É –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""
        pub_patterns = [
            r'href=["\']([^"\']*publication\.pravo\.gov\.ru[^"\']*)["\']',
            r'href=["\']([^"\']*Document/View[^"\']*)["\']',
            r'href=["\']([^"\']*documents/[^"\']*)["\']'
        ]
        
        pub_links = []
        for pattern in pub_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if match.startswith('/'):
                    full_url = self.base_url + match
                elif match.startswith('http'):
                    full_url = match
                else:
                    full_url = self.base_url + '/' + match
                
                pub_links.append(full_url)
        
        return list(set(pub_links))  # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def parse_codex_section_deep(self):
        """–ì–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ –∫–æ–¥–µ–∫—Å–æ–≤"""
        self.log("üîç –ù–∞—á–∏–Ω–∞–µ–º –≥–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ –∫–æ–¥–µ–∫—Å–æ–≤")
        
        # 1. –°–∫–∞—á–∏–≤–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–∞–∑–¥–µ–ª–∞ –∫–æ–¥–µ–∫—Å–æ–≤
        html_content = self.download_and_save(
            self.codex_url, 
            "codex_main_page.html",
            "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–∞–∑–¥–µ–ª–∞ –∫–æ–¥–µ–∫—Å–æ–≤"
        )
        
        if not html_content:
            return False
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        analysis = self.analyze_codex_page(self.codex_url, html_content)
        self.log(f"üìä –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {analysis}")
        
        # 3. –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–¥–µ–∫—Å—ã
        codex_links = self.extract_codex_links_advanced(html_content, self.codex_url)
        self.log(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–¥–µ–∫—Å—ã: {len(codex_links)}")
        
        # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        for i, link in enumerate(codex_links[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            self.log(f"   {i}. {link['codex_type']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {link['confidence']}%) - {link['url']}")
        
        # 5. –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∏—Å—Ç–µ–º—É –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        pub_links = self.find_publication_links(html_content)
        self.log(f"üìö –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {len(pub_links)}")
        
        # 6. –°–∫–∞—á–∏–≤–∞–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–¥–µ–∫—Å–æ–≤
        for i, link in enumerate(codex_links[:5], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 5
            self.log(f"üì• –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥–µ–∫—Å {i}/{min(5, len(codex_links))}: {link['codex_type']}")
            
            filename = f"codex_{i}_{link['confidence']}.html"
            page_content = self.download_and_save(
                link['url'],
                filename,
                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–æ–¥–µ–∫—Å–∞: {link['codex_type']}"
            )
            
            if page_content:
                page_analysis = self.analyze_codex_page(link['url'], page_content)
                page_analysis['codex_type'] = link['codex_type']
                page_analysis['confidence'] = link['confidence']
                
                self.parsed_data.append(page_analysis)
                self.found_codexes.append(link)
            
            time.sleep(self.download_delay)
        
        # 7. –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Å–∏—Å—Ç–µ–º—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        for i, pub_url in enumerate(pub_links[:3], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 3
            self.log(f"üìö –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é {i}/{min(3, len(pub_links))}: {pub_url}")
            
            filename = f"publication_{i}.html"
            page_content = self.download_and_save(
                pub_url,
                filename,
                f"–ü—É–±–ª–∏–∫–∞—Ü–∏—è {i}"
            )
            
            if page_content:
                page_analysis = self.analyze_codex_page(pub_url, page_content)
                page_analysis['type'] = 'publication'
                
                self.parsed_data.append(page_analysis)
            
            time.sleep(self.download_delay)
        
        return True
    
    def save_parsing_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–¥–µ–∫—Å—ã
        codexes_file = self.output_dir / "parsed_data" / "found_codexes.json"
        with open(codexes_file, 'w', encoding='utf-8') as f:
            json.dump(self.found_codexes, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü
        analysis_file = self.output_dir / "parsed_data" / "page_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(self.parsed_data, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_codexes_found': len(self.found_codexes),
            'total_pages_analyzed': len(self.parsed_data),
            'codexes_by_type': {},
            'summary': {
                'high_confidence_codexes': len([c for c in self.found_codexes if c['confidence'] >= 70]),
                'medium_confidence_codexes': len([c for c in self.found_codexes if 40 <= c['confidence'] < 70]),
                'low_confidence_codexes': len([c for c in self.found_codexes if c['confidence'] < 40])
            }
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–æ–¥–µ–∫—Å—ã –ø–æ —Ç–∏–ø–∞–º
        for codex in self.found_codexes:
            codex_type = codex['codex_type']
            if codex_type not in report['codexes_by_type']:
                report['codexes_by_type'][codex_type] = 0
            report['codexes_by_type'][codex_type] += 1
        
        report_file = self.output_dir / "parsed_data" / "parsing_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.log(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        self.log(f"   üîó –ö–æ–¥–µ–∫—Å—ã: {codexes_file}")
        self.log(f"   üìä –ê–Ω–∞–ª–∏–∑: {analysis_file}")
        self.log(f"   üìã –û—Ç—á–µ—Ç: {report_file}")
        
        return report
    
    def run_deep_parsing(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≥–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥"""
        self.log("üöÄ –ó–∞–ø—É—Å–∫ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–¥–µ–∫—Å–æ–≤")
        start_time = datetime.now()
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            success = self.parse_codex_section_deep()
            
            if success:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                report = self.save_parsing_results()
                
                end_time = datetime.now()
                duration = end_time - start_time
                
                self.log(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                self.log(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
                self.log(f"üîó –ù–∞–π–¥–µ–Ω–æ –∫–æ–¥–µ–∫—Å–æ–≤: {report['total_codexes_found']}")
                self.log(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {report['total_pages_analyzed']}")
                
                return report
            else:
                self.log("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è", "ERROR")
                return None
                
        except Exception as e:
            self.log(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", "ERROR")
            return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –∫–æ–¥–µ–∫—Å–æ–≤")
    print("=" * 50)
    
    parser = AdvancedCodexParser()
    report = parser.run_deep_parsing()
    
    if report:
        print(f"\nüìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–ê–†–°–ò–ù–ì–ê:")
        print(f"   üîó –ù–∞–π–¥–µ–Ω–æ –∫–æ–¥–µ–∫—Å–æ–≤: {report['total_codexes_found']}")
        print(f"   üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {report['total_pages_analyzed']}")
        print(f"   ‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['summary']['high_confidence_codexes']}")
        print(f"   ‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['summary']['medium_confidence_codexes']}")
        print(f"   ‚ùì –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['summary']['low_confidence_codexes']}")
        
        if report['codexes_by_type']:
            print(f"\nüèõÔ∏è –ö–û–î–ï–ö–°–´ –ü–û –¢–ò–ü–ê–ú:")
            for codex_type, count in report['codexes_by_type'].items():
                print(f"   ‚Ä¢ {codex_type}: {count}")
    
    print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()

