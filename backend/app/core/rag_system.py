import os
import json
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import asyncio
import aiohttp
from pathlib import Path
import re
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class Document:
    """–î–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    id: str
    title: str
    content: str
    category: str
    source: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    created_at: datetime = None
    updated_at: datetime = None

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    document: Document
    score: float
    relevance: float
    matched_snippets: List[str]

@dataclass
class QueryContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞"""
    query: str
    user_id: int
    session_id: Optional[str]
    category: Optional[str]
    filters: Dict[str, Any]
    max_results: int = 10

class DocumentProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        self.stop_words = {
            "–∏", "–≤", "–≤–æ", "–Ω–µ", "—á—Ç–æ", "–æ–Ω", "–Ω–∞", "—è", "—Å", "—Å–æ", "–∫–∞–∫", "–∞", "—Ç–æ", "–≤—Å–µ", "–æ–Ω–∞", "—Ç–∞–∫", "–µ–≥–æ", "–Ω–æ", "–¥–∞", "—Ç—ã", "–∫", "—É", "–∂–µ", "–≤—ã", "–∑–∞", "–±—ã", "–ø–æ", "—Ç–æ–ª—å–∫–æ", "–µ–µ", "–º–Ω–µ", "–±—ã–ª–æ", "–≤–æ—Ç", "–æ—Ç", "–º–µ–Ω—è", "–µ—â–µ", "–Ω–µ—Ç", "–æ", "–∏–∑", "–µ–º—É", "—Ç–µ–ø–µ—Ä—å", "–∫–æ–≥–¥–∞", "–¥–∞–∂–µ", "–Ω—É", "–≤–¥—Ä—É–≥", "–ª–∏", "–µ—Å–ª–∏", "—É–∂–µ", "–∏–ª–∏", "–Ω–∏", "–±—ã—Ç—å", "–±—ã–ª", "–Ω–µ–≥–æ", "–¥–æ", "–≤–∞—Å", "–Ω–∏–±—É–¥—å", "–æ–ø—è—Ç—å", "—É–∂", "–≤–∞–º", "–≤–µ–¥—å", "—Ç–∞–º", "–ø–æ—Ç–æ–º", "—Å–µ–±—è", "–Ω–∏—á–µ–≥–æ", "–µ–π", "–º–æ–∂–µ—Ç", "–æ–Ω–∏", "—Ç—É—Ç", "–≥–¥–µ", "–µ—Å—Ç—å", "–Ω–∞–¥–æ", "–Ω–µ–π", "–¥–ª—è", "–º—ã", "—Ç–µ–±—è", "–∏—Ö", "—á–µ–º", "–±—ã–ª–∞", "—Å–∞–º", "—á—Ç–æ–±", "–±–µ–∑", "–±—É–¥—Ç–æ", "—á–µ–≥–æ", "—Ä–∞–∑", "—Ç–æ–∂–µ", "—Å–µ–±–µ", "–ø–æ–¥", "–±—É–¥–µ—Ç", "–∂", "—Ç–æ–≥–¥–∞", "–∫—Ç–æ", "—ç—Ç–æ—Ç", "—Ç–æ–≥–æ", "–ø–æ—Ç–æ–º—É", "—ç—Ç–æ–≥–æ", "–∫–∞–∫–æ–π", "—Å–æ–≤—Å–µ–º", "–Ω–∏–º", "–∑–¥–µ—Å—å", "—ç—Ç–æ–º", "–æ–¥–∏–Ω", "–ø–æ—á—Ç–∏", "–º–æ–π", "—Ç–µ–º", "—á—Ç–æ–±—ã", "–Ω–µ–µ", "—Å–µ–π—á–∞—Å", "–±—ã–ª–∏", "–∫—É–¥–∞", "–∑–∞—á–µ–º", "–≤—Å–µ—Ö", "–Ω–∏–∫–æ–≥–¥–∞", "–º–æ–∂–Ω–æ", "–ø—Ä–∏", "–Ω–∞–∫–æ–Ω–µ—Ü", "–¥–≤–∞", "–æ–±", "–¥—Ä—É–≥–æ–π", "—Ö–æ—Ç—å", "–ø–æ—Å–ª–µ", "–Ω–∞–¥", "–±–æ–ª—å—à–µ", "—Ç–æ—Ç", "—á–µ—Ä–µ–∑", "—ç—Ç–∏", "–Ω–∞—Å", "–ø—Ä–æ", "–≤—Å–µ–≥–æ", "–Ω–∏—Ö", "–∫–∞–∫–∞—è", "–º–Ω–æ–≥–æ", "—Ä–∞–∑–≤–µ", "—Ç—Ä–∏", "—ç—Ç—É", "–º–æ—è", "–≤–ø—Ä–æ—á–µ–º", "—Ö–æ—Ä–æ—à–æ", "—Å–≤–æ—é", "—ç—Ç–æ–π", "–ø–µ—Ä–µ–¥", "–∏–Ω–æ–≥–¥–∞", "–ª—É—á—à–µ", "—á—É—Ç—å", "—Ç–æ–º", "–Ω–µ–ª—å–∑—è", "—Ç–∞–∫–æ–π", "–∏–º", "–±–æ–ª–µ–µ", "–≤—Å–µ–≥–¥–∞", "–∫–æ–Ω–µ—á–Ω–æ", "–≤—Å—é", "–º–µ–∂–¥—É"
        }
    
    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = text.split()
        words = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        return ' '.join(words)
    
    def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        processed_text = self.preprocess_text(text)
        words = processed_text.split()
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]
    
    def split_document(self, document: Document, chunk_size: int = 500, overlap: int = 50) -> List[Document]:
        """–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (–¥–ª—è –Ω–µ-—é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"""
        import hashlib
        
        content = document.content
        chunks = []
        
        # Create document signature for unique parent_doc_id
        doc_signature_data = f"{document.source}:{document.metadata.get('edition', 'v0')}:{content[:2000]}"
        doc_sig = hashlib.md5(doc_signature_data.encode()).hexdigest()[:8]
        parent_doc_id = f"{document.source}:{document.metadata.get('edition', 'v0')}:{doc_sig}"
        
        start = 0
        chunk_index = 0
        
        while start < len(content):
            end = start + chunk_size
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–Ω—á–∏—Ç—å –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(content):
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if content[i] in '.!?':
                        end = i + 1
                        break
            
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                # Create unique chunk ID with content hash to prevent collisions
                content_hash = hashlib.md5(chunk_content.encode()).hexdigest()[:8]
                chunk_id = f"{parent_doc_id}:chunk:{chunk_index}:{content_hash}"
                
                chunk = Document(
                    id=chunk_id,
                    title=f"{document.title} (—á–∞—Å—Ç—å {chunk_index + 1})",
                    content=chunk_content,
                    category=document.category,
                    source=document.source,
                    metadata={
                        **document.metadata,
                        "chunk_index": chunk_index,
                        "original_doc_id": document.id,
                        "parent_doc_id": parent_doc_id,
                        "content_hash": content_hash,
                        "start_pos": start,
                        "end_pos": end,
                        "chunk_type": "paragraph"  # Default for non-legal
                    },
                    created_at=document.created_at,
                    updated_at=document.updated_at
                )
                chunks.append(chunk)
                chunk_index += 1
            
            start = end - overlap
        
        return chunks
        import hashlib
        from ..core.legal_chunker import chunk_legal_document
        
        # Use enhanced legal chunking for legal documents
        if self._is_legal_document(document):
            logger.info(f"üè¶ Using legal chunking for document: {document.id}")
            
            legal_chunks = chunk_legal_document(
                text=document.content,
                document_id=document.id,
                edition=document.metadata.get('edition', 'v0'),
                max_tokens=chunk_size // 4  # Convert chars to approximate tokens
            )
            
            # Convert LegalChunk to Document format
            chunks = []
            for legal_chunk in legal_chunks:
                chunk_doc = Document(
                    id=legal_chunk.id,
                    title=f"{document.title} ({legal_chunk.chunk_type.value} {legal_chunk.hierarchy.get('article', '')})",
                    content=legal_chunk.content,
                    category=document.category,
                    source=document.source,
                    metadata={
                        **document.metadata,
                        **legal_chunk.hierarchy,
                        "chunk_type": legal_chunk.chunk_type.value,
                        "token_count": legal_chunk.token_count,
                        "chunk_index": legal_chunk.metadata.get("chunk_index"),
                        "content_hash": legal_chunk.metadata.get("content_hash"),
                        "start_pos": legal_chunk.start_pos,
                        "end_pos": legal_chunk.end_pos,
                        "original_doc_id": document.id,
                        "parent_doc_id": legal_chunk.metadata.get("doc_signature")
                    },
                    created_at=document.created_at,
                    updated_at=document.updated_at
                )
                chunks.append(chunk_doc)
            
            return chunks
        else:
            # Use original chunking for non-legal documents
            return self._split_document_original(document, chunk_size, overlap)
    
    def _is_legal_document(self, document: Document) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º"""
        # Check category
        if document.category in ['law', 'legal', '–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤–æ']:
            return True
        
        # Check content for legal indicators
        content_lower = document.content.lower()
        legal_indicators = [
            '—Å—Ç–∞—Ç—å—è', '–≥–∫ —Ä—Ñ', '—É–∫ —Ä—Ñ', '—Ç–∫ —Ä—Ñ', '–Ω–∫ —Ä—Ñ',
            '–∫–æ–∞–ø —Ä—Ñ', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω', '–∫–æ–¥–µ–∫—Å',
            'article', 'law', 'legal code', '–ø—É–Ω–∫—Ç', '—á–∞—Å—Ç—å'
        ]
        
        return any(indicator in content_lower for indicator in legal_indicators)
    
    def _split_document_original(self, document: Document, chunk_size: int = 500, overlap: int = 50) -> List[Document]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ ID"""
        import hashlib
        
        content = document.content
        chunks = []
        
        # Create document signature for unique parent_doc_id
        doc_signature_data = f"{document.source}:{document.metadata.get('edition', 'v0')}:{content[:2000]}"
        doc_sig = hashlib.md5(doc_signature_data.encode()).hexdigest()[:8]
        parent_doc_id = f"{document.source}:{document.metadata.get('edition', 'v0')}:{doc_sig}"
        
        start = 0
        chunk_index = 0
        
        while start < len(content):
            end = start + chunk_size
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–Ω—á–∏—Ç—å –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(content):
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if content[i] in '.!?':
                        end = i + 1
                        break
            
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                # Create unique chunk ID with content hash to prevent collisions
                content_hash = hashlib.md5(chunk_content.encode()).hexdigest()[:8]
                chunk_id = f"{parent_doc_id}:chunk:{chunk_index}:{content_hash}"
                
                chunk = Document(
                    id=chunk_id,
                    title=f"{document.title} (—á–∞—Å—Ç—å {chunk_index + 1})",
                    content=chunk_content,
                    category=document.category,
                    source=document.source,
                    metadata={
                        **document.metadata,
                        "chunk_index": chunk_index,
                        "original_doc_id": document.id,
                        "parent_doc_id": parent_doc_id,
                        "content_hash": content_hash,
                        "start_pos": start,
                        "end_pos": end
                    },
                    created_at=document.created_at,
                    updated_at=document.updated_at
                )
                chunks.append(chunk)
                chunk_index += 1
            
            start = end - overlap
        
        return chunks
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        entities = {
            "organizations": [],
            "laws": [],
            "dates": [],
            "numbers": []
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–û–û–û, –ê–û, –ò–ü –∏ —Ç.–¥.)
        org_pattern = r'\b(?:–û–û–û|–ê–û|–ó–ê–û|–û–ê–û|–ò–ü|–ü–ê–û|–ê–û|–¢–û–û|–ß–ü)\s+[–ê-–Ø–∞-—è\s"]+'
        entities["organizations"] = re.findall(org_pattern, text)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–∫–æ–Ω–æ–≤ –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤
        law_pattern = r'\b(?:–§–ó|–ì–ö|–¢–ö|–ù–ö|–£–ö|–ö–æ–ê–ü|–ì–ü–ö|–ê–ü–ö|–£–ü–ö)\s*[‚Ññ#]?\s*\d+'
        entities["laws"] = re.findall(law_pattern, text)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç
        date_pattern = r'\b\d{1,2}[./]\d{1,2}[./]\d{2,4}\b'
        entities["dates"] = re.findall(date_pattern, text)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–µ–ª
        number_pattern = r'\b\d+(?:[.,]\d+)?\s*(?:—Ä—É–±|–¥–æ–ª–ª|–µ–≤—Ä–æ|%|—Ç—ã—Å|–º–ª–Ω|–º–ª—Ä–¥)?\b'
        entities["numbers"] = re.findall(number_pattern, text)
        
        return entities

class EmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.embedding_model = "text-embedding-ada-002"
        self.cache = {}  # –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        text_hash = hash(text)
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        try:
            if self.openai_api_key:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI API
                embedding = await self._get_openai_embedding(text)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF
                embedding = self._get_simple_embedding(text)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.cache[text_hash] = embedding
            return embedding
            
        except Exception as e:
            logger.error(f"Error getting embedding: {str(e)}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return [0.0] * 1536  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ OpenAI
    
    async def _get_openai_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ OpenAI API"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.openai.com/v1/embeddings',
                headers={
                    'Authorization': f'Bearer {self.openai_api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.embedding_model,
                    'input': text
                }
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data['data'][0]['embedding']
                else:
                    raise Exception(f"OpenAI API error: {response.status}")
    
    def _get_simple_embedding(self, text: str) -> List[float]:
        """–ü—Ä–æ—Å—Ç–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF"""
        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sentence-transformers
        
        words = text.lower().split()
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
        embedding = [0.0] * 1536
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–µ–π —Å–ª–æ–≤
        for i, word in enumerate(words[:1536]):
            word_hash = hash(word) % 1000
            embedding[i] = word_freq[word] / len(words) * (word_hash / 1000.0)
        
        return embedding
    
    def cosine_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not embedding1 or not embedding2:
            return 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        dot_product = np.dot(embedding1, embedding2)
        return dot_product / (norm1 * norm2)

class RAGSystem:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self):
        self.documents: Dict[str, Document] = {}
        self.document_processor = DocumentProcessor()
        self.embedding_service = EmbeddingService()
        self.search_index = {}  # –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    
    async def add_document(self, document: Document) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            processed_content = self.document_processor.preprocess_text(document.content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.document_processor.extract_keywords(document.content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            entities = self.document_processor.extract_entities(document.content)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = await self.embedding_service.get_embedding(processed_content)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document.embedding = embedding
            document.metadata.update({
                "keywords": keywords,
                "entities": entities,
                "processed_content": processed_content
            })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            self.documents[document.id] = document
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
            self._update_search_index(document)
            
            logger.info(f"Document added: {document.id}")
            return document.id
            
        except Exception as e:
            logger.error(f"Error adding document: {str(e)}")
            raise
    
    async def add_documents_batch(self, documents: List[Document]) -> List[str]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        added_ids = []
        
        for document in documents:
            try:
                doc_id = await self.add_document(document)
                added_ids.append(doc_id)
            except Exception as e:
                logger.error(f"Error adding document {document.id}: {str(e)}")
                continue
        
        return added_ids
    
    def _update_search_index(self, document: Document):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in document.metadata.get("keywords", []):
            if keyword not in self.search_index:
                self.search_index[keyword] = []
            self.search_index[keyword].append(document.id)
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_key = f"category:{document.category}"
        if category_key not in self.search_index:
            self.search_index[category_key] = []
        self.search_index[category_key].append(document.id)
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
        for entity_type, entities in document.metadata.get("entities", {}).items():
            for entity in entities:
                entity_key = f"entity:{entity_type}:{entity}"
                if entity_key not in self.search_index:
                    self.search_index[entity_key] = []
                self.search_index[entity_key].append(document.id)
    
    async def search(self, query_context: QueryContext) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            processed_query = self.document_processor.preprocess_text(query_context.query)
            query_embedding = await self.embedding_service.get_embedding(processed_query)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            query_keywords = self.document_processor.extract_keywords(query_context.query)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            candidates = self._get_search_candidates(query_context, query_keywords)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            results = []
            for doc_id in candidates:
                document = self.documents.get(doc_id)
                if not document or not document.embedding:
                    continue
                
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                semantic_score = self.embedding_service.cosine_similarity(
                    query_embedding, document.embedding
                )
                
                # –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ —Å—Ö–æ–¥—Å—Ç–≤–æ
                keyword_score = self._calculate_keyword_score(
                    query_keywords, document.metadata.get("keywords", [])
                )
                
                # –û–±—â–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                relevance = (semantic_score * 0.7) + (keyword_score * 0.3)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if relevance > 0.1:
                    # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                    snippets = self._extract_relevant_snippets(
                        query_context.query, document.content
                    )
                    
                    results.append(SearchResult(
                        document=document,
                        score=semantic_score,
                        relevance=relevance,
                        matched_snippets=snippets
                    ))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x.relevance, reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return results[:query_context.max_results]
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return []
    
    def _get_search_candidates(self, query_context: QueryContext, query_keywords: List[str]) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        candidates = set()
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in query_keywords:
            if keyword in self.search_index:
                candidates.update(self.search_index[keyword])
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if query_context.category:
            category_key = f"category:{query_context.category}"
            if category_key in self.search_index:
                candidates.update(self.search_index[category_key])
        
        # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –º–∞–ª–æ, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if len(candidates) < 10:
            candidates.update(self.documents.keys())
        
        return list(candidates)
    
    def _calculate_keyword_score(self, query_keywords: List[str], doc_keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if not query_keywords or not doc_keywords:
            return 0.0
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        query_set = set(query_keywords)
        doc_set = set(doc_keywords)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        intersection = query_set.intersection(doc_set)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–ª—é –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        return len(intersection) / len(query_set)
    
    def _extract_relevant_snippets(self, query: str, content: str, max_snippets: int = 3) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
        snippets = []
        query_words = set(query.lower().split())
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_scores = []
        for sentence in sentences:
            sentence_words = set(sentence.lower().split())
            intersection = query_words.intersection(sentence_words)
            score = len(intersection) / len(query_words) if query_words else 0
            sentence_scores.append((sentence.strip(), score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        for sentence, score in sentence_scores[:max_snippets]:
            if score > 0 and sentence:
                snippets.append(sentence)
        
        return snippets
    
    async def get_document(self, doc_id: str) -> Optional[Document]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ ID"""
        return self.documents.get(doc_id)
    
    async def delete_document(self, doc_id: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if doc_id in self.documents:
            document = self.documents[doc_id]
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
            self._remove_from_index(document)
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            del self.documents[doc_id]
            
            logger.info(f"Document deleted: {doc_id}")
            return True
        
        return False
    
    def _remove_from_index(self, document: Document):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞"""
        # –£–¥–∞–ª—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in document.metadata.get("keywords", []):
            if keyword in self.search_index:
                if document.id in self.search_index[keyword]:
                    self.search_index[keyword].remove(document.id)
                if not self.search_index[keyword]:
                    del self.search_index[keyword]
        
        # –£–¥–∞–ª—è–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_key = f"category:{document.category}"
        if category_key in self.search_index:
            if document.id in self.search_index[category_key]:
                self.search_index[category_key].remove(document.id)
            if not self.search_index[category_key]:
                del self.search_index[category_key]
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG —Å–∏—Å—Ç–µ–º—ã"""
        total_docs = len(self.documents)
        categories = {}
        
        for doc in self.documents.values():
            category = doc.category
            categories[category] = categories.get(category, 0) + 1
        
        return {
            "total_documents": total_docs,
            "categories": categories,
            "index_size": len(self.search_index),
            "embedding_cache_size": len(self.embedding_service.cache)
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã
rag_system = RAGSystem()

def get_rag_system() -> RAGSystem:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ RAG —Å–∏—Å—Ç–µ–º—ã"""
    return rag_system
