"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
"""
import logging
import json
import os
import subprocess
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from sqlalchemy.orm import Session

from ..models.training_data import TrainingData, TrainingJob, ModelVersion
from ..core.config import settings
from ..core.optimized_lora_config import OptimizedLoRAConfig, ModelSize, TaskComplexity

logger = logging.getLogger(__name__)


class LoRATrainingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA"""
    
    def __init__(self, db: Session):
        self.db = db
        self.training_dir = os.path.join(settings.BASE_DIR, "lora_training")
        self.data_dir = os.path.join(self.training_dir, "data")
        self.models_dir = os.path.join(self.training_dir, "models")
        self.scripts_dir = os.path.join(self.training_dir, "scripts")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        os.makedirs(self.training_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.scripts_dir, exist_ok=True)
    
    async def run_training(self, job_id: int) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è LoRA
        
        Args:
            job_id: ID –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è LoRA –¥–ª—è –∑–∞–¥–∞—á–∏ {job_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è
            job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
            if not job:
                logger.error(f"–ó–∞–¥–∞—á–∞ {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return {"error": "–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            job.status = "running"
            job.started_at = datetime.utcnow()
            self.db.commit()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data_path = await self._prepare_training_data(job_id)
            if not data_path:
                job.status = "failed"
                job.error_message = "–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"
                job.completed_at = datetime.utcnow()
                self.db.commit()
                return {"error": "–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"}
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
            config_path = await self._create_training_config(job, data_path)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            result = await self._execute_training(job, config_path)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
            if result["success"]:
                job.status = "completed"
                job.progress = 100.0
                job.loss = result.get("final_loss")
                job.accuracy = result.get("final_accuracy")
                
                # –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏
                model_version = await self._create_model_version(job, result)
                if model_version:
                    job.metadata = job.metadata or {}
                    job.metadata["model_version_id"] = model_version.id
            else:
                job.status = "failed"
                job.error_message = result.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
            
            job.completed_at = datetime.utcnow()
            self.db.commit()
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è –∑–∞–¥–∞—á–∏ {job_id}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
            job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
            if job:
                job.status = "failed"
                job.error_message = str(e)
                job.completed_at = datetime.utcnow()
                self.db.commit()
            
            return {"error": str(e)}
    
    async def _prepare_training_data(self, job_id: int) -> Optional[str]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–¥–æ–±—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            training_data = self.db.query(TrainingData).filter(
                TrainingData.is_approved == True,
                TrainingData.is_used_for_training == False
            ).all()
            
            if not training_data:
                logger.error("–ù–µ—Ç –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return None
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            formatted_data = []
            for data in training_data:
                formatted_item = {
                    "instruction": data.instruction,
                    "input": data.input or "",
                    "output": data.output
                }
                formatted_data.append(formatted_item)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL —Ñ–∞–π–ª
            data_file = os.path.join(self.data_dir, f"training_data_{job_id}.jsonl")
            with open(data_file, 'w', encoding='utf-8') as f:
                for item in formatted_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            # –ü–æ–º–µ—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
            for data in training_data:
                data.is_used_for_training = True
            self.db.commit()
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(formatted_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return data_file
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    async def _create_training_config(self, job: TrainingJob, data_path: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            logger.info("‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
            metadata = job.metadata or {}
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            model_size = ModelSize(metadata.get("model_size", "medium"))
            task_complexity = TaskComplexity(metadata.get("task_complexity", "moderate"))
            legal_task_type = metadata.get("legal_task_type")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            optimized_config = OptimizedLoRAConfig.get_optimized_config(
                model_size=model_size,
                task_complexity=task_complexity,
                legal_task_type=legal_task_type,
                custom_modifications=job.hyperparameters
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            warnings = OptimizedLoRAConfig.validate_config(optimized_config)
            if warnings:
                logger.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {warnings}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            final_config = {
                "model_name_or_path": settings.SAIGA_MODEL_PATH,
                "data_path": data_path,
                "output_dir": os.path.join(self.models_dir, f"lora_model_{job.id}"),
                "max_seq_length": 1024,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
                "packing": False,
                "remove_unused_columns": False,
                "dataloader_pin_memory": False,
                "seed": 42,
                "report_to": "none",
                **optimized_config
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_file = os.path.join(self.scripts_dir, f"training_config_{job.id}.json")
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(final_config, f, indent=2, ensure_ascii=False)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ metadata –∑–∞–¥–∞—á–∏
            job.metadata = job.metadata or {}
            job.metadata["optimized_config"] = optimized_config
            job.metadata["config_warnings"] = warnings
            self.db.commit()
            
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_file}")
            logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {model_size.value}/{task_complexity.value}/{legal_task_type}")
            
            return config_file
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    async def _execute_training(self, job: TrainingJob, config_path: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            logger.info("ü§ñ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            
            # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
            script_path = await self._create_training_script(config_path)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            process = await asyncio.create_subprocess_exec(
                "python", script_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.scripts_dir
            )
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            stdout_lines = []
            stderr_lines = []
            
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                
                line_str = line.decode('utf-8').strip()
                stdout_lines.append(line_str)
                
                # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if "loss" in line_str.lower():
                    try:
                        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ loss –∏–∑ –ª–æ–≥–∞
                        if "loss=" in line_str:
                            loss_str = line_str.split("loss=")[1].split()[0]
                            loss = float(loss_str)
                            job.loss = loss
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (–ø—Ä–∏–º–µ—Ä–Ω–æ)
                            if "epoch" in line_str:
                                epoch_str = line_str.split("epoch")[1].split()[0]
                                epoch = float(epoch_str)
                                progress = (epoch / job.hyperparameters.get("num_train_epochs", 3)) * 100
                                job.progress = min(100.0, progress)
                                self.db.commit()
                    except:
                        pass
                
                logger.info(f"Training: {line_str}")
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            return_code = await process.wait()
            
            # –ß–∏—Ç–∞–µ–º stderr
            stderr = await process.stderr.read()
            stderr_lines = stderr.decode('utf-8').split('\n')
            
            if return_code == 0:
                logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
                return {
                    "success": True,
                    "final_loss": job.loss,
                    "final_accuracy": job.accuracy,
                    "output": stdout_lines
                }
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è (–∫–æ–¥ {return_code})")
                return {
                    "success": False,
                    "error": f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {stderr_lines[-2] if stderr_lines else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}",
                    "output": stdout_lines,
                    "stderr": stderr_lines
                }
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _create_training_script(self, config_path: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            logger.info("üìù –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è")
            
            script_content = f'''
import json
import os
import sys
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_config(config_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def prepare_dataset(data_path):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    data = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    formatted_data = []
    for item in data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', '')
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è Saiga
        if input_text:
            prompt = f"<s>system\\n–¢—ã - –ò–ò-—é—Ä–∏—Å—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º –ø—Ä–∞–≤–µ.\\n<|im_end|>\\n<|im_start|>user\\n{instruction}\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>"
        else:
            prompt = f"<s>system\\n–¢—ã - –ò–ò-—é—Ä–∏—Å—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º –ø—Ä–∞–≤–µ.\\n<|im_end|>\\n<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>"
        
        formatted_data.append({{"text": prompt}})
    
    return Dataset.from_list(formatted_data)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = load_config("{config_path}")
        logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {{config}}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(config["model_name_or_path"])
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForCausalLM.from_pretrained(
            config["model_name_or_path"],
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
        lora_config = LoraConfig(
            r=config["lora_r"],
            lora_alpha=config["lora_alpha"],
            target_modules=config["lora_target_modules"],
            lora_dropout=config["lora_dropout"],
            bias=config["lora_bias"],
            task_type=TaskType.CAUSAL_LM,
        )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –º–æ–¥–µ–ª–∏
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        dataset = prepare_dataset(config["data_path"])
        logger.info(f"–î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {{len(dataset)}} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=config["max_seq_length"],
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=config["output_dir"],
            num_train_epochs=config["num_train_epochs"],
            per_device_train_batch_size=config["per_device_train_batch_size"],
            gradient_accumulation_steps=config["gradient_accumulation_steps"],
            learning_rate=config["learning_rate"],
            warmup_steps=config["warmup_steps"],
            logging_steps=config["logging_steps"],
            save_steps=config["save_steps"],
            evaluation_strategy=config["evaluation_strategy"],
            eval_steps=config["eval_steps"],
            fp16=config["fp16"],
            bf16=config["bf16"],
            gradient_checkpointing=config["gradient_checkpointing"],
            optim=config["optim"],
            lr_scheduler_type=config["lr_scheduler_type"],
            warmup_ratio=config["warmup_ratio"],
            weight_decay=config["weight_decay"],
            max_grad_norm=config["max_grad_norm"],
            seed=config["seed"],
            report_to=config["report_to"],
            remove_unused_columns=config["remove_unused_columns"],
            dataloader_pin_memory=config["dataloader_pin_memory"],
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(config["output_dir"])
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {{e}}")
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
            
            script_path = os.path.join(self.scripts_dir, f"train_lora_{job.id}.py")
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(script_content)
            
            logger.info(f"‚úÖ –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω: {script_path}")
            return script_path
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {e}")
            raise
    
    async def _create_model_version(self, job: TrainingJob, result: Dict[str, Any]) -> Optional[ModelVersion]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ—Ä—Å–∏—é
            version = f"v{job.id}.0.0"
            
            # –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            model_path = os.path.join(self.models_dir, f"lora_model_{job.id}")
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î
            model_version = ModelVersion(
                version=version,
                base_model=settings.SAIGA_MODEL_PATH,
                lora_config=job.hyperparameters.get("lora_config", {}),
                training_data_count=job.training_data_count,
                performance_metrics={
                    "final_loss": result.get("final_loss"),
                    "final_accuracy": result.get("final_accuracy"),
                    "training_duration": (job.completed_at - job.started_at).total_seconds() if job.completed_at and job.started_at else None
                },
                model_path=model_path,
                created_by=job.created_by
            )
            
            self.db.add(model_version)
            self.db.commit()
            self.db.refresh(model_version)
            
            logger.info(f"‚úÖ –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω–∞: {version}")
            return model_version
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def get_training_status(self, job_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
            if not job:
                return {"error": "–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}
            
            return {
                "id": job.id,
                "job_name": job.job_name,
                "status": job.status,
                "progress": job.progress,
                "current_epoch": job.current_epoch,
                "total_epochs": job.total_epochs,
                "loss": job.loss,
                "accuracy": job.accuracy,
                "error_message": job.error_message,
                "started_at": job.started_at.isoformat() if job.started_at else None,
                "completed_at": job.completed_at.isoformat() if job.completed_at else None
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
            return {"error": str(e)}
    
    @classmethod
    def get_recommended_config(
        cls, 
        model_size: str = "medium",
        task_type: str = "legal_qa",
        dataset_size: int = 1000
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LoRA
        
        Args:
            model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ ('small', 'medium', 'large')
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            dataset_size: –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            if dataset_size < 500:
                complexity = TaskComplexity.SIMPLE
            elif dataset_size < 2000:
                complexity = TaskComplexity.MODERATE
            else:
                complexity = TaskComplexity.COMPLEX
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config = OptimizedLoRAConfig.get_optimized_config(
                model_size=ModelSize(model_size),
                task_complexity=complexity,
                legal_task_type=task_type
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            config["recommendation_info"] = {
                "model_size": model_size,
                "task_type": task_type,
                "dataset_size": dataset_size,
                "complexity": complexity.value,
                "estimated_training_time": cls._estimate_training_time(config, dataset_size),
                "memory_requirements": cls._estimate_memory_requirements(config),
                "available_legal_tasks": OptimizedLoRAConfig.get_available_legal_tasks()
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            warnings = OptimizedLoRAConfig.validate_config(config)
            if warnings:
                config["warnings"] = warnings
            
            return {
                "success": True,
                "config": config,
                "description": f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è {model_size} –º–æ–¥–µ–ª–∏, –∑–∞–¥–∞—á–∞: {task_type}"
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    @staticmethod
    def _estimate_training_time(config: Dict[str, Any], dataset_size: int) -> str:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è"""
        epochs = config.get("num_train_epochs", 3)
        batch_size = config.get("per_device_train_batch_size", 1)
        grad_accum = config.get("gradient_accumulation_steps", 4)
        
        effective_batch_size = batch_size * grad_accum
        steps_per_epoch = dataset_size // effective_batch_size
        total_steps = steps_per_epoch * epochs
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1-2 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ —à–∞–≥ –¥–ª—è LoRA
        estimated_seconds = total_steps * 1.5
        
        if estimated_seconds < 3600:
            return f"{int(estimated_seconds // 60)} –º–∏–Ω—É—Ç"
        else:
            return f"{estimated_seconds / 3600:.1f} —á–∞—Å–æ–≤"
    
    @staticmethod
    def _estimate_memory_requirements(config: Dict[str, Any]) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏"""
        lora_config = config.get("lora_config", {})
        r = lora_config.get("r", 16)
        batch_size = config.get("per_device_train_batch_size", 1)
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è 7B –º–æ–¥–µ–ª–∏
        base_memory = 14  # GB –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        lora_memory = (r / 16) * 2  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è LoRA
        batch_memory = batch_size * 2  # –ü–∞–º—è—Ç—å –¥–ª—è batch
        
        total_memory = base_memory + lora_memory + batch_memory
        
        return f"{total_memory:.1f} GB VRAM"
