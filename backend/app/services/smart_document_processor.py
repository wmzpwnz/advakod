"""
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
"""

import asyncio
import logging
import re
import time
import uuid
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

from .unified_llm_service import unified_llm_service
from .embeddings_service import embeddings_service
from .vector_store_service import vector_store_service
from ..core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class DocumentMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    title: str
    document_type: str  # "–∫–æ–¥–µ–∫—Å", "–∑–∞–∫–æ–Ω", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ", etc.
    sections: List[str]  # –°–ø–∏—Å–æ–∫ —Ä–∞–∑–¥–µ–ª–æ–≤/–≥–ª–∞–≤
    articles: List[Dict[str, Any]]  # –°—Ç–∞—Ç—å–∏ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    keywords: List[str]  # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    legal_entities: List[str]  # –ü—Ä–∞–≤–æ–≤—ã–µ —Å—É–±—ä–µ–∫—Ç—ã
    dates: List[str]  # –î–∞—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    structure_score: float  # –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏


@dataclass
class SmartChunk:
    """–£–º–Ω—ã–π —á–∞–Ω–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    content: str
    chunk_type: str  # "—Å—Ç–∞—Ç—å—è", "—Ä–∞–∑–¥–µ–ª", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–ø—Ä–∏–º–µ—Ä"
    article_number: Optional[str] = None
    section_title: Optional[str] = None
    keywords: List[str] = None
    legal_entities: List[str] = None
    importance_score: float = 0.0
    context: str = ""  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç


class SmartDocumentProcessor:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.llm_service = unified_llm_service
        self.embeddings_service = embeddings_service
        self.vector_store_service = vector_store_service
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self.article_pattern = re.compile(r'—Å—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)?)', re.IGNORECASE)
        self.section_pattern = re.compile(r'—Ä–∞–∑–¥–µ–ª\s+[ivx\d]+|–≥–ª–∞–≤–∞\s+\d+', re.IGNORECASE)
        self.definition_pattern = re.compile(r'–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–ø–æ–Ω—è—Ç–∏–µ|—Ç–µ—Ä–º–∏–Ω', re.IGNORECASE)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.legal_keywords = {
            '—É–≥–æ–ª–æ–≤–Ω—ã–π': ['–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '—Å—É–¥'],
            '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π': ['–¥–æ–≥–æ–≤–æ—Ä', '–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞—Å–ª–µ–¥—Å—Ç–≤–æ'],
            '—Ç—Ä—É–¥–æ–≤–æ–π': ['—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '–æ—Ç–ø—É—Å–∫', '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ'],
            '–Ω–∞–ª–æ–≥–æ–≤—ã–π': ['–Ω–∞–ª–æ–≥', '—Å–±–æ—Ä', '–¥–µ–∫–ª–∞—Ä–∞—Ü–∏—è', '—à—Ç—Ä–∞—Ñ'],
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π': ['–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏–µ', '—à—Ç—Ä–∞—Ñ', '–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ']
        }
    
    async def process_document(self, file_path: str, content: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ"""
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: {file_path}")
        
        start_time = time.time()
        
        try:
            # 1. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata = await self._analyze_document_structure(content)
            
            # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            extracted_info = await self._extract_key_information(content, metadata)
            
            # 3. –°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            smart_chunks = await self._create_smart_chunks(content, metadata, extracted_info)
            
            # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            enhanced_chunks = await self._enhance_chunks_with_embeddings(smart_chunks)
            
            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            await self._save_enhanced_chunks(file_path, enhanced_chunks, metadata)
            
            processing_time = time.time() - start_time
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.2f}—Å")
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(enhanced_chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            logger.info(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(metadata.articles)} —Å—Ç–∞—Ç–µ–π")
            
            return {
                "success": True,
                "processing_time": processing_time,
                "chunks_count": len(enhanced_chunks),
                "articles_count": len(metadata.articles),
                "sections_count": len(metadata.sections),
                "metadata": metadata.__dict__,
                "chunks_preview": [chunk.__dict__ for chunk in enhanced_chunks[:3]]
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    async def _analyze_document_structure(self, content: str) -> DocumentMetadata:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏
        articles = []
        article_matches = self.article_pattern.findall(content)
        for i, article_num in enumerate(article_matches):
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            article_text = self._extract_article_text(content, article_num)
            articles.append({
                "number": article_num,
                "title": self._extract_article_title(article_text),
                "content_preview": article_text[:200] + "..." if len(article_text) > 200 else article_text
            })
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª—ã
        sections = []
        section_matches = self.section_pattern.findall(content)
        for section in section_matches:
            sections.append(section.strip())
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_type = self._classify_document_type(content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–æ–≤—ã–µ —Å—É–±—ä–µ–∫—Ç—ã
        legal_entities = self._extract_legal_entities(content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã
        dates = self._extract_dates(content)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        structure_score = self._calculate_structure_score(content, articles, sections)
        
        return DocumentMetadata(
            title=self._extract_document_title(content),
            document_type=document_type,
            sections=sections,
            articles=articles,
            keywords=keywords,
            legal_entities=legal_entities,
            dates=dates,
            structure_score=structure_score
        )
    
    async def _extract_key_information(self, content: str, metadata: DocumentMetadata) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é AI"""
        logger.info("ü§ñ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é AI...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        analysis_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:

        –î–û–ö–£–ú–ï–ù–¢:
        {content[:1500]}...

        –ò–∑–≤–ª–µ–∫–∏:
        1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        2. –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ —Å—Ä–æ–∫–∏
        3. –í–∞–∂–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Å—É–±—ä–µ–∫—Ç—ã
        4. –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∏–¥—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        5. –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è

        –û—Ç–≤–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
        """
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            import asyncio
            analysis_result = ""
            async for chunk in self.llm_service.generate_response(
                prompt=analysis_prompt,
                max_tokens=settings.AI_DOCUMENT_ANALYSIS_TOKENS,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (4000)
                temperature=0.1,
                stream=True
            ):
                analysis_result += chunk
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            extracted_info = {
                "legal_concepts": self._extract_legal_concepts(analysis_result),
                "procedures": self._extract_procedures(analysis_result),
                "entities": self._extract_entities(analysis_result),
                "responsibilities": self._extract_responsibilities(analysis_result),
                "terms": self._extract_terms(analysis_result)
            }
            
            return extracted_info
            
        except asyncio.TimeoutError:
            logger.warning(f"‚ö†Ô∏è AI –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–≤—ã—Å–∏–ª —Ç–∞–π–º–∞—É—Ç ({settings.AI_DOCUMENT_ANALYSIS_TIMEOUT} —Å–µ–∫), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑")
            return {"error": "AI analysis timeout", "skipped": True}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"error": str(e), "skipped": True}
    
    async def _create_smart_chunks(self, content: str, metadata: DocumentMetadata, extracted_info: Dict[str, Any]) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        logger.info("üß© –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–µ —á–∞–Ω–∫–∏...")
        
        smart_chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç–∞—Ç—å—è–º
        for article in metadata.articles:
            article_content = self._extract_article_content(content, article["number"])
            if article_content:
                chunk = SmartChunk(
                    content=article_content,
                    chunk_type="—Å—Ç–∞—Ç—å—è",
                    article_number=article["number"],
                    section_title=article["title"],
                    keywords=metadata.keywords,
                    legal_entities=metadata.legal_entities,
                    importance_score=self._calculate_importance_score(article_content),
                    context=f"–°—Ç–∞—Ç—å—è {article['number']} –∏–∑ {metadata.document_type}"
                )
                smart_chunks.append(chunk)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
        for section in metadata.sections:
            section_content = self._extract_section_content(content, section)
            if section_content:
                chunk = SmartChunk(
                    content=section_content,
                    chunk_type="—Ä–∞–∑–¥–µ–ª",
                    section_title=section,
                    keywords=metadata.keywords,
                    legal_entities=metadata.legal_entities,
                    importance_score=self._calculate_importance_score(section_content),
                    context=f"–†–∞–∑–¥–µ–ª {section} –∏–∑ {metadata.document_type}"
                )
                smart_chunks.append(chunk)
        
        # FALLBACK: –ï—Å–ª–∏ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π –∏ —Ä–∞–∑–¥–µ–ª–æ–≤, —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —á–∞–Ω–∫–∏
        if not smart_chunks and not metadata.articles and not metadata.sections:
            logger.info("üîÑ –°–æ–∑–¥–∞–µ–º fallback —á–∞–Ω–∫–∏ (–Ω–µ—Ç —Å—Ç–∞—Ç–µ–π/—Ä–∞–∑–¥–µ–ª–æ–≤)")
            smart_chunks = self._create_fallback_chunks(content, metadata)
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        definition_chunks = self._extract_definitions(content)
        for definition in definition_chunks:
            chunk = SmartChunk(
                content=definition,
                chunk_type="–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
                keywords=metadata.keywords,
                legal_entities=metadata.legal_entities,
                importance_score=0.9,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã
                context=f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ {metadata.document_type}"
            )
            smart_chunks.append(chunk)
        
        return smart_chunks
    
    async def _enhance_chunks_with_embeddings(self, smart_chunks: List[SmartChunk]) -> List[Dict[str, Any]]:
        """–£–ª—É—á—à–∞–µ—Ç —á–∞–Ω–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ (–±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)"""
        logger.info("üîó –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤...")
        
        if not smart_chunks:
            return []
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        enhanced_texts = []
        chunk_metadata = []
        
        for chunk in smart_chunks:
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            enhanced_text = f"{chunk.content}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {chunk.context}"
            if chunk.keywords:
                enhanced_text += f"\n–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(chunk.keywords)}"
            
            enhanced_texts.append(enhanced_text)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "chunk_type": chunk.chunk_type,
                "article_number": chunk.article_number,
                "section_title": chunk.section_title,
                "keywords": chunk.keywords or [],
                "legal_entities": chunk.legal_entities or [],
                "importance_score": chunk.importance_score,
                "context": chunk.context,
                "enhanced_text": enhanced_text
            }
            chunk_metadata.append(metadata)
        
        # –ë–ê–¢–ß–ï–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å—Ä–∞–∑—É
        logger.info(f"üöÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(enhanced_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–µ–º...")
        embeddings = await self.embeddings_service.encode_texts(enhanced_texts)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        enhanced_chunks = []
        for i, (chunk, embedding) in enumerate(zip(smart_chunks, embeddings)):
            if embedding is None:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ {i} - embedding –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
                continue
                
            enhanced_chunks.append({
                "content": chunk.content,
                "embedding": embedding,
                "metadata": chunk_metadata[i]
            })
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(enhanced_chunks)} —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
        return enhanced_chunks
    
    async def _save_enhanced_chunks(self, file_path: str, enhanced_chunks: List[Dict[str, Any]], metadata: DocumentMetadata):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        contents = [chunk["content"] for chunk in enhanced_chunks]
        embeddings = [chunk["embedding"] for chunk in enhanced_chunks]
        metadatas = []
        
        for i, chunk in enumerate(enhanced_chunks):
            chunk_metadata = chunk["metadata"].copy()
            chunk_metadata.update({
                "filename": Path(file_path).name,
                "document_type": metadata.document_type,
                "document_title": metadata.title,
                "chunk_index": i,
                "total_chunks": len(enhanced_chunks),
                "structure_score": metadata.structure_score
            })
            metadatas.append(chunk_metadata)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π document_id
        document_uuid = str(uuid.uuid4())
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –æ—Ç–¥–µ–ª—å–Ω–æ
        added_count = 0
        for i, (content, embedding, metadata) in enumerate(zip(contents, embeddings, metadatas)):
            # –í–∞–ª–∏–¥–∞—Ü–∏—è embedding –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            if not embedding or len(embedding) < 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä embedding
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ {i} –∏–∑-–∑–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ embedding")
                continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π chunk_id
            content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
            chunk_id = f"{document_uuid}_{i}_{content_hash}"
            
            # –û–±–Ω–æ–≤–ª—è–µ–º metadata —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ ID
            metadata['document_id'] = document_uuid
            metadata['chunk_id'] = chunk_id
            
            success = await self.vector_store_service.add_document(
                content=content,
                embedding=embedding,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º embedding
                metadata=metadata,
                document_id=chunk_id  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π chunk_id
            )
            if success:
                added_count += 1
        
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(enhanced_chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_article_text(self, content: str, article_num: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏
        normalized_num = article_num.replace('.', r'\.').replace('‚Ññ', '')
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π
        patterns = [
            # –°—Ç–∞—Ç—å—è 123
            rf'\b—Å—Ç–∞—Ç—å—è\s+{normalized_num}\b.*?(?=\b—Å—Ç–∞—Ç—å—è\s+\d+|$)',
            # –°—Ç. 123
            rf'\b—Å—Ç\.\s*{normalized_num}\b.*?(?=\b—Å—Ç\.\s*\d+|$)',
            # –°—Ç–∞—Ç—å—è ‚Ññ 123
            rf'\b—Å—Ç–∞—Ç—å—è\s*‚Ññ\s*{normalized_num}\b.*?(?=\b—Å—Ç–∞—Ç—å—è\s*‚Ññ\s*\d+|$)',
            # –°—Ç ‚Ññ 123
            rf'\b—Å—Ç\s*‚Ññ\s*{normalized_num}\b.*?(?=\b—Å—Ç\s*‚Ññ\s*\d+|$)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(0).strip()
        
        return ""
    
    def _extract_article_title(self, article_text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        lines = article_text.split('\n')
        for line in lines[1:3]:  # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 3 —Å—Ç—Ä–æ–∫–∞—Ö
            if line.strip() and not line.strip().startswith('—Å—Ç–∞—Ç—å—è'):
                return line.strip()
        return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    
    def _classify_document_type(self, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content_lower = content.lower()
        
        if '—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å' in content_lower or '—É–∫ —Ä—Ñ' in content_lower:
            return '—É–≥–æ–ª–æ–≤–Ω—ã–π_–∫–æ–¥–µ–∫—Å'
        elif '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å' in content_lower or '–≥–∫ —Ä—Ñ' in content_lower:
            return '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π_–∫–æ–¥–µ–∫—Å'
        elif '—Ç—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å' in content_lower or '—Ç–∫ —Ä—Ñ' in content_lower:
            return '—Ç—Ä—É–¥–æ–≤–æ–π_–∫–æ–¥–µ–∫—Å'
        elif '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è' in content_lower:
            return '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è'
        else:
            return '–¥—Ä—É–≥–æ–π_–¥–æ–∫—É–º–µ–Ω—Ç'
    
    def _extract_keywords(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', content.lower())
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-20 —Å–ª–æ–≤
        return sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
    
    def _extract_legal_entities(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–æ–≤—ã–µ —Å—É–±—ä–µ–∫—Ç—ã"""
        entities = []
        patterns = [
            r'–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ',
            r'–≥—Ä–∞–∂–¥–∞–Ω–∏–Ω',
            r'—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ',
            r'–æ—Ä–≥–∞–Ω –≤–ª–∞—Å—Ç–∏',
            r'—Å—É–¥',
            r'–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞',
            r'–ø–æ–ª–∏—Ü–∏—è'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities.extend(matches)
        
        return list(set(entities))
    
    def _extract_dates(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        date_pattern = r'\d{1,2}\.\d{1,2}\.\d{4}|\d{4}\s*–≥–æ–¥–∞'
        return re.findall(date_pattern, content)
    
    def _calculate_structure_score(self, content: str, articles: List[Dict], sections: List[str]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        score = 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å—Ç–∞—Ç—å–∏
        score += min(len(articles) * 0.1, 0.5)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–¥–µ–ª—ã
        score += min(len(sections) * 0.05, 0.3)
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–µ)
        if len(content) > 10000:
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_importance_score(self, content: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞"""
        score = 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if any(word in content.lower() for word in ['–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '–ø–æ–Ω—è—Ç–∏–µ', '—Ç–µ—Ä–º–∏–Ω']):
            score += 0.3
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
        if any(word in content.lower() for word in ['–ø–æ—Ä—è–¥–æ–∫', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '—Å—Ä–æ–∫']):
            score += 0.2
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å
        if any(word in content.lower() for word in ['–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '—à—Ç—Ä–∞—Ñ']):
            score += 0.2
        
        return min(score, 1.0)
    
    def _extract_document_title(self, content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        lines = content.split('\n')
        for line in lines[:5]:
            if line.strip() and len(line.strip()) > 10:
                return line.strip()
        return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    
    def _extract_article_content(self, content: str, article_num: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        return self._extract_article_text(content, article_num)
    
    def _extract_section_content(self, content: str, section: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞"""
        pattern = rf'{re.escape(section)}.*?(?=—Ä–∞–∑–¥–µ–ª|–≥–ª–∞–≤–∞|$)'
        match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
        return match.group(0) if match else ""
    
    def _extract_definitions(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        definitions = []
        pattern = r'–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.*?(?=—Å—Ç–∞—Ç—å—è|—Ä–∞–∑–¥–µ–ª|$)'
        matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
        definitions.extend(matches)
        return definitions
    
    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é AI
    def _extract_legal_concepts(self, analysis_result: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        concepts = []
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –≤ AI-–æ—Ç–≤–µ—Ç–µ
        keywords = [
            '–ø—Ä–∞–≤–æ', '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '—Å—Ä–æ–∫', '–ø–æ—Ä—è–¥–æ–∫',
            '—Å–≤–æ–±–æ–¥–∞', '—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ', '—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å', '–∑–∞–∫–æ–Ω', '–Ω–æ—Ä–º–∞', '–ø—Ä–∏–Ω—Ü–∏–ø',
            '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–∑–∞—â–∏—Ç–∞', '–æ—Ö—Ä–∞–Ω–∞', '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ'
        ]
        
        for keyword in keywords:
            if keyword in analysis_result.lower():
                concepts.append(keyword)
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –∏ —Ä–∞–∑–¥–µ–ª—ã
        article_pattern = r'—Å—Ç–∞—Ç—å—è\s+\d+'
        articles = re.findall(article_pattern, analysis_result, re.IGNORECASE)
        concepts.extend(articles)
        
        return list(set(concepts))
    
    def _extract_procedures(self, analysis_result: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        procedures = []
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        keywords = [
            '–ø–æ—Ä—è–¥–æ–∫', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '—Å—Ä–æ–∫', '—ç—Ç–∞–ø', '—Å—Ç–∞–¥–∏—è', '–ø—Ä–æ—Ü–µ—Å—Å',
            '—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è', '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ', '—Å–æ–±–ª—é–¥–µ–Ω–∏–µ'
        ]
        
        for keyword in keywords:
            if keyword in analysis_result.lower():
                procedures.append(keyword)
        
        return list(set(procedures))
    
    def _extract_entities(self, analysis_result: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É–±—ä–µ–∫—Ç—ã –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        entities = []
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—É–±—ä–µ–∫—Ç—ã –ø—Ä–∞–≤–∞
        keywords = [
            '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ', '–≥—Ä–∞–∂–¥–∞–Ω–∏–Ω', '—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ', '–æ—Ä–≥–∞–Ω –≤–ª–∞—Å—Ç–∏',
            '—Å—É–¥', '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞', '–ø–æ–ª–∏—Ü–∏—è', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
            '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è'
        ]
        
        for keyword in keywords:
            if keyword in analysis_result.lower():
                entities.append(keyword)
        
        return list(set(entities))
    
    def _extract_responsibilities(self, analysis_result: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–¥—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        responsibilities = []
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        keywords = [
            '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '—à—Ç—Ä–∞—Ñ', '–ª–∏—à–µ–Ω–∏–µ', '–∞—Ä–µ—Å—Ç',
            '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã', '–∏—Å–ø—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã', '–ª–∏—à–µ–Ω–∏–µ —Å–≤–æ–±–æ–¥—ã',
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '—É–≥–æ–ª–æ–≤–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å'
        ]
        
        for keyword in keywords:
            if keyword in analysis_result.lower():
                responsibilities.append(keyword)
        
        return list(set(responsibilities))
    
    def _extract_terms(self, analysis_result: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        terms = []
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        keywords = [
            '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '–ø–æ–Ω—è—Ç–∏–µ', '—Ç–µ—Ä–º–∏–Ω', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–ø—Ä–∏–∑–Ω–∞–∫', '—ç–ª–µ–º–µ–Ω—Ç',
            '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è', '–∑–∞–∫–æ–Ω', '–Ω–æ—Ä–º–∞', '–ø—Ä–∏–Ω—Ü–∏–ø', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '—Å–≤–æ–±–æ–¥–∞',
            '—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ', '—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å', '–¥–µ–º–æ–∫—Ä–∞—Ç–∏—è', '—Ñ–µ–¥–µ—Ä–∞—Ü–∏—è'
        ]
        
        for keyword in keywords:
            if keyword in analysis_result.lower():
                terms.append(keyword)
        
        return list(set(terms))
    
    def _create_fallback_chunks(self, content: str, metadata: DocumentMetadata) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        logger.info("üîÑ –°–æ–∑–¥–∞–µ–º fallback —á–∞–Ω–∫–∏...")
        
        chunks = []
        chunk_size = 2000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        overlap = 200      # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏
        for i in range(0, len(content), chunk_size - overlap):
            chunk_content = content[i:i + chunk_size]
            if len(chunk_content.strip()) < 100:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
                continue
                
            chunk = SmartChunk(
                content=chunk_content,
                chunk_type="fallback",
                section_title=f"–ß–∞—Å—Ç—å {i // (chunk_size - overlap) + 1}",
                keywords=metadata.keywords or [],
                legal_entities=metadata.legal_entities or [],
                importance_score=0.5,  # –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å
                context=f"–î–æ–∫—É–º–µ–Ω—Ç: {metadata.document_type}"
            )
            chunks.append(chunk)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} fallback —á–∞–Ω–∫–æ–≤")
        return chunks


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
smart_document_processor = SmartDocumentProcessor()
