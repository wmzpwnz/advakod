"""
–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
–ë–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
"""

import logging
import hashlib
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, date
from dataclasses import dataclass, asdict
import re

logger = logging.getLogger(__name__)

@dataclass
class LegalMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    source: str  # "–ì–ö –†–§", "–£–ö –†–§", etc.
    article: Optional[str] = None  # "—Å—Ç. 432"
    part: Optional[str] = None  # "—á.2"
    item: Optional[str] = None  # "–ø.3"
    edition: Optional[str] = None  # "—Ä–µ–¥. –æ—Ç 01.07.2024"
    valid_from: Optional[date] = None
    valid_to: Optional[date] = None
    url: Optional[str] = None
    ingested_at: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
        for key, value in result.items():
            if isinstance(value, (date, datetime)):
                result[key] = value.isoformat()
        return result

@dataclass
class LegalChunk:
    """–ß–∞–Ω–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: str
    content: str
    metadata: LegalMetadata
    token_count: int
    chunk_index: int
    parent_doc_id: str
    neighbors: List[str] = None
    
    def __post_init__(self):
        if self.neighbors is None:
            self.neighbors = []

class SimpleExpertRAG:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self):
        self.documents = {}  # –•—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏
        self.chunks = {}      # –•—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        self.initialized = False
        self.persistence_file = "data/simple_rag_data.json"
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            await self._load_data()
            
            self.initialized = True
            logger.info("üéâ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise
    
    async def _save_data(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª"""
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(self.persistence_file), exist_ok=True)
            
            data = {
                "documents": self.documents,
                "chunks": self.chunks,
                "saved_at": datetime.now().isoformat()
            }
            
            with open(self.persistence_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.persistence_file}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    async def _load_data(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if not os.path.exists(self.persistence_file):
                logger.info("üìÅ –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –±–∞–∑—ã")
                return
                
            with open(self.persistence_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            self.documents = data.get("documents", {})
            self.chunks = data.get("chunks", {})
            
            logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ {len(self.chunks)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –±–∞–∑—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.documents = {}
            self.chunks = {}
    
    def count_tokens(self, text: str) -> int:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞)"""
        return len(text) // 4
    
    def extract_hierarchy(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        hierarchy = {
            "code": None,
            "section": None,
            "chapter": None,
            "article": None,
            "part": None,
            "item": None,
            "paragraph": None
        }
        
        # –ü–æ–∏—Å–∫ –∫–æ–¥–∞ (–ì–ö –†–§, –£–ö –†–§, –¢–ö –†–§ –∏ —Ç.–¥.)
        code_patterns = [
            r'(–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å|–ì–ö)\s*(?:–†–§|–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)',
            r'(–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å|–£–ö)\s*(?:–†–§|–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)',
            r'(–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å|–¢–ö)\s*(?:–†–§|–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)',
            r'(–°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å|–°–ö)\s*(?:–†–§|–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)',
            r'(–ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å|–ñ–ö)\s*(?:–†–§|–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)'
        ]
        
        for pattern in code_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                hierarchy["code"] = match.group(1)
                break
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        article_match = re.search(r'—Å—Ç\.\s*(\d+(?:\.\d+)*)', text, re.IGNORECASE)
        if article_match:
            hierarchy["article"] = f"—Å—Ç. {article_match.group(1)}"
        
        # –ü–æ–∏—Å–∫ —á–∞—Å—Ç–µ–π
        part_match = re.search(r'—á\.\s*(\d+)', text, re.IGNORECASE)
        if part_match:
            hierarchy["part"] = f"—á.{part_match.group(1)}"
        
        # –ü–æ–∏—Å–∫ –ø—É–Ω–∫—Ç–æ–≤
        item_match = re.search(r'–ø\.\s*(\d+)', text, re.IGNORECASE)
        if item_match:
            hierarchy["item"] = f"–ø.{item_match.group(1)}"
        
        return hierarchy
    
    def split_into_chunks(self, text: str, metadata: LegalMetadata) -> List[LegalChunk]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏"""
        chunks = []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        normalized_text = self._normalize_text(text)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        hierarchy = self.extract_hierarchy(normalized_text)
        
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(normalized_text)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        chunk_size = 500  # —Ç–æ–∫–µ–Ω–æ–≤
        overlap_size = 75  # 15% –æ—Ç 500
        
        for i, sentence in enumerate(sentences):
            sentence_tokens = self.count_tokens(sentence)
            
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
            if current_tokens + sentence_tokens > chunk_size:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        current_chunk.strip(),
                        metadata,
                        hierarchy,
                        chunk_index,
                        len(chunks)
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                overlap_text = self._get_overlap_text(current_chunk, overlap_size)
                current_chunk = overlap_text + " " + sentence
                current_tokens = self.count_tokens(current_chunk)
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_tokens += sentence_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunk = self._create_chunk(
                current_chunk.strip(),
                metadata,
                hierarchy,
                chunk_index,
                len(chunks)
            )
            chunks.append(chunk)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
        self._set_neighbors(chunks)
        
        return chunks
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫ –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É
        text = re.sub(r'["""]', '"', text)
        text = re.sub(r"[''']", "'", text)
        
        # –°–∫–ª–µ–π–∫–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        text = re.sub(r'-\s*\n\s*', '', text)
        text = re.sub(r'\n\s*', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        text = re.sub(r'–°—Ç—Ä–∞–Ω–∏—Ü–∞ \d+', '', text)
        text = re.sub(r'–ì–ª–∞–≤–∞ \d+', '', text)
        
        return text.strip()
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        sentences = []
        current = ""
        
        for char in text:
            current += char
            if char in '.!?':
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å—é –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç—å–∏/–ø—É–Ω–∫—Ç–∞
                if not re.search(r'\d+\.\d*$', current.strip()):
                    sentences.append(current.strip())
                    current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if s.strip()]
    
    def _get_overlap_text(self, text: str, overlap_tokens: int) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è"""
        overlap_chars = overlap_tokens * 4  # 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
        if len(text) <= overlap_chars:
            return text
        
        return text[-overlap_chars:]
    
    def _create_chunk(self, content: str, metadata: LegalMetadata, hierarchy: Dict[str, Any], 
                     chunk_index: int, total_chunks: int) -> LegalChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        chunk_id = f"{metadata.source}_{chunk_index}_{hashlib.md5(content.encode()).hexdigest()[:8]}"
        
        return LegalChunk(
            id=chunk_id,
            content=content,
            metadata=metadata,
            token_count=self.count_tokens(content),
            chunk_index=chunk_index,
            parent_doc_id=f"{metadata.source}_doc"
        )
    
    def _set_neighbors(self, chunks: List[LegalChunk]):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏"""
        for i, chunk in enumerate(chunks):
            neighbors = []
            if i > 0:
                neighbors.append(chunks[i-1].id)
            if i < len(chunks) - 1:
                neighbors.append(chunks[i+1].id)
            chunk.neighbors = neighbors
    
    async def add_document(self, file_path: str, metadata: LegalMetadata) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {file_path}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)
            text = await self._extract_text(file_path)
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            logger.info("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —Ç–æ–∫–µ–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏...")
            chunks = self.split_into_chunks(text, metadata)
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏
            for chunk in chunks:
                self.chunks[chunk.id] = chunk
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            self.documents[metadata.source] = {
                "metadata": metadata,
                "chunks": [chunk.id for chunk in chunks],
                "total_chunks": len(chunks)
            }
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            await self._save_data()
            
            return {
                "success": True,
                "chunks_created": len(chunks),
                "document_id": metadata.source,
                "status": "processed"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def add_document_with_text(self, text_content: str, metadata: LegalMetadata) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –≥–æ—Ç–æ–≤—ã–º —Ç–µ–∫—Å—Ç–æ–º –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: {metadata.source}")
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            logger.info("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —Ç–æ–∫–µ–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏...")
            chunks = self.split_into_chunks(text_content, metadata)
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏
            for chunk in chunks:
                self.chunks[chunk.id] = chunk
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            self.documents[metadata.source] = {
                "metadata": metadata,
                "chunks": [chunk.id for chunk in chunks],
                "total_chunks": len(chunks)
            }
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            await self._save_data()
            
            return {
                "success": True,
                "chunks_created": len(chunks),
                "document_id": metadata.source,
                "status": "processed"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def search_documents(self, query: str, situation_date: Optional[date] = None, 
                             top_k: int = 20) -> List[Dict[str, Any]]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫: '{query[:50]}...'")
            
            results = []
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —á–∞–Ω–∫–æ–≤
            for chunk_id, chunk in self.chunks.items():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã —Å–∏—Ç—É–∞—Ü–∏–∏
                if situation_date and chunk.metadata.valid_from:
                    if chunk.metadata.valid_from > situation_date:
                        continue
                    if chunk.metadata.valid_to and chunk.metadata.valid_to < situation_date:
                        continue
                
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                query_words = query.lower().split()
                content_words = chunk.content.lower().split()
                
                # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                matches = sum(1 for word in query_words if word in content_words)
                if matches > 0:
                    score = matches / len(query_words)
                    
                    results.append({
                        "id": chunk.id,
                        "content": chunk.content,
                        "metadata": chunk.metadata.to_dict(),
                        "final_score": score,
                        "is_neighbor": False
                    })
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x['final_score'], reverse=True)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ (windowed retrieval)
            enhanced_results = []
            for result in results[:top_k]:
                enhanced_results.append(result)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤
                chunk = self.chunks.get(result['id'])
                if chunk and chunk.neighbors:
                    for neighbor_id in chunk.neighbors:
                        if neighbor_id in self.chunks:
                            neighbor_chunk = self.chunks[neighbor_id]
                            enhanced_results.append({
                                "id": neighbor_id,
                                "content": neighbor_chunk.content,
                                "metadata": neighbor_chunk.metadata.to_dict(),
                                "final_score": 0.0,
                                "is_neighbor": True
                            })
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(enhanced_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return enhanced_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def _extract_text(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (–∑–∞–≥–ª—É—à–∫–∞)"""
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        return """
        –°—Ç–∞—Ç—å—è 432. –î–æ–≥–æ–≤–æ—Ä —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞–∫–ª—é—á–µ–Ω–Ω—ã–º, –µ—Å–ª–∏ –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –≤ —Ç—Ä–µ–±—É–µ–º–æ–π –≤ –ø–æ–¥–ª–µ–∂–∞—â–∏—Ö —Å–ª—É—á–∞—è—Ö —Ñ–æ—Ä–º–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –¥–æ–≥–æ–≤–æ—Ä–∞.
        
        –ß–∞—Å—Ç—å 2. –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —è–≤–ª—è—é—Ç—Å—è —É—Å–ª–æ–≤–∏—è –æ –ø—Ä–µ–¥–º–µ—Ç–µ –¥–æ–≥–æ–≤–æ—Ä–∞, —É—Å–ª–æ–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∑–≤–∞–Ω—ã –≤ –∑–∞–∫–æ–Ω–µ –∏–ª–∏ –∏–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–∞—Ö –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–∞, –∞ —Ç–∞–∫–∂–µ –≤—Å–µ —Ç–µ —É—Å–ª–æ–≤–∏—è, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ –∑–∞—è–≤–ª–µ–Ω–∏—é –æ–¥–Ω–æ–π –∏–∑ —Å—Ç–æ—Ä–æ–Ω –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ.
        
        –ß–∞—Å—Ç—å 3. –î–æ–≥–æ–≤–æ—Ä –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ñ–µ—Ä—Ç—ã (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä) –æ–¥–Ω–æ–π –∏–∑ —Å—Ç–æ—Ä–æ–Ω –∏ –µ–µ –∞–∫—Ü–µ–ø—Ç–∞ (–ø—Ä–∏–Ω—è—Ç–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω–æ–π.
        """
    
    async def delete_document(self, document_id: str) -> Dict[str, Any]:
        """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—Å–µ –µ–≥–æ —á–∞–Ω–∫–∏ –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã"""
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_id}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
            if document_id not in self.documents:
                return {
                    "success": False,
                    "error": f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω",
                    "chunks_deleted": 0
                }
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            document_info = self.documents[document_id]
            chunk_ids = document_info.get("chunks", [])
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks_deleted = 0
            for chunk_id in chunk_ids:
                if chunk_id in self.chunks:
                    del self.chunks[chunk_id]
                    chunks_deleted += 1
            
            # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
            del self.documents[document_id]
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {document_id} —É–¥–∞–ª–µ–Ω: {chunks_deleted} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            await self._save_data()
            
            return {
                "success": True,
                "document_id": document_id,
                "chunks_deleted": chunks_deleted,
                "status": "deleted"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "chunks_deleted": 0
            }
    
    async def clear_all_documents(self) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —á–∞–Ω–∫–∏ –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã"""
        if not self.initialized:
            await self.initialize()
        
        try:
            logger.info("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ simple_expert_rag")
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —á–∞–Ω–∫–æ–≤
            documents_count = len(self.documents)
            chunks_count = len(self.chunks)
            
            # –û—á–∏—â–∞–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            self.documents.clear()
            self.chunks.clear()
            
            logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–æ: {documents_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {chunks_count} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            await self._save_data()
            
            return {
                "success": True,
                "documents_deleted": documents_count,
                "chunks_deleted": chunks_count,
                "status": "cleared"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return {
                "success": False,
                "error": str(e),
                "documents_deleted": 0,
                "chunks_deleted": 0
            }

    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "status": "simple_expert_rag_operational",
            "system_type": "simple_expert",
            "features": [
                "token_based_chunking",
                "hierarchy_extraction", 
                "versioning_support",
                "windowed_retrieval",
                "simple_search",
                "document_deletion",
                "bulk_clear"
            ],
            "documents_indexed": len(self.documents),
            "chunks_indexed": len(self.chunks),
            "initialized": self.initialized
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
simple_expert_rag = SimpleExpertRAG()
