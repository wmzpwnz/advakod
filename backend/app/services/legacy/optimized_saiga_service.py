"""
Optimized Saiga Service - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é Vistral
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ saiga_service.py —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import logging
import time
import threading
import asyncio
from typing import Optional, AsyncGenerator, Any, Dict
from queue import Queue
from ..core.config import settings

# –í–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å llama_cpp –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
from llama_cpp import Llama

logger = logging.getLogger(__name__)


def _redact_for_logs(text: str, max_len: int = 120) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –∏ –º–∞—Å–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–æ–≥–æ–≤."""
    if not text:
        return ""
    s = text.strip()
    if len(s) <= max_len:
        return s.replace("\n", " ")
    return (s[:max_len//2] + " ... " + s[-max_len//2:]).replace("\n", " ")


def _estimate_tokens(text: str) -> int:
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: 1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞."""
    if not text:
        return 0
    return max(1, len(text) // 4)


class OptimizedSaigaService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é Vistral"""
    
    def __init__(self):
        self.model: Optional[Llama] = None
        self._model_loaded: bool = False
        self._load_lock = threading.Lock()
        self._async_semaphore: Optional[asyncio.Semaphore] = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._inference_timeout = getattr(settings, "VISTRAL_INFERENCE_TIMEOUT", 900)
        self._max_concurrency = getattr(settings, "VISTRAL_MAX_CONCURRENCY", 3)  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 3
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "last_response_time": 0.0,
            "queue_length": 0,
            "concurrent_requests": 0
        }
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self._request_queue = asyncio.Queue()
        self._active_requests = set()
        
    def _ensure_semaphore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self._async_semaphore is None:
            try:
                loop = asyncio.get_running_loop()
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
            except RuntimeError:
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
    
    def _load_model(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Vistral"""
        if self._model_loaded and self.model is not None:
            return

        with self._load_lock:
            if self._model_loaded and self.model is not None:
                return
            try:
                import os
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º VISTRAL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–º–µ—Å—Ç–æ SAIGA
                model_path = getattr(settings, "VISTRAL_MODEL_PATH", settings.SAIGA_MODEL_PATH)
                n_ctx = getattr(settings, "VISTRAL_N_CTX", settings.SAIGA_N_CTX)
                n_threads = getattr(settings, "VISTRAL_N_THREADS", settings.SAIGA_N_THREADS)
                n_gpu_layers = getattr(settings, "VISTRAL_N_GPU_LAYERS", getattr(settings, "SAIGA_N_GPU_LAYERS", 0))
                
                if not os.path.exists(model_path):
                    raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                
                logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Vistral –∏–∑ %s", model_path)
                logger.info("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_ctx=%s, n_threads=%s, max_concurrency=%s", n_ctx, n_threads, self._max_concurrency)

                self.model = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_gpu_layers=n_gpu_layers,
                    logits_all=False,
                    use_mmap=True,
                    use_mlock=False,
                    verbose=False
                )
                self._model_loaded = True
                logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vistral —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Vistral: %s", e)
                raise

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OptimizedSaigaService...")
            await self.ensure_model_loaded_async()
            logger.info("‚úÖ OptimizedSaigaService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OptimizedSaigaService: %s", e)
            raise

    async def ensure_model_loaded_async(self) -> None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self._model_loaded and self.model is not None:
            return
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._load_model)

    def _compute_max_gen_tokens(self, prompt: str, requested_max: int) -> int:
        """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç n_ctx –∏ –¥–ª–∏–Ω—ã prompt"""
        n_ctx = getattr(settings, "VISTRAL_N_CTX", getattr(settings, "SAIGA_N_CTX", 4096))
        prompt_tokens = _estimate_tokens(prompt)
        safety_margin = getattr(settings, "VISTRAL_TOKEN_MARGIN", getattr(settings, "SAIGA_TOKEN_MARGIN", 32))
        available = max(1, n_ctx - prompt_tokens - safety_margin)
        if requested_max > available:
            logger.debug("requested_max (%s) > available (%s) -> limiting to available", requested_max, available)
            return available
        return requested_max

    async def generate_legal_response(
        self,
        prompt: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8,
        request_id: Optional[str] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        start_time = time.time()
        request_id = request_id or f"req_{int(time.time() * 1000)}"
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        self._ensure_semaphore()
        await self.ensure_model_loaded_async()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        self._active_requests.add(request_id)
        self._stats["concurrent_requests"] = len(self._active_requests)
        
        try:
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
            assert self._async_semaphore is not None
            async with self._async_semaphore:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º max tokens
                allowed_max = self._compute_max_gen_tokens(prompt, max_tokens)

                if getattr(settings, "LOG_PROMPTS", False):
                    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (optimized) prompt=%s... max_tokens=%s temp=%s", 
                              _redact_for_logs(prompt, 120), allowed_max, temperature)
                else:
                    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (optimized) max_tokens=%s temp=%s", allowed_max, temperature)

                loop = asyncio.get_running_loop()

                def _blocking_call():
                    try:
                        result = self.model(
                            prompt,
                            max_tokens=allowed_max,
                            temperature=temperature,
                            top_p=top_p,
                            stop=getattr(settings, "VISTRAL_STOP_TOKENS", getattr(settings, "SAIGA_STOP_TOKENS", None)),
                            repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", getattr(settings, "SAIGA_REPEAT_PENALTY", 1.1)),
                        )
                        return result
                    except Exception as e:
                        logger.exception("–û—à–∏–±–∫–∞ –≤ blocking_call –º–æ–¥–µ–ª–∏: %s", e)
                        raise

                try:
                    result = await asyncio.wait_for(
                        loop.run_in_executor(None, _blocking_call), 
                        timeout=self._inference_timeout
                    )
                except asyncio.TimeoutError:
                    logger.error("‚è∞ Inference timeout after %s seconds", self._inference_timeout)
                    raise RuntimeError(f"Inference timeout after {self._inference_timeout} seconds")

                try:
                    text = ""
                    if isinstance(result, dict) and "choices" in result and len(result["choices"]) > 0:
                        text = (result["choices"][0].get("text") or "").strip()
                    else:
                        text = str(result)
                    
                    response_time = time.time() - start_time
                    self._update_stats(True, response_time)
                    logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (len=%s, time=%.2fs)", len(text), response_time)
                    return text
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    self._update_stats(False, response_time)
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥–µ–ª–∏: %s", e)
                    raise
                    
        finally:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self._active_requests.discard(request_id)
            self._stats["concurrent_requests"] = len(self._active_requests)

    async def stream_response(
        self,
        prompt: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8
    ) -> AsyncGenerator[str, None]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç—Ä–∏–º–∏–Ω–≥-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        await self.ensure_model_loaded_async()
        self._ensure_semaphore()

        allowed_max = self._compute_max_gen_tokens(prompt, max_tokens)
        loop = asyncio.get_running_loop()
        q: asyncio.Queue = asyncio.Queue()

        stop_tokens = getattr(settings, "VISTRAL_STOP_TOKENS", getattr(settings, "SAIGA_STOP_TOKENS", None))
        repeat_penalty = getattr(settings, "VISTRAL_REPEAT_PENALTY", getattr(settings, "SAIGA_REPEAT_PENALTY", 1.1))

        def worker():
            try:
                for chunk in self.model(
                    prompt,
                    max_tokens=allowed_max,
                    temperature=temperature,
                    top_p=top_p,
                    stop=stop_tokens,
                    repeat_penalty=repeat_penalty,
                    stream=True
                ):
                    if not chunk:
                        continue
                    choices = chunk.get("choices") or []
                    if not choices:
                        continue
                    delta = choices[0].get("text", "")
                    if delta:
                        loop.call_soon_threadsafe(q.put_nowait, delta)
                loop.call_soon_threadsafe(q.put_nowait, None)
            except Exception as e:
                logger.exception("–û—à–∏–±–∫–∞ –≤ streaming worker: %s", e)
                loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] {str(e)}")
                loop.call_soon_threadsafe(q.put_nowait, None)

        t = threading.Thread(target=worker, daemon=True)
        t.start()

        while True:
            token = await q.get()
            if token is None:
                break
            if isinstance(token, str) and token.startswith("[ERROR]"):
                raise RuntimeError(token)
            yield token

    def create_legal_prompt(self, question: str, context: Optional[str] = None) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º API)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞, —á—Ç–æ –∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Å–µ—Ä–≤–∏—Å–µ
        special_instructions = ""
        question_lower = (question or "").lower()
        
        if "–∏–ø" in question_lower or "–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å" in question_lower:
            special_instructions = "\n# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: —Å–º. —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ (–Ω–µ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å –≤ –∫–æ–¥–µ)\n"
        
        prompt = f"""<s><system>
–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ, —Ç–æ—á–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –¢–û–ß–ù–û–°–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–ò:
- –ù–ò–ö–û–ì–î–ê –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π –∑–∞–∫–æ–Ω—ã, —Å—Ç–∞—Ç—å–∏, —Å—Ä–æ–∫–∏ –∏–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ "—É—Ç–æ—á–Ω–∏—Ç–µ –≤ –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∞—Ö"
- –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 2024 –≥–æ–¥
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–∫–æ–Ω—ã –†–§
- –£–∫–∞–∑—ã–≤–∞–π —Ç–æ—á–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–≤
- –°—Ä–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ (–¥–Ω–∏, –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü—ã, –ù–ï –≥–æ–¥—ã)
- –ù–ï —É–ø–æ–º–∏–Ω–∞–π –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (consultant.ru, garant.ru –∏ —Ç.–¥.)
- –ù–ï –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–µ–±-—Å–∞–π—Ç—ã –≤ –æ—Ç–≤–µ—Ç
- –ù–ï –ü–£–¢–ê–ô –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π!
- –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ - —Å–∫–∞–∂–∏ "—Å—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"

{special_instructions}

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:
- –î–∞–≤–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (1000-2000 —Å–ª–æ–≤)
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —è–∑—ã–∫, –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É
- –ü—Ä–∏–≤–æ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–æ–≤ –∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ –Ω–æ–º–µ—Ä–∞–º–∏
- –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å —á–µ—Ç–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
- –û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
- –í–∫–ª—é—á–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ—á–Ω—ã—Ö —Å—Ä–æ–∫–∞—Ö, —à—Ç—Ä–∞—Ñ–∞—Ö, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
- –î–∞–≤–∞–π –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏
- –ù–ï –ü–û–í–¢–û–†–Ø–ô–°–Ø - –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
- –ó–ê–í–ï–†–®–ê–ô –æ—Ç–≤–µ—Ç –∫—Ä–∞—Ç–∫–∏–º —Ä–µ–∑—é–º–µ

–ü–æ–º–Ω–∏: –ª—É—á—à–µ —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∑–∞—Ç—å "—É—Ç–æ—á–Ω–∏—Ç–µ –≤ –Ω–∞–ª–æ–≥–æ–≤–æ–π/—Å—É–¥–µ/–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–µ", —á–µ–º –¥–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!
</system>
<user>
{question}
</user>
<bot>"""
        
        if context:
            context_section = f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ===\n{context}\n=== –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ===\n\n"
            prompt = prompt.replace(f"<user>\n{question}", f"{context_section}<user>\n{question}")
        
        return prompt

    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._model_loaded and self.model is not None
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        stats = self._stats.copy()
        stats.update({
            "model_loaded": self.is_model_loaded(),
            "max_concurrency": self._max_concurrency,
            "inference_timeout": self._inference_timeout,
            "active_requests": len(self._active_requests)
        })
        return stats
    
    def reset_stats(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "last_response_time": 0.0,
            "queue_length": 0,
            "concurrent_requests": 0
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "status": "healthy" if self.is_model_loaded() else "unhealthy",
            "model_loaded": self.is_model_loaded(),
            "active_requests": len(self._active_requests),
            "max_concurrency": self._max_concurrency,
            "stats": self.get_stats()
        }
    
    def _update_stats(self, success: bool, response_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self._stats["total_requests"] += 1
        if success:
            self._stats["successful_requests"] += 1
        else:
            self._stats["failed_requests"] += 1
        
        self._stats["last_response_time"] = response_time
        
        if self._stats["successful_requests"] > 0:
            total_time = self._stats["average_response_time"] * (self._stats["successful_requests"] - 1)
            self._stats["average_response_time"] = (total_time + response_time) / self._stats["successful_requests"]


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
optimized_saiga_service = OptimizedSaigaService()