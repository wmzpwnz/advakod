"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö (ChromaDB)
–•—Ä–∞–Ω–∏—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã
"""

import logging
import chromadb
from chromadb.config import Settings
import os
from typing import List, Dict, Any, Optional, Tuple, Union
import uuid
import json
from datetime import datetime, date
from ..core.date_utils import DateUtils

logger = logging.getLogger(__name__)

def determine_document_type(file_name: str, document_id: str, text_content: str = "") -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞, ID –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    
    –¢–∏–ø—ã:
    - codex: –∫–æ–¥–µ–∫—Å
    - federal_law: —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω
    - supreme_court_resolution: –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –í–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ —Å—É–¥–∞
    - resolution: –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    - decree: —É–∫–∞–∑
    - order: –ø—Ä–∏–∫–∞–∑
    - other: –¥—Ä—É–≥–æ–µ
    """
    file_name_lower = file_name.lower()
    doc_id_lower = document_id.lower()
    text_lower = text_content.lower()[:5000] if text_content else ""  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    # –ö–æ–¥–µ–∫—Å—ã - –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É –∏–ª–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    if doc_id_lower.startswith('codex_') or '–∫–æ–¥–µ–∫—Å' in file_name_lower:
        return "codex"
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è PDF –∏ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if text_lower:
        # –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –í–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ —Å—É–¥–∞
        if ('–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ' in text_lower and 
            ('–≤–µ—Ä—Ö–æ–≤–Ω' in text_lower or '–≤–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ —Å—É–¥–∞' in text_lower or '–≤—Å —Ä—Ñ' in text_lower)):
            return "supreme_court_resolution"
        
        # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω
        if ('—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω' in text_lower or 
            '—Ñ–∑' in text_lower or 
            '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω —Ä—Ñ' in text_lower):
            return "federal_law"
        
        # –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (–æ–±—â–µ–µ)
        if '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ' in text_lower:
            return "resolution"
        
        # –£–∫–∞–∑
        if '—É–∫–∞–∑' in text_lower and ('–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞' in text_lower or '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç' in text_lower):
            return "decree"
        
        # –ü—Ä–∏–∫–∞–∑
        if '–ø—Ä–∏–∫–∞–∑' in text_lower:
            return "order"
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –¥—Ä—É–≥–æ–µ
    return "other"

class VectorStoreService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.client = None
        self.collection = None
        self.collection_name = os.getenv("CHROMA_COLLECTION_NAME", "legal_documents")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        self.db_path = os.getenv("CHROMA_DB_PATH", os.path.join(os.getcwd(), "backend", "data", "chroma_db"))
        self.is_initialized = False
        # –ù–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ - —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.use_ai_classification = os.getenv("USE_AI_CLASSIFICATION", "true").lower() == "true"
        self._classification_cache = {}  # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self._ai_classifier = None  # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
    def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            os.makedirs(self.db_path, exist_ok=True)
            
            logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB –≤ {self.db_path}")
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç ChromaDB
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                self.collection = self.client.get_collection(name=self.collection_name)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")
            except Exception:
                # –î–ª—è –≤–µ—Ä—Å–∏–∏ 0.4.18 –∏—Å–ø–æ–ª—å–∑—É–µ–º DefaultEmbeddingFunction
                try:
                    from chromadb.utils import embedding_functions
                    default_ef = embedding_functions.DefaultEmbeddingFunction()
                except ImportError:
                    # –ï—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º None (–¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π)
                    default_ef = None
                
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "–ö–æ–ª–ª–µ–∫—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG"},
                    embedding_function=default_ef
                )
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")
            
            self.is_initialized = True
            logger.info("‚úÖ ChromaDB —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            try:
                count = self.collection.count()
                logger.info(f"üìä –í –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            self.is_initialized = False
    
    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≥–æ—Ç–æ–≤–∞ –ª–∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∫ —Ä–∞–±–æ—Ç–µ"""
        return self.is_initialized and self.client is not None and self.collection is not None
    
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        count = 0
        if self.is_ready():
            try:
                count = self.collection.count()
            except:
                count = 0
                
        return {
            "initialized": self.is_initialized,
            "db_path": self.db_path,
            "collection_name": self.collection_name,
            "documents_count": count
        }
    
    def _validate_embedding(self, embedding) -> list:
        """Validates embedding data before storing"""
        import numpy as np
        
        if embedding is None:
            raise ValueError("Embedding cannot be None")
            
        try:
            # Convert to numpy array for validation
            arr = np.asarray(embedding, dtype=float)
            
            # Check dimensions
            if arr.ndim != 1:
                raise ValueError(f"Embedding must be 1-dimensional, got {arr.ndim}D")
                
            # Check for invalid values
            if np.isnan(arr).any():
                raise ValueError("Embedding contains NaN values")
                
            if np.isinf(arr).any():
                raise ValueError("Embedding contains infinite values")
                
            # Check reasonable size (typical embeddings are 384-1536 dimensions)
            if len(arr) < 50 or len(arr) > 5000:
                raise ValueError(f"Embedding dimension {len(arr)} seems unreasonable")
                
            return arr.tolist()
            
        except (ValueError, TypeError) as e:
            logger.error(f"Embedding validation failed: {e}")
            raise ValueError(f"Invalid embedding data: {e}")
    
    def _validate_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Validates and sanitizes metadata"""
        if not isinstance(metadata, dict):
            raise ValueError("Metadata must be a dictionary")
            
        # Allowed metadata keys to prevent injection
        ALLOWED_KEYS = {
            "source", "article", "valid_from", "valid_to", "edition",
            "title", "filename", "file_name", "file_path", "content_length", "added_at", 
            "part", "item", "document_type", "document_id", "chunk_index",
            "start_position", "end_position", "chunk_length", "total_chunks",
            "processing_timestamp", "source_type", "text_length"
        }
        
        sanitized = {}
        for key, value in metadata.items():
            if key in ALLOWED_KEYS:
                # Ensure values are safe types
                if isinstance(value, (str, int, float, bool, type(None))):
                    sanitized[key] = value
                else:
                    sanitized[key] = str(value)
                    
        return sanitized 
    def add_document(self, 
                    content: str, 
                    metadata: Dict[str, Any],
                    document_id: Optional[str] = None,
                    embedding: Optional[List[float]] = None) -> bool:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        if not self.is_ready():
            logger.info("üîÑ Vector store –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é...")
            self.initialize()
        
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            # Validate inputs
            if not content or not content.strip():
                raise ValueError("Document content cannot be empty")
                
            # Validate and sanitize metadata
            sanitized_metadata = self._validate_metadata(metadata)
            
            # Validate embedding if provided
            if embedding is not None:
                validated_embedding = self._validate_embedding(embedding)
            else:
                validated_embedding = None
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
            if not document_id:
                document_id = str(uuid.uuid4())
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
            if "document_type" not in sanitized_metadata:
                file_name = sanitized_metadata.get("file_name", sanitized_metadata.get("filename", ""))
                doc_type = self._determine_document_type_hybrid(
                    file_name=file_name,
                    document_id=document_id,
                    text_content=content
                )
                sanitized_metadata["document_type"] = doc_type
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            sanitized_metadata.update({
                "added_at": datetime.now().isoformat(),
                "content_length": len(content)
            })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            if validated_embedding:
                self.collection.add(
                    documents=[content],
                    metadatas=[sanitized_metadata],
                    ids=[document_id],
                    embeddings=[validated_embedding]
                )
            else:
                self.collection.add(
                    documents=[content],
                    metadatas=[sanitized_metadata],
                    ids=[document_id]
                )
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {document_id} (–¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return True
            
        except ValueError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
    
    async def add_documents(self, 
                           documents: List[Dict[str, Any]]) -> int:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return 0
            
        added_count = 0
        for doc in documents:
            success = await self.add_document(
                content=doc.get("content", ""),
                metadata=doc.get("metadata", {}),
                document_id=doc.get("id")
            )
            if success:
                added_count += 1
                
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {added_count}/{len(documents)}")
        return added_count
    
    async def search_similar(self, 
                           query: str, 
                           limit: int = 5,
                           min_similarity: float = 0.5,
                           situation_date: Optional[Union[str, date, datetime]] = None) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return []
            
        try:
            # Create date filter if situation_date is provided
            where_filter = None
            if situation_date:
                where_filter = DateUtils.create_date_filter(situation_date)
                logger.info(f"üìÖ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ: {situation_date} -> {where_filter}")
            
            logger.info(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫: '{query[:50]}...' (limit={limit}, min_similarity={min_similarity})")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ
            search_kwargs = {
                "query_texts": [query],
                "n_results": limit,
                "include": ['documents', 'metadatas', 'distances']
            }
            
            if where_filter:
                search_kwargs["where"] = where_filter
                
            results = self.collection.query(**search_kwargs)
            
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {len(results.get('documents', [[]])[0])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            documents = []
            if results['documents'] and len(results['documents']) > 0:
                docs = results['documents'][0]
                metadatas = results['metadatas'][0] if results['metadatas'] else []
                distances = results['distances'][0] if results['distances'] else []
                
                logger.info(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {len(metadatas)} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, {len(distances)} —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π")
                
                for i, content in enumerate(docs):
                    # ChromaDB –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = 1.0 - distance  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
                    
                    logger.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç {i+1}: similarity={similarity:.3f}, distance={distance:.3f}, content_length={len(content)}")
                    
                    if similarity >= min_similarity:
                        documents.append({
                            "content": content,
                            "metadata": metadatas[i] if i < len(metadatas) else {},
                            "similarity": similarity,
                            "distance": distance
                        })
                        logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {i+1} –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É")
                    else:
                        logger.info(f"‚ùå –î–æ–∫—É–º–µ–Ω—Ç {i+1} –Ω–µ –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É (similarity={similarity:.3f} < {min_similarity})")
            
            logger.info(f"üîç –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID"""
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return None
            
        try:
            results = self.collection.get(
                ids=[document_id],
                include=['documents', 'metadatas']
            )
            
            if results['documents'] and len(results['documents']) > 0:
                return {
                    "id": document_id,
                    "content": results['documents'][0],
                    "metadata": results['metadatas'][0] if results['metadatas'] else {}
                }
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            
        return None
    
    async def delete_document(self, document_id: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID"""
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            self.collection.delete(ids=[document_id])
            logger.info(f"üóëÔ∏è –î–æ–∫—É–º–µ–Ω—Ç —É–¥–∞–ª–µ–Ω: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
    
    async def clear_collection(self) -> bool:
        """–û—á–∏—â–∞–µ—Ç –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""
        if not self.is_ready():
            logger.warning("VectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            # –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            self.client.delete_collection(name=self.collection_name)
            try:
                from chromadb.utils import embedding_functions
                default_ef = embedding_functions.DefaultEmbeddingFunction()
            except ImportError:
                default_ef = None
            
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "–ö–æ–ª–ª–µ–∫—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG"},
                embedding_function=default_ef
            )
            logger.info("üóëÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—á–∏—â–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return False
    
    def _determine_document_type_hybrid(
        self,
        file_name: str,
        document_id: str,
        text_content: str = ""
    ) -> str:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
        1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–±—ã—Å—Ç—Ä–æ)
        2. –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã (other) ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º AI (—Ç–æ—á–Ω–æ)
        3. –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{document_id}:{file_name}"
        if cache_key in self._classification_cache:
            return self._classification_cache[cache_key]
        
        # –®–∞–≥ 1: –ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–±—ã—Å—Ç—Ä–æ)
        rule_type = determine_document_type(file_name, document_id, text_content)
        
        # –®–∞–≥ 2: –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω—ã - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–∞–∑—É –∏ –∫—ç—à–∏—Ä—É–µ–º
        if rule_type != 'other' and (
            document_id.startswith('codex_') or 
            '–∫–æ–¥–µ–∫—Å' in file_name.lower() or
            ('—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω' in text_content.lower()[:1000] if text_content else False) or
            ('—Ñ–∑' in text_content.lower()[:500] if text_content else False)
        ):
            self._classification_cache[cache_key] = rule_type
            return rule_type
        
        # –®–∞–≥ 3: –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã (other) –∏ AI –≤–∫–ª—é—á–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º AI
        if rule_type == 'other' and self.use_ai_classification and text_content:
            try:
                # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                if self._ai_classifier is None:
                    try:
                        from .ai_document_classifier import ai_document_classifier
                        self._ai_classifier = ai_document_classifier
                    except ImportError:
                        logger.warning("AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based")
                        self.use_ai_classification = False
                        self._classification_cache[cache_key] = rule_type
                        return rule_type
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                try:
                    from .ai_document_classifier import classify_document_with_ai_sync
                    ai_type = classify_document_with_ai_sync(
                        text_content[:2000],  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        file_name,
                        document_id
                    )
                    
                    if ai_type != 'other':
                        logger.info(f"‚úÖ AI –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–∏–ø: {ai_type} (–±—ã–ª–æ: {rule_type})")
                        self._classification_cache[cache_key] = ai_type
                        return ai_type
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º rule-based —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –∫—ç—à–∏—Ä—É–µ–º
        self._classification_cache[cache_key] = rule_type
        return rule_type

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
vector_store_service = VectorStoreService()
