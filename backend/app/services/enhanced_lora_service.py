"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π LoRA —Å–µ—Ä–≤–∏—Å —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
"""

import logging
import json
import hashlib
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class LoRAMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è LoRA –æ–±—É—á–µ–Ω–∏—è"""
    citation_recall: float
    support_coverage: float
    hallucination_score: float
    legal_consistency: float
    overall_quality: float
    response_time: float
    user_satisfaction: float
    complexity_score: float

@dataclass
class TrainingExample:
    """–ü—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    id: str
    query: str
    response: str
    sources: List[Dict[str, Any]]
    metrics: LoRAMetrics
    is_approved: bool = False
    priority: float = 0.0
    created_at: datetime = None

class EnhancedLoRAService:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π LoRA —Å–µ—Ä–≤–∏—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    def __init__(self):
        self.training_examples: List[TrainingExample] = []
        self.quality_thresholds = {
            "min_citation_recall": 0.7,
            "min_support_coverage": 0.5,
            "max_hallucination_score": 0.3,
            "min_legal_consistency": 0.8,
            "min_overall_quality": 0.6
        }
        self.priority_weights = {
            "citation_recall": 0.3,
            "support_coverage": 0.2,
            "hallucination_score": 0.2,
            "legal_consistency": 0.2,
            "user_satisfaction": 0.1
        }
        self.initialized = False
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ LoRA —Å–µ—Ä–≤–∏—Å–∞...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            await self._load_training_examples()
            
            self.initialized = True
            logger.info("üéâ –£–ª—É—á—à–µ–Ω–Ω—ã–π LoRA —Å–µ—Ä–≤–∏—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LoRA —Å–µ—Ä–≤–∏—Å–∞: {e}")
            raise
    
    async def _load_training_examples(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            # –ü–æ–∫–∞ —á—Ç–æ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            self.training_examples = []
            logger.info("üì• –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤: {e}")
    
    async def add_training_example(
        self,
        query: str,
        response: str,
        sources: List[Dict[str, Any]],
        metrics: LoRAMetrics,
        user_satisfaction: float = None
    ) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        try:
            if not self.initialized:
                await self.initialize()
            
            # –°–æ–∑–¥–∞–µ–º ID –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            example_id = f"example_{int(datetime.now().timestamp())}_{hashlib.md5(query.encode()).hexdigest()[:8]}"
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å user_satisfaction
            if user_satisfaction is not None:
                metrics.user_satisfaction = user_satisfaction
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            priority = self._calculate_priority(metrics)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            is_approved = self._check_quality_thresholds(metrics)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä
            example = TrainingExample(
                id=example_id,
                query=query,
                response=response,
                sources=sources,
                metrics=metrics,
                is_approved=is_approved,
                priority=priority,
                created_at=datetime.now()
            )
            
            self.training_examples.append(example)
            
            logger.info(f"‚úÖ –ü—Ä–∏–º–µ—Ä –¥–æ–±–∞–≤–ª–µ–Ω: {example_id} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority:.2f}, –æ–¥–æ–±—Ä–µ–Ω: {is_approved})")
            
            return example_id
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞: {e}")
            raise
    
    def _calculate_priority(self, metrics: LoRAMetrics) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
            priority = 0.0
            
            # Citation recall (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
            priority += metrics.citation_recall * self.priority_weights["citation_recall"]
            
            # Support coverage (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
            priority += metrics.support_coverage * self.priority_weights["support_coverage"]
            
            # Hallucination score (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
            priority += (1.0 - metrics.hallucination_score) * self.priority_weights["hallucination_score"]
            
            # Legal consistency (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
            priority += metrics.legal_consistency * self.priority_weights["legal_consistency"]
            
            # User satisfaction (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
            priority += metrics.user_satisfaction * self.priority_weights["user_satisfaction"]
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
            if metrics.overall_quality > 0.8:
                priority += 0.1  # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            
            if metrics.complexity_score > 0.7:
                priority += 0.05  # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–æ–∂–Ω—ã–µ –∫–µ–π—Å—ã
            
            return min(1.0, priority)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 1.0
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: {e}")
            return 0.5
    
    def _check_quality_thresholds(self, metrics: LoRAMetrics) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–æ—Ä–æ–≥–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            return (
                metrics.citation_recall >= self.quality_thresholds["min_citation_recall"] and
                metrics.support_coverage >= self.quality_thresholds["min_support_coverage"] and
                metrics.hallucination_score <= self.quality_thresholds["max_hallucination_score"] and
                metrics.legal_consistency >= self.quality_thresholds["min_legal_consistency"] and
                metrics.overall_quality >= self.quality_thresholds["min_overall_quality"]
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return False
    
    async def get_training_batch(
        self,
        batch_size: int = 100,
        min_quality: float = 0.6,
        include_unapproved: bool = False
    ) -> List[TrainingExample]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if not self.initialized:
                await self.initialize()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            filtered_examples = []
            for example in self.training_examples:
                if example.metrics.overall_quality >= min_quality:
                    if include_unapproved or example.is_approved:
                        filtered_examples.append(example)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            filtered_examples.sort(key=lambda x: x.priority, reverse=True)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–∏–º–µ—Ä–æ–≤
            batch = filtered_examples[:batch_size]
            
            logger.info(f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –±–∞—Ç—á: {len(batch)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–∫–∞—á–µ—Å—Ç–≤–æ >= {min_quality})")
            
            return batch
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞—Ç—á–∞: {e}")
            return []
    
    async def get_priority_examples(self, limit: int = 50) -> List[TrainingExample]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if not self.initialized:
                await self.initialize()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            sorted_examples = sorted(self.training_examples, key=lambda x: x.priority, reverse=True)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–∏–º–µ—Ä–æ–≤
            priority_examples = sorted_examples[:limit]
            
            logger.info(f"‚≠ê –ü–æ–ª—É—á–µ–Ω–æ {len(priority_examples)} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            return priority_examples
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {e}")
            return []
    
    async def get_quality_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤"""
        try:
            if not self.initialized:
                await self.initialize()
            
            if not self.training_examples:
                return {
                    "total_examples": 0,
                    "approved_examples": 0,
                    "average_quality": 0.0,
                    "quality_distribution": {}
                }
            
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_examples = len(self.training_examples)
            approved_examples = sum(1 for ex in self.training_examples if ex.is_approved)
            
            # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            avg_metrics = {
                "citation_recall": np.mean([ex.metrics.citation_recall for ex in self.training_examples]),
                "support_coverage": np.mean([ex.metrics.support_coverage for ex in self.training_examples]),
                "hallucination_score": np.mean([ex.metrics.hallucination_score for ex in self.training_examples]),
                "legal_consistency": np.mean([ex.metrics.legal_consistency for ex in self.training_examples]),
                "overall_quality": np.mean([ex.metrics.overall_quality for ex in self.training_examples]),
                "user_satisfaction": np.mean([ex.metrics.user_satisfaction for ex in self.training_examples])
            }
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_ranges = {
                "high": sum(1 for ex in self.training_examples if ex.metrics.overall_quality >= 0.8),
                "medium": sum(1 for ex in self.training_examples if 0.6 <= ex.metrics.overall_quality < 0.8),
                "low": sum(1 for ex in self.training_examples if ex.metrics.overall_quality < 0.6)
            }
            
            return {
                "total_examples": total_examples,
                "approved_examples": approved_examples,
                "approval_rate": approved_examples / total_examples if total_examples > 0 else 0,
                "average_quality": avg_metrics["overall_quality"],
                "average_metrics": avg_metrics,
                "quality_distribution": quality_ranges,
                "priority_distribution": {
                    "high": sum(1 for ex in self.training_examples if ex.priority >= 0.8),
                    "medium": sum(1 for ex in self.training_examples if 0.5 <= ex.priority < 0.8),
                    "low": sum(1 for ex in self.training_examples if ex.priority < 0.5)
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}
    
    async def update_quality_thresholds(self, new_thresholds: Dict[str, float]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            for key, value in new_thresholds.items():
                if key in self.quality_thresholds:
                    self.quality_thresholds[key] = value
                    logger.info(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω –ø–æ—Ä–æ–≥ {key}: {value}")
            
            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            for example in self.training_examples:
                example.is_approved = self._check_quality_thresholds(example.metrics)
            
            logger.info("‚úÖ –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤: {e}")
    
    async def export_training_data(
        self,
        format: str = "json",
        include_metrics: bool = True
    ) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if not self.initialized:
                await self.initialize()
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–¥–æ–±—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            approved_examples = [ex for ex in self.training_examples if ex.is_approved]
            
            if format == "json":
                data = []
                for example in approved_examples:
                    item = {
                        "id": example.id,
                        "query": example.query,
                        "response": example.response,
                        "sources": example.sources,
                        "created_at": example.created_at.isoformat(),
                        "priority": example.priority
                    }
                    
                    if include_metrics:
                        item["metrics"] = {
                            "citation_recall": example.metrics.citation_recall,
                            "support_coverage": example.metrics.support_coverage,
                            "hallucination_score": example.metrics.hallucination_score,
                            "legal_consistency": example.metrics.legal_consistency,
                            "overall_quality": example.metrics.overall_quality,
                            "user_satisfaction": example.metrics.user_satisfaction,
                            "complexity_score": example.metrics.complexity_score
                        }
                    
                    data.append(item)
                
                return json.dumps(data, ensure_ascii=False, indent=2)
            
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return ""
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "status": "enhanced_lora_operational",
            "initialized": self.initialized,
            "total_examples": len(self.training_examples),
            "approved_examples": sum(1 for ex in self.training_examples if ex.is_approved),
            "quality_thresholds": self.quality_thresholds,
            "priority_weights": self.priority_weights
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
enhanced_lora_service = EnhancedLoRAService()
