"""
RAG (Retrieval-Augmented Generation) —Å–µ—Ä–≤–∏—Å
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤ AI –º–æ–¥–µ–ª—å—é
–¢–µ–ø–µ—Ä—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Ñ–∞–∫—Ç-—á–µ–∫–∏–Ω–≥–∞ –∏ explainability
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from .embeddings_service import embeddings_service
from .vector_store_service import vector_store_service
from .document_service import document_service
from .unified_llm_service import unified_llm_service
from .integrated_rag_service import integrated_rag_service
from .simple_expert_rag import simple_expert_rag

logger = logging.getLogger(__name__)

class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è RAG (Retrieval-Augmented Generation)"""
    
    def __init__(self):
        self.min_similarity_threshold = -1.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã)
        self.max_context_documents = 5       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        self.max_context_length = 4000       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        self.use_enhanced_search = True      # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        self.enable_fact_checking = True     # –í–∫–ª—é—á–∏—Ç—å —Ñ–∞–∫—Ç-—á–µ–∫–∏–Ω–≥
        self.enable_explainability = True    # –í–∫–ª—é—á–∏—Ç—å explainability
        
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "embeddings_ready": embeddings_service.is_ready(),
            "vector_store_ready": vector_store_service.is_ready(),
            "ai_model_ready": unified_llm_service.is_model_loaded(),
            "min_similarity_threshold": self.min_similarity_threshold,
            "max_context_documents": self.max_context_documents,
            "max_context_length": self.max_context_length,
            "embeddings_status": embeddings_service.get_status(),
            "vector_store_status": vector_store_service.get_status(),
            "enhancements": {
                "enhanced_search_enabled": self.use_enhanced_search,
                "fact_checking_enabled": self.enable_fact_checking,
                "explainability_enabled": self.enable_explainability,
                "integrated_rag_status": integrated_rag_service.get_status() if hasattr(integrated_rag_service, 'get_status') else {}
            }
        }
    
    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≥–æ—Ç–æ–≤–∞ –ª–∏ RAG —Å–∏—Å—Ç–µ–º–∞ –∫ —Ä–∞–±–æ—Ç–µ"""
        return (
            embeddings_service.is_ready() and 
            vector_store_service.is_ready() and 
            unified_llm_service.is_model_loaded()
        )
    
    async def retrieve_relevant_documents(self, query: str) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º simple_expert_rag –¥–ª—è –ø–æ–∏—Å–∫–∞
            if not simple_expert_rag.initialized:
                await simple_expert_rag.initialize()
            
            # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ simple_expert_rag
            search_results = await simple_expert_rag.search_documents(
                query=query,
                top_k=self.max_context_documents * 2
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            relevant_docs = []
            total_length = 0
            
            for result in search_results:
                similarity = result.get('final_score', 0.0)
                content = result.get('content', '')
                metadata = result.get('metadata', {})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
                if similarity < self.min_similarity_threshold:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ
                if total_length + len(content) > self.max_context_length:
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é
                    remaining_length = self.max_context_length - total_length
                    if remaining_length > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–µ–∑–Ω–∞—è –¥–ª–∏–Ω–∞
                        content = content[:remaining_length] + "..."
                        relevant_docs.append({
                            'content': content,
                            'similarity': similarity,
                            'metadata': metadata
                        })
                    break
                
                relevant_docs.append({
                    'content': content,
                    'similarity': similarity,
                    'metadata': metadata
                })
                total_length += len(content)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if len(relevant_docs) >= self.max_context_documents:
                    break
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs)} –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")
            
            return relevant_docs
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    def _build_context_prompt(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents:
            return unified_llm_service.create_legal_prompt(query)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        context_parts.append("=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ===")
        
        for i, doc in enumerate(documents, 1):
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            similarity = doc.get('similarity', 0.0)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            source_info = ""
            if 'title' in metadata:
                source_info = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {metadata['title']}"
            elif 'filename' in metadata:
                source_info = f"–§–∞–π–ª: {metadata['filename']}"
            else:
                source_info = f"–î–æ–∫—É–º–µ–Ω—Ç {i}"
            
            context_parts.append(f"\n--- {source_info} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.2f}) ---")
            context_parts.append(content.strip())
        
        context_parts.append("\n=== –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ===\n")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
        context = "\n".join(context_parts)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞ Saiga —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        return unified_llm_service.create_legal_prompt(query, context=context)
    
    async def generate_rag_response(self, query: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG (—Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –∏–ª–∏ –±–µ–∑)"""
        if not self.is_ready():
            logger.warning("RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞")
            return {
                "success": False,
                "error": "RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞",
                "response": "",
                "sources": [],
                "context_used": False
            }
        
        try:
            start_time = datetime.now()
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
            if self.use_enhanced_search:
                try:
                    return await self._generate_enhanced_response(query, start_time)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –æ–±—ã—á–Ω—ã–π: {e}")
                    self.use_enhanced_search = False
            
            # –û–±—ã—á–Ω—ã–π RAG –ø–æ–∏—Å–∫
            return await self._generate_standard_response(query, start_time)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RAG –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": "",
                "sources": [],
                "context_used": False
            }
    
    async def _generate_enhanced_response(self, query: str, start_time: datetime) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π RAG –ø–æ–∏—Å–∫ –¥–ª—è: '{query[:50]}...'")
        
        # 1. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        search_data = await integrated_rag_service.search_with_enhancements(
            query=query,
            use_enhanced=True,
            top_k=self.max_context_documents * 2,
            rerank_top_k=self.max_context_documents
        )
        
        if not search_data.get("success"):
            raise Exception("–û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
        
        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        context_documents = search_data.get("context_documents", [])
        if context_documents:
            enhanced_prompt = self._build_context_prompt(query, context_documents)
            context_used = True
            logger.info(f"üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        else:
            enhanced_prompt = unified_llm_service.create_legal_prompt(query)
            context_used = False
            logger.info("üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é AI –º–æ–¥–µ–ª–∏")
        response_text = ""
        async for chunk in unified_llm_service.generate_response(
            prompt=enhanced_prompt,
            max_tokens=2500,
            stream=True
        ):
            response_text += chunk
        
        if not response_text:
            raise Exception("AI –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç")
        
        # 4. –£–ª—É—á—à–µ–Ω–∏—è (—Ñ–∞–∫—Ç-—á–µ–∫–∏–Ω–≥ –∏ explainability)
        enhanced_result = await integrated_rag_service.generate_enhanced_response(
            query=query,
            search_data=search_data,
            response_text=response_text,
            enable_fact_checking=self.enable_fact_checking,
            enable_explainability=self.enable_explainability
        )
        
        # 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources = self._prepare_sources_from_search_data(search_data)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π RAG –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {processing_time:.2f} —Å–µ–∫")
        
        return {
            "success": True,
            "response": response_text,
            "sources": sources,
            "context_used": context_used,
            "documents_found": len(context_documents),
            "processing_time": processing_time,
            "query": query,
            "enhancements": enhanced_result.get("enhancements", {}),
            "search_type": search_data.get("search_type", "enhanced")
        }
    
    async def _generate_standard_response(self, query: str, start_time: datetime) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RAG –æ—Ç–≤–µ—Ç–∞"""
        logger.info(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG –ø–æ–∏—Å–∫ –¥–ª—è: '{query[:50]}...'")
        
        # –®–∞–≥ 1: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = await self.retrieve_relevant_documents(query)
        
        # –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
        if documents:
            enhanced_prompt = self._build_context_prompt(query, documents)
            context_used = True
            logger.info(f"üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        else:
            enhanced_prompt = unified_llm_service.create_legal_prompt(query)
            context_used = False
            logger.info("üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é AI –º–æ–¥–µ–ª–∏")
        response_text = ""
        async for chunk in unified_llm_service.generate_response(
            prompt=enhanced_prompt,
            max_tokens=2500,
            stream=True
        ):
            response_text += chunk
        
        if not response_text:
            return {
                "success": False,
                "error": "AI –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç",
                "response": "",
                "sources": [],
                "context_used": context_used
            }
        
        # –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources = []
        for doc in documents:
            metadata = doc.get('metadata', {})
            source = {
                "similarity": doc.get('similarity', 0.0),
                "content_preview": doc.get('content', '')[:200] + "..." if len(doc.get('content', '')) > 200 else doc.get('content', ''),
                "title": metadata.get('title') or metadata.get('filename') or metadata.get('source', '–ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                "source": metadata.get('source', '–ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                "article": metadata.get('article', ''),
                "part": metadata.get('part', ''),
                "item": metadata.get('item', ''),
                "edition": metadata.get('edition', '')
            }
            
            if 'file_path' in metadata:
                source['file_path'] = metadata['file_path']
            
            sources.append(source)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {processing_time:.2f} —Å–µ–∫")
        
        return {
            "success": True,
            "response": response_text,
            "sources": sources,
            "context_used": context_used,
            "documents_found": len(documents),
            "processing_time": processing_time,
            "query": query,
            "search_type": "standard"
        }
    
    def _prepare_sources_from_search_data(self, search_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        sources = []
        search_results = search_data.get("search_results", [])
        
        for result in search_results:
            if hasattr(result, 'metadata'):
                metadata = result.metadata
                source = {
                    "similarity": result.final_score,
                    "content_preview": result.content[:200] + "..." if len(result.content) > 200 else result.content,
                    "title": metadata.get('source', '–î–æ–∫—É–º–µ–Ω—Ç'),
                    "article": metadata.get('article', ''),
                    "part": metadata.get('part', ''),
                    "item": metadata.get('item', ''),
                    "edition": metadata.get('edition', ''),
                    "url": metadata.get('url', '')
                }
                sources.append(source)
        
        return sources
    
    async def stream_rag_response(self, query: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
        if not self.is_ready():
            yield {
                "type": "error",
                "content": "RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞"
            }
            return
        
        try:
            # –®–∞–≥ 1: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            yield {
                "type": "status",
                "content": "–ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã..."
            }
            
            documents = await self.retrieve_relevant_documents(query)
            
            yield {
                "type": "status", 
                "content": f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}"
            }
            
            # –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
            if documents:
                enhanced_prompt = self._build_context_prompt(query, documents)
                context_used = True
            else:
                enhanced_prompt = unified_llm_service.create_legal_prompt(query)
                context_used = False
            
            yield {
                "type": "context_info",
                "context_used": context_used,
                "documents_count": len(documents)
            }
            
            # –®–∞–≥ 3: –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            yield {
                "type": "status",
                "content": "–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç..."
            }
            
            async for chunk in unified_llm_service.generate_response(enhanced_prompt, max_tokens=2500, stream=True):
                yield {
                    "type": "chunk",
                    "content": chunk
                }
            
            # –®–∞–≥ 4: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            if documents:
                sources = []
                for doc in documents:
                    metadata = doc.get('metadata', {})
                    source = {
                        "similarity": doc.get('similarity', 0.0),
                        "title": metadata.get('title') or metadata.get('filename') or metadata.get('source', '–ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                        "content_preview": doc.get('content', '')[:200] + "..." if len(doc.get('content', '')) > 200 else doc.get('content', ''),
                        "source": metadata.get('source', '–ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                        "article": metadata.get('article', ''),
                        "part": metadata.get('part', ''),
                        "item": metadata.get('item', ''),
                        "edition": metadata.get('edition', '')
                    }
                    sources.append(source)
                
                yield {
                    "type": "sources",
                    "sources": sources
                }
            
            yield {
                "type": "complete"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RAG –æ—Ç–≤–µ—Ç–∞: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
rag_service = RAGService()