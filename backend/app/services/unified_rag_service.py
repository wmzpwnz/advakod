"""
Unified RAG Service - –µ–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è Retrieval-Augmented Generation
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å enhanced_rag_service.py, integrated_rag_service.py, simple_expert_rag.py
"""

import logging
import time
import asyncio
import hashlib
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass
from datetime import datetime, date
import numpy as np
from functools import lru_cache

from ..core.config import settings
from ..core.cache import cache_service
from .vector_store_service import vector_store_service
from .embeddings_service import embeddings_service

logger = logging.getLogger(__name__)


@dataclass
class DocumentSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è RAG"""
    content: str
    metadata: Dict[str, Any]
    similarity: float
    relevance: float
    source_type: str  # "semantic", "keyword", "hybrid"
    chunk_id: Optional[str] = None


@dataclass
class RAGResponse:
    """–û—Ç–≤–µ—Ç RAG —Å–∏—Å—Ç–µ–º—ã"""
    answer: str
    sources: List[DocumentSource]
    confidence: float
    search_time: float
    generation_time: float
    total_time: float
    search_metadata: Dict[str, Any]


@dataclass
class RAGMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RAG"""
    search_time_avg: float
    generation_time_avg: float
    cache_hit_rate: float
    vector_store_size: int
    embedding_generation_time: float
    total_searches: int
    successful_searches: int
    failed_searches: int


class SearchStrategy:
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞"""
    SEMANTIC_ONLY = "semantic_only"
    KEYWORD_ONLY = "keyword_only"
    HYBRID = "hybrid"
    AUTO = "auto"


class UnifiedRAGService:
    """–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è Retrieval-Augmented Generation"""
    
    def __init__(self):
        self.vector_store = vector_store_service
        self.embeddings_service = embeddings_service
        self.cache_service = cache_service
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.search_config = {
            "max_results": getattr(settings, "RAG_MAX_RESULTS", 20),
            "similarity_threshold": getattr(settings, "RAG_SIMILARITY_THRESHOLD", 0.7),
            "rerank_top_k": getattr(settings, "RAG_RERANK_TOP_K", 5),
            "context_window": getattr(settings, "RAG_CONTEXT_WINDOW", 4000),
            "chunk_overlap": getattr(settings, "RAG_CHUNK_OVERLAP", 200),
            "enable_reranking": getattr(settings, "RAG_ENABLE_RERANKING", True),
            "enable_hybrid_search": getattr(settings, "RAG_ENABLE_HYBRID_SEARCH", True)
        }
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._search_cache = {}
        self._cache_ttl = 300  # 5 –º–∏–Ω—É—Ç
        self._max_cache_size = 200
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_searches": 0,
            "successful_searches": 0,
            "failed_searches": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "average_search_time": 0.0,
            "average_generation_time": 0.0,
            "average_embedding_time": 0.0,
            "last_search_time": 0.0,
            "vector_store_size": 0
        }
        
        # –ò—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        self._search_history = []
        self._max_history = 1000
        
        # –§–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._initialized = False
        
        # RRF –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self._rrf_k = 60  # –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è Reciprocal Rank Fusion
        
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedRAGService...")
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã
            if not self.vector_store.is_ready():
                await asyncio.to_thread(self.vector_store.initialize)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º embeddings service
            if not hasattr(self.embeddings_service, '_model_loaded') or not self.embeddings_service._model_loaded:
                await asyncio.to_thread(self.embeddings_service.load_model)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            await self._update_vector_store_stats()
            
            self._initialized = True
            duration = time.time() - start_time
            
            logger.info("‚úÖ UnifiedRAGService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ %.2f —Å–µ–∫—É–Ω–¥", duration)
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ UnifiedRAGService: %s", e)
            self._initialized = False
            raise

    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞"""
        return (self._initialized and 
                self.vector_store.is_ready() and
                hasattr(self.embeddings_service, '_model_loaded') and 
                self.embeddings_service._model_loaded)

    async def search_and_generate(
        self,
        query: str,
        max_results: int = 10,
        similarity_threshold: float = 0.7,
        situation_date: Optional[Union[str, date, datetime]] = None,
        strategy: str = SearchStrategy.HYBRID,
        enable_reranking: bool = True,
        context_window: int = None
    ) -> RAGResponse:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        
        if not self.is_ready():
            logger.warning("UnifiedRAGService –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
            return RAGResponse(
                answer="–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                sources=[],
                confidence=0.0,
                search_time=0.0,
                generation_time=0.0,
                total_time=0.0,
                search_metadata={"error": "service_not_ready"}
            )
        
        total_start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = self._generate_cache_key(query, max_results, similarity_threshold, 
                                               str(situation_date), strategy)
            
            cached_response = await self._get_cached_response(cache_key)
            if cached_response:
                self._stats["cache_hits"] += 1
                logger.info("üéØ –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ –¥–ª—è RAG –∑–∞–ø—Ä–æ—Å–∞: '%s'", query[:50])
                return cached_response
            
            self._stats["cache_misses"] += 1
            
            # 1. –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            search_start_time = time.time()
            sources = await self._search_documents(
                query, max_results, similarity_threshold, situation_date, strategy
            )
            search_time = time.time() - search_start_time
            
            # 2. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if enable_reranking and len(sources) > 1:
                sources = await self._rerank_sources(query, sources)
            
            # 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_window = context_window or self.search_config["context_window"]
            context = self._build_context(sources, context_window)
            
            # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ LLM)
            generation_start_time = time.time()
            answer = await self._generate_answer(query, context, sources)
            generation_time = time.time() - generation_start_time
            
            # 5. –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(sources, query)
            
            total_time = time.time() - total_start_time
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç
            response = RAGResponse(
                answer=answer,
                sources=sources[:self.search_config["rerank_top_k"]],
                confidence=confidence,
                search_time=search_time,
                generation_time=generation_time,
                total_time=total_time,
                search_metadata={
                    "strategy": strategy,
                    "total_sources": len(sources),
                    "reranking_enabled": enable_reranking,
                    "situation_date": str(situation_date) if situation_date else None
                }
            )
            
            # –ö—ç—à–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            await self._cache_response(cache_key, response)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_search_stats(True, search_time, generation_time)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self._add_to_history(query, response)
            
            logger.info("‚úÖ RAG –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å %.2f, –≤—Ä–µ–º—è %.2fs", 
                       len(sources), confidence, total_time)
            
            return response
            
        except Exception as e:
            total_time = time.time() - total_start_time
            self._update_search_stats(False, 0, 0)
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ RAG –ø–æ–∏—Å–∫–µ: %s", e)
            
            return RAGResponse(
                answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
                sources=[],
                confidence=0.0,
                search_time=0.0,
                generation_time=0.0,
                total_time=total_time,
                search_metadata={"error": str(e)}
            )

    async def _search_documents(
        self,
        query: str,
        max_results: int,
        similarity_threshold: float,
        situation_date: Optional[Union[str, date, datetime]],
        strategy: str
    ) -> List[DocumentSource]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        if strategy == SearchStrategy.SEMANTIC_ONLY:
            return await self._semantic_search(query, max_results, similarity_threshold, situation_date)
        elif strategy == SearchStrategy.KEYWORD_ONLY:
            return await self._keyword_search(query, max_results, situation_date)
        elif strategy == SearchStrategy.HYBRID:
            return await self._hybrid_search(query, max_results, similarity_threshold, situation_date)
        else:  # AUTO
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞
            if len(query.split()) <= 3:
                return await self._keyword_search(query, max_results, situation_date)
            else:
                return await self._hybrid_search(query, max_results, similarity_threshold, situation_date)

    async def _semantic_search(
        self,
        query: str,
        max_results: int,
        similarity_threshold: float,
        situation_date: Optional[Union[str, date, datetime]]
    ) -> List[DocumentSource]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            results = await self.vector_store.search_similar(
                query=query,
                limit=max_results,
                min_similarity=similarity_threshold,
                situation_date=situation_date
            )
            
            sources = []
            for result in results:
                source = DocumentSource(
                    content=result["content"],
                    metadata=result["metadata"],
                    similarity=result["similarity"],
                    relevance=result["similarity"],  # –î–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ similarity = relevance
                    source_type="semantic",
                    chunk_id=result["metadata"].get("chunk_id")
                )
                sources.append(source)
            
            logger.info("üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ %d –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", len(sources))
            return sources
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: %s", e)
            return []

    async def _keyword_search(
        self,
        query: str,
        max_results: int,
        situation_date: Optional[Union[str, date, datetime]]
    ) -> List[DocumentSource]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)"""
        
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –Ω–∏–∑–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
            results = await self.vector_store.search_similar(
                query=query,
                limit=max_results * 2,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                min_similarity=0.3,  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è keyword –ø–æ–∏—Å–∫–∞
                situation_date=situation_date
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            query_words = set(query.lower().split())
            filtered_results = []
            
            for result in results:
                content_words = set(result["content"].lower().split())
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                word_overlap = len(query_words.intersection(content_words))
                if word_overlap > 0:
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å–ª–æ–≤
                    relevance = word_overlap / len(query_words)
                    
                    source = DocumentSource(
                        content=result["content"],
                        metadata=result["metadata"],
                        similarity=result["similarity"],
                        relevance=relevance,
                        source_type="keyword",
                        chunk_id=result["metadata"].get("chunk_id")
                    )
                    filtered_results.append(source)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            filtered_results.sort(key=lambda x: x.relevance, reverse=True)
            final_results = filtered_results[:max_results]
            
            logger.info("üîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: –Ω–∞–π–¥–µ–Ω–æ %d –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", len(final_results))
            return final_results
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: %s", e)
            return []

    async def _hybrid_search(
        self,
        query: str,
        max_results: int,
        similarity_threshold: float,
        situation_date: Optional[Union[str, date, datetime]]
    ) -> List[DocumentSource]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RRF (Reciprocal Rank Fusion)"""
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±–∞ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            semantic_task = self._semantic_search(query, max_results, similarity_threshold, situation_date)
            keyword_task = self._keyword_search(query, max_results, situation_date)
            
            semantic_results, keyword_results = await asyncio.gather(semantic_task, keyword_task)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º RRF –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            combined_results = self._apply_rrf(semantic_results, keyword_results)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = combined_results[:max_results]
            
            logger.info("üîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π=%d, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞=%d, –∏—Ç–æ–≥–æ=%d", 
                       len(semantic_results), len(keyword_results), len(final_results))
            
            return final_results
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: %s", e)
            # Fallback –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            return await self._semantic_search(query, max_results, similarity_threshold, situation_date)

    def _apply_rrf(self, semantic_results: List[DocumentSource], keyword_results: List[DocumentSource]) -> List[DocumentSource]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç Reciprocal Rank Fusion –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        combined_scores = {}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for rank, source in enumerate(semantic_results, 1):
            content_hash = hashlib.md5(source.content.encode()).hexdigest()
            if content_hash not in combined_scores:
                combined_scores[content_hash] = {
                    "source": source,
                    "rrf_score": 0.0
                }
            combined_scores[content_hash]["rrf_score"] += 1.0 / (self._rrf_k + rank)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for rank, source in enumerate(keyword_results, 1):
            content_hash = hashlib.md5(source.content.encode()).hexdigest()
            if content_hash not in combined_scores:
                combined_scores[content_hash] = {
                    "source": source,
                    "rrf_score": 0.0
                }
            combined_scores[content_hash]["rrf_score"] += 1.0 / (self._rrf_k + rank)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        final_results = []
        for item in combined_scores.values():
            source = item["source"]
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            source.source_type = "hybrid"
            source.relevance = item["rrf_score"]
            final_results.append(source)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ RRF score
        final_results.sort(key=lambda x: x.relevance, reverse=True)
        
        return final_results

    async def _rerank_sources(self, query: str, sources: List[DocumentSource]) -> List[DocumentSource]:
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)"""
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å –∑–∞–ø—Ä–æ—Å–æ–º
            query_words = set(query.lower().split())
            
            for source in sources:
                content_words = set(source.content.lower().split())
                word_overlap = len(query_words.intersection(content_words))
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ–º —Å–ª–æ–≤
                rerank_score = (source.relevance * 0.7) + (word_overlap / len(query_words) * 0.3)
                source.relevance = rerank_score
            
            # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ–º
            sources.sort(key=lambda x: x.relevance, reverse=True)
            
            logger.info("üîÑ –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(sources))
            return sources
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: %s", e)
            return sources

    def _build_context(self, sources: List[DocumentSource], max_length: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        context_parts = []
        current_length = 0
        
        for i, source in enumerate(sources):
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_header = f"\n=== –ò–°–¢–û–ß–ù–ò–ö {i+1} ===\n"
            source_text = source_header + source.content + "\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º –ª–∏ –ª–∏–º–∏—Ç
            if current_length + len(source_text) > max_length:
                # –û–±—Ä–µ–∑–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                remaining_space = max_length - current_length - len(source_header)
                if remaining_space > 100:  # –ú–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    truncated_content = source.content[:remaining_space] + "..."
                    context_parts.append(source_header + truncated_content)
                break
            
            context_parts.append(source_text)
            current_length += len(source_text)
        
        context = "".join(context_parts)
        logger.info("üìù –ü–æ—Å—Ç—Ä–æ–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π %d —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", 
                   len(context), len(context_parts))
        
        return context

    async def _generate_answer(self, query: str, context: str, sources: List[DocumentSource]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)"""
        
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ LLM
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        if not sources:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É."
        
        # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        answer_parts = [
            f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ {len(sources)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:",
            ""
        ]
        
        for i, source in enumerate(sources[:3], 1):  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            metadata = source.metadata
            source_info = f"{i}. "
            
            if "article" in metadata:
                source_info += f"–°—Ç–∞—Ç—å—è {metadata['article']}"
            if "title" in metadata:
                source_info += f" - {metadata['title']}"
            if "source" in metadata:
                source_info += f" ({metadata['source']})"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫—É—é –≤—ã–¥–µ—Ä–∂–∫—É
            content_preview = source.content[:200] + "..." if len(source.content) > 200 else source.content
            source_info += f"\n   {content_preview}\n"
            
            answer_parts.append(source_info)
        
        answer_parts.extend([
            "",
            "‚ö†Ô∏è –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–π –ø—Ä–∞–≤–æ–≤–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —é—Ä–∏—Å—Ç—É.",
            "",
            f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)} | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self._calculate_confidence(sources, query):.1%}"
        ])
        
        return "\n".join(answer_parts)

    def _calculate_confidence(self, sources: List[DocumentSource], query: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ"""
        
        if not sources:
            return 0.0
        
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        base_confidence = min(len(sources) / 5.0, 1.0)  # –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏ 5+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        
        # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        avg_relevance = sum(source.relevance for source in sources) / len(sources)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã
        confidence = (base_confidence * 0.4) + (avg_relevance * 0.6)
        
        return min(confidence, 1.0)

    async def add_document(
        self,
        content: str,
        metadata: Dict[str, Any],
        document_id: Optional[str] = None,
        chunk_size: int = 1000
    ) -> bool:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        
        if not self.is_ready():
            logger.warning("UnifiedRAGService –Ω–µ –≥–æ—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return False
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._split_document(content, chunk_size)
            
            success_count = 0
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    "chunk_id": f"{document_id or 'doc'}_{i}",
                    "chunk_index": i,
                    "total_chunks": len(chunks)
                })
                
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                success = await self.vector_store.add_document(
                    content=chunk,
                    metadata=chunk_metadata,
                    document_id=chunk_metadata["chunk_id"]
                )
                
                if success:
                    success_count += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            await self._update_vector_store_stats()
            
            logger.info("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω: %d/%d —á–∞–Ω–∫–æ–≤ —É—Å–ø–µ—à–Ω–æ", success_count, len(chunks))
            return success_count == len(chunks)
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: %s", e)
            return False

    def _split_document(self, content: str, chunk_size: int) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        
        if len(content) <= chunk_size:
            return [content]
        
        chunks = []
        overlap = self.search_config["chunk_overlap"]
        start = 0
        
        while start < len(content):
            end = start + chunk_size
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(content):
                # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Ç–æ—á–∫—É –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
                for i in range(end, max(start + chunk_size // 2, end - 100), -1):
                    if content[i] in '.!?\n':
                        end = i + 1
                        break
            
            chunk = content[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            start = end - overlap
            
            if start >= len(content):
                break
        
        return chunks

    def _generate_cache_key(self, query: str, max_results: int, similarity_threshold: float, 
                          situation_date: str, strategy: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        key_data = f"{query}:{max_results}:{similarity_threshold}:{situation_date}:{strategy}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def _get_cached_response(self, cache_key: str) -> Optional[RAGResponse]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞"""
        try:
            if cache_key in self._search_cache:
                timestamp, response = self._search_cache[cache_key]
                if time.time() - timestamp < self._cache_ttl:
                    return response
                else:
                    # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∑–∞–ø–∏—Å—å
                    del self._search_cache[cache_key]
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: %s", e)
        
        return None

    async def _cache_response(self, cache_key: str, response: RAGResponse):
        """–ö—ç—à–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç"""
        try:
            # –û—á–∏—â–∞–µ–º –∫—ç—à –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω
            if len(self._search_cache) >= self._max_cache_size:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
                sorted_items = sorted(self._search_cache.items(), key=lambda x: x[1][0])
                items_to_remove = len(self._search_cache) - self._max_cache_size + 10
                for i in range(items_to_remove):
                    del self._search_cache[sorted_items[i][0]]
            
            self._search_cache[cache_key] = (time.time(), response)
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: %s", e)

    async def _update_vector_store_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        try:
            status = self.vector_store.get_status()
            self._stats["vector_store_size"] = status.get("documents_count", 0)
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: %s", e)

    def _update_search_stats(self, success: bool, search_time: float, generation_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞"""
        self._stats["total_searches"] += 1
        
        if success:
            self._stats["successful_searches"] += 1
        else:
            self._stats["failed_searches"] += 1
        
        self._stats["last_search_time"] = search_time
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∞
        if self._stats["successful_searches"] > 0:
            total_search_time = self._stats["average_search_time"] * (self._stats["successful_searches"] - 1)
            self._stats["average_search_time"] = (total_search_time + search_time) / self._stats["successful_searches"]
            
            total_gen_time = self._stats["average_generation_time"] * (self._stats["successful_searches"] - 1)
            self._stats["average_generation_time"] = (total_gen_time + generation_time) / self._stats["successful_searches"]

    def _add_to_history(self, query: str, response: RAGResponse):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        history_entry = {
            "timestamp": datetime.now(),
            "query": query,
            "sources_count": len(response.sources),
            "confidence": response.confidence,
            "search_time": response.search_time,
            "generation_time": response.generation_time,
            "total_time": response.total_time
        }
        
        self._search_history.append(history_entry)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self._search_history) > self._max_history:
            self._search_history = self._search_history[-self._max_history:]

    def get_metrics(self) -> RAGMetrics:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        cache_hit_rate = 0.0
        if self._stats["total_searches"] > 0:
            cache_hit_rate = self._stats["cache_hits"] / self._stats["total_searches"]
        
        return RAGMetrics(
            search_time_avg=self._stats["average_search_time"],
            generation_time_avg=self._stats["average_generation_time"],
            cache_hit_rate=cache_hit_rate,
            vector_store_size=self._stats["vector_store_size"],
            embedding_generation_time=self._stats["average_embedding_time"],
            total_searches=self._stats["total_searches"],
            successful_searches=self._stats["successful_searches"],
            failed_searches=self._stats["failed_searches"]
        )

    async def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "status": "healthy" if self.is_ready() else "unhealthy",
            "initialized": self._initialized,
            "vector_store_ready": self.vector_store.is_ready(),
            "embeddings_ready": hasattr(self.embeddings_service, '_model_loaded') and self.embeddings_service._model_loaded,
            "cache_size": len(self._search_cache),
            "vector_store_size": self._stats["vector_store_size"],
            "metrics": self.get_metrics().__dict__
        }

    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            "performance": self._stats.copy(),
            "cache": {
                "size": len(self._search_cache),
                "max_size": self._max_cache_size,
                "ttl": self._cache_ttl
            },
            "history": {
                "size": len(self._search_history),
                "max_size": self._max_history
            },
            "config": self.search_config.copy()
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
unified_rag_service = UnifiedRAGService()