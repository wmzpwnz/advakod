"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Tuple, Union
import numpy as np
from dataclasses import dataclass
from functools import lru_cache

from ..core.performance_optimizer import monitor_performance, response_cache
from ..core.security import validate_and_sanitize_query
from .embeddings_service import embeddings_service
from .vector_store_service import vector_store_service

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    content: str
    score: float
    metadata: Dict[str, Any]
    source: str
    relevance: float


@dataclass
class RAGResponse:
    """–û—Ç–≤–µ—Ç RAG —Å–∏—Å—Ç–µ–º—ã"""
    answer: str
    sources: List[SearchResult]
    confidence: float
    processing_time: float
    search_metadata: Dict[str, Any]


class EnhancedRAGService:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self):
        self.embeddings_service = embeddings_service
        self.vector_store_service = vector_store_service
        self.response_cache = response_cache
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.search_config = {
            "max_results": 10,
            "similarity_threshold": 0.7,
            "rerank_top_k": 5,
            "context_window": 2000
        }
        
        # –ö—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings_cache = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_searches": 0,
            "cache_hits": 0,
            "average_search_time": 0.0,
            "average_rerank_time": 0.0
        }
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            logger.info("üîÑ Initializing Enhanced RAG service...")
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å—ã
            await asyncio.to_thread(self.embeddings_service.load_model)
            await asyncio.to_thread(self.vector_store_service.initialize)
            
            duration = time.time() - start_time
            logger.info(f"‚úÖ Enhanced RAG service initialized in {duration:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Enhanced RAG service: {e}")
            raise
    
    @monitor_performance("enhanced_rag_search")
    async def search_legal_documents(
        self, 
        query: str, 
        context: str = None,
        user_id: str = None,
        ip_address: str = None,
        situation_date: Optional[str] = None
    ) -> RAGResponse:
        """–ü–æ–∏—Å–∫ –≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        validation_result = validate_and_sanitize_query(query, user_id, ip_address)
        if not validation_result["is_safe"]:
            return RAGResponse(
                answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.",
                sources=[],
                confidence=0.0,
                processing_time=0.0,
                search_metadata={"error": "unsafe_query"}
            )
        
        sanitized_query = validation_result["sanitized_query"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_response = self.response_cache.get_cached_response(sanitized_query, context)
        if cached_response:
            self.stats["cache_hits"] += 1
            logger.info("üì¶ Cache hit for RAG search")
            return RAGResponse(**cached_response)
        
        start_time = time.time()
        
        try:
            # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –¥–∞—Ç—ã
            semantic_results = await self._semantic_search(sanitized_query, situation_date)
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keyword_results = await self._keyword_search(sanitized_query)
            
            # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results = self._combine_and_rank_results(
                semantic_results, keyword_results
            )
            
            # 4. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            reranked_results = await self._rerank_results(sanitized_query, combined_results)
            
            # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_text = self._build_context(reranked_results)
            
            # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = await self._generate_answer(sanitized_query, context_text)
            
            processing_time = time.time() - start_time
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = RAGResponse(
                answer=answer,
                sources=reranked_results[:self.search_config["rerank_top_k"]],
                confidence=self._calculate_confidence(reranked_results),
                processing_time=processing_time,
                search_metadata={
                    "semantic_results": len(semantic_results),
                    "keyword_results": len(keyword_results),
                    "total_results": len(combined_results),
                    "reranked_results": len(reranked_results)
                }
            )
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.response_cache.cache_response(
                sanitized_query, 
                response.__dict__, 
                context
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats["total_searches"] += 1
            self._update_average_search_time(processing_time)
            
            logger.info(f"‚úÖ Enhanced RAG search completed in {processing_time:.2f}s")
            
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"‚ùå Error in Enhanced RAG search: {e}")
            
            return RAGResponse(
                answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ø—Ä–∞–≤–æ–≤–æ–π –±–∞–∑–µ.",
                sources=[],
                confidence=0.0,
                processing_time=processing_time,
                search_metadata={"error": str(e)}
            )
    
    async def _semantic_search(self, query: str, situation_date: Optional[str] = None) -> List[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º —Å —É—á–µ—Ç–æ–º –¥–∞—Ç—ã"""
        try:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –£–ö –†–§
            enhanced_query = self._enhance_uk_search_query(query)
            
            # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ —Å —É—á–µ—Ç–æ–º –¥–∞—Ç—ã
            search_results = await self.vector_store_service.search_similar(
                query=enhanced_query,
                limit=self.search_config["max_results"],
                min_similarity=0.1,  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
                situation_date=situation_date
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ SearchResult
            results = []
            for result in search_results:
                # –ü–æ–≤—ã—à–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞—Ç–µ–π –£–ö –†–§
                relevance_boost = self._calculate_uk_relevance_boost(query, result["content"])
                
                search_result = SearchResult(
                    content=result["content"],
                    score=float(result["similarity"]) + relevance_boost,
                    metadata=result["metadata"],
                    source=result["metadata"].get("source", "unknown"),
                    relevance=float(result["similarity"]) + relevance_boost
                )
                results.append(search_result)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x.relevance, reverse=True)
            
            return results
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            return []
    
    async def _keyword_search(self, query: str) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(query)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            keyword_query = " ".join(keywords)
            search_results = await self.vector_store_service.search_similar(
                query=keyword_query,
                limit=self.search_config["max_results"],
                min_similarity=0.1
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ SearchResult
            results = []
            for result in search_results:
                search_result = SearchResult(
                    content=result["content"],
                    score=float(result["similarity"]) * 0.8,  # –ù–µ–º–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    metadata=result["metadata"],
                    source=result["metadata"].get("source", "unknown"),
                    relevance=float(result["similarity"]) * 0.8
                )
                results.append(search_result)
            
            return results
            
        except Exception as e:
            logger.error(f"Error in keyword search: {e}")
            return []
    
    def _combine_and_rank_results(
        self, 
        semantic_results: List[SearchResult], 
        keyword_results: List[SearchResult]
    ) -> List[SearchResult]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é RRF (Reciprocal Rank Fusion)"""
        from collections import defaultdict
        
        # RRF –ø–∞—Ä–∞–º–µ—Ç—Ä (k) - –æ–±—ã—á–Ω–æ 60 –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º
        k = 60
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è RRF —Å–∫–æ—Ä–æ–≤
        rrf_scores = defaultdict(float)
        doc_content_map = {}  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç—ã SearchResult
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for rank, result in enumerate(semantic_results):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∫–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            content_key = result.content[:100] if result.content else f"semantic_{rank}"
            
            # RRF —Ñ–æ—Ä–º—É–ª–∞: score = 1 / (k + rank)
            rrf_score = 1.0 / (k + rank + 1)  # +1 –ø–æ—Ç–æ–º—É —á—Ç–æ rank –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
            rrf_scores[content_key] += rrf_score
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç —Å –ª—É—á—à–∏–º –∏—Å—Ö–æ–¥–Ω—ã–º —Å–∫–æ—Ä–æ–º
            if content_key not in doc_content_map or result.score > doc_content_map[content_key].score:
                # –û–±–Ω–æ–≤–ª—è–µ–º relevance –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
                result.relevance = rrf_scores[content_key]
                doc_content_map[content_key] = result
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for rank, result in enumerate(keyword_results):
            content_key = result.content[:100] if result.content else f"keyword_{rank}"
            
            # RRF —Å–∫–æ—Ä –¥–ª—è BM25/keyword —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            rrf_score = 1.0 / (k + rank + 1)
            rrf_scores[content_key] += rrf_score
            
            if content_key not in doc_content_map:
                # –ù–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ keyword –ø–æ–∏—Å–∫–∞
                result.relevance = rrf_scores[content_key]
                doc_content_map[content_key] = result
            else:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                doc_content_map[content_key].relevance = rrf_scores[content_key]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ RRF —Å–∫–æ—Ä—É
        ranked_results = sorted(
            doc_content_map.values(), 
            key=lambda x: x.relevance, 
            reverse=True
        )
        
        logger.info(
            f"üîÑ RRF –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: {len(semantic_results)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö + "
            f"{len(keyword_results)} keyword ‚Üí {len(ranked_results)} –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        )
        
        return ranked_results[:self.search_config["max_results"]]
    
    async def _rerank_results(
        self, 
        query: str, 
        results: List[SearchResult]
    ) -> List[SearchResult]:
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        if not results:
            return results
        
        try:
            start_time = time.time()
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self._get_query_embedding(query)
            
            # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            reranked = []
            for result in results:
                # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                content_embedding = await self._get_query_embedding(result.content)
                
                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = np.dot(query_embedding, content_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(content_embedding)
                )
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                result.relevance = (result.relevance + similarity) / 2
                reranked.append(result)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            reranked.sort(key=lambda x: x.relevance, reverse=True)
            
            rerank_time = time.time() - start_time
            self._update_average_rerank_time(rerank_time)
            
            return reranked
            
        except Exception as e:
            logger.error(f"Error in reranking: {e}")
            return results
    
    def _build_context(self, results: List[SearchResult]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        context_parts = []
        current_length = 0
        
        for result in results:
            content = result.content
            if current_length + len(content) > self.search_config["context_window"]:
                # –û–±—Ä–µ–∑–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
                remaining = self.search_config["context_window"] - current_length
                if remaining > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                    content = content[:remaining] + "..."
                    context_parts.append(content)
                break
            
            context_parts.append(content)
            current_length += len(content)
        
        return "\n\n".join(context_parts)
    
    async def _generate_answer(self, query: str, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        if len(context.strip()) < 200:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –ø—Ä–∞–≤–æ–≤—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."
        
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        return f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∞–≤–æ–≤–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:\n\n{context}\n\n–≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ {len(context.split())} —Å–ª–æ–≤–∞—Ö –∏–∑ –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
    
    def _calculate_confidence(self, results: List[SearchResult]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ"""
        if not results:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_results = results[:3]
        avg_relevance = sum(r.relevance for r in top_results) / len(top_results)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 0-1
        confidence = min(avg_relevance, 1.0)
        
        return confidence
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        import re
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∫—Ç–æ", "–∫–∞–∫–æ–π",
            "–∫–∞–∫–∞—è", "–∫–∞–∫–æ–µ", "–∫–∞–∫–∏–µ", "—ç—Ç–æ", "—Ç–æ", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è",
            "–æ—Ç", "–¥–æ", "–∏–∑", "–∫", "—É", "–æ", "–æ–±", "–ø—Ä–æ", "–ø—Ä–∏", "–±–µ–∑", "—á–µ—Ä–µ–∑"
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ë]+\b', query.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        keywords = [
            word for word in words 
            if word not in stop_words and len(word) > 2
        ]
        
        return keywords[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    @lru_cache(maxsize=1000)
    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            embedding = await self.embeddings_service.encode_text(query)
            if embedding is None:
                return np.zeros(384)  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return np.array(embedding)
        except Exception as e:
            logger.error(f"Error getting query embedding: {e}")
            return np.zeros(384)  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _update_average_search_time(self, new_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞"""
        total_searches = self.stats["total_searches"]
        if total_searches == 1:
            self.stats["average_search_time"] = new_time
        else:
            alpha = 0.1
            self.stats["average_search_time"] = (
                alpha * new_time + (1 - alpha) * self.stats["average_search_time"]
            )
    
    def _update_average_rerank_time(self, new_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if self.stats["average_rerank_time"] == 0:
            self.stats["average_rerank_time"] = new_time
        else:
            alpha = 0.1
            self.stats["average_rerank_time"] = (
                alpha * new_time + (1 - alpha) * self.stats["average_rerank_time"]
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RAG —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "rag_stats": self.stats,
            "search_config": self.search_config,
            "cache_stats": self.response_cache.get_stats()
        }
    
    def _enhance_uk_search_query(self, query: str) -> str:
        """–£–ª—É—á—à–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ç–∞—Ç–µ–π –£–ö –†–§"""
        query_lower = query.lower()
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –£–ö –†–§
        if any(keyword in query_lower for keyword in ['—Å—Ç–∞—Ç—å—è', '—Å—Ç.', '—É–∫ —Ä—Ñ', '—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å']):
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            enhanced_query = f"{query} –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ —Ö–∏—â–µ–Ω–∏–µ –æ–±–º–∞–Ω –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –¥–æ–≤–µ—Ä–∏–µ–º —É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å"
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            import re
            article_match = re.search(r'(\d+(?:\.\d+)?)', query)
            if article_match:
                article_num = article_match.group(1)
                enhanced_query += f" —Å—Ç–∞—Ç—å—è {article_num} —Å—Ç {article_num}"
            
            return enhanced_query
        
        return query
    
    def _calculate_uk_relevance_boost(self, query: str, content: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞—Ç–µ–π –£–ö –†–§"""
        boost = 0.0
        query_lower = query.lower()
        content_lower = content.lower()
        
        # –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        import re
        article_match = re.search(r'(\d+(?:\.\d+)?)', query_lower)
        if article_match:
            article_num = article_match.group(1)
            if f"—Å—Ç–∞—Ç—å—è {article_num}" in content_lower or f"—Å—Ç. {article_num}" in content_lower:
                boost += 0.3  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏
        
        # –ë—É—Å—Ç –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –£–ö –†–§
        uk_keywords = ['–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ö–∏—â–µ–Ω–∏–µ', '–æ–±–º–∞–Ω', '–∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –¥–æ–≤–µ—Ä–∏–µ–º']
        for keyword in uk_keywords:
            if keyword in query_lower and keyword in content_lower:
                boost += 0.1
        
        # –ë—É—Å—Ç –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –£–ö –†–§
        if '—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å' in content_lower or '—É–∫ —Ä—Ñ' in content_lower:
            boost += 0.2
        
        return min(boost, 0.5)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç

    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à"""
        self.response_cache.cache.clear()
        self.embeddings_cache.clear()
        logger.info("üßπ Enhanced RAG cache cleared")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
enhanced_rag_service = EnhancedRAGService()
