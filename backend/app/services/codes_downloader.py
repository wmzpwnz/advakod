"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–æ–¥–µ–∫—Å–æ–≤ —Å pravo.gov.ru
"""

import asyncio
import aiohttp
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import json

logger = logging.getLogger(__name__)

class CodesDownloader:
    def __init__(self, output_dir: str = "downloaded_codexes"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # –°–ø–∏—Å–æ–∫ –∫–æ–¥–µ–∫—Å–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        self.codexes = {
            "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201410140002",
            "–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001202203030006",
            "–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140001",
            "–°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140002",
            "–ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140003",
            "–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140004",
            "–ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140005",
            "–ö–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140006"
        }
        
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            },
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

    async def download_pdf(self, url: str, filename: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç PDF —Ñ–∞–π–ª"""
        try:
            logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {filename}")
            
            async with self.session.get(url) as response:
                response.raise_for_status()
                content = await response.read()
            
            filepath = self.output_dir / filename
            with open(filepath, 'wb') as f:
                f.write(content)
            
            logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω: {filename} ({len(content)} –±–∞–π—Ç)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {filename}: {e}")
            return False

    async def download_codex(self, name: str, url: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–æ–¥–µ–∫—Å –ø–æ URL"""
        logger.info(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–µ–∫—Å–∞: {name}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        link_param = url.split('link=')[-1] if 'link=' in url else 'unknown'
        filename = f"{link_param}.pdf"
        
        return await self.download_pdf(url, filename)

    async def download_all_codexes(self) -> Tuple[int, int]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –∫–æ–¥–µ–∫—Å—ã"""
        logger.info("üöÄ –ù–∞—á–∞–ª–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–æ–¥–µ–∫—Å–æ–≤")
        logger.info(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.output_dir}")
        
        success_count = 0
        total_count = len(self.codexes)
        
        async with self:
            for name, url in self.codexes.items():
                if await self.download_codex(name, url):
                    success_count += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(2)
        
        logger.info(f"‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {success_count}/{total_count} –∫–æ–¥–µ–∫—Å–æ–≤")
        return success_count, total_count

    def get_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        files = list(self.output_dir.glob("*.pdf"))
        return {
            "total_files": len(files),
            "files": [f.name for f in files],
            "total_size": sum(f.stat().st_size for f in files),
            "output_dir": str(self.output_dir)
        }

    def get_downloaded_files(self) -> List[Path]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        return list(self.output_dir.glob("*.pdf"))

    def cleanup_old_files(self, days: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã"""
        cutoff_time = datetime.now().timestamp() - (days * 24 * 60 * 60)
        cleaned_count = 0
        
        for file_path in self.output_dir.glob("*.pdf"):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                cleaned_count += 1
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {file_path.name}")
        
        if cleaned_count > 0:
            logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–æ {cleaned_count} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤")
        else:
            logger.info("‚ÑπÔ∏è –°—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")



