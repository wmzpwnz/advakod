"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–æ–¥–µ–∫—Å–æ–≤ —Å pravo.gov.ru
"""

import asyncio
import aiohttp
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs

logger = logging.getLogger(__name__)

class CodesDownloader:
    def __init__(self, output_dir: str = "data/codes_downloads"):
        import os
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        if not os.path.isabs(output_dir):
            base_dir = os.getenv("DATA_DIR", "/app/data")
            self.output_dir = Path(base_dir) / output_dir
        else:
            self.output_dir = Path(output_dir)
        
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError) as e:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∞–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            import tempfile
            temp_base = Path(tempfile.gettempdir()) / "codes_downloads"
            temp_base.mkdir(parents=True, exist_ok=True)
            self.output_dir = temp_base
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é: {temp_base}")
        
        # –°–ø–∏—Å–æ–∫ –∫–æ–¥–µ–∫—Å–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        self.codexes = {
            "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201410140002",
            "–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001202203030006",
            "–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140001",
            "–°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140002",
            "–ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140003",
            "–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140004",
            "–ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140005",
            "–ö–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§": "http://pravo.gov.ru/proxy/ips/?docbody=&nd=102120000&rdk=&backlink=1&page=1&rdnd=102120000&link=0001201412140006"
        }
        
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            },
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

    def _is_html_content(self, content: bytes) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç HTML"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ HTML —Ç–µ–≥–æ–≤
            content_start = content[:1024].decode('utf-8', errors='ignore').lower()
            return '<html' in content_start or '<!doctype html' in content_start
        except:
            return False
    
    async def _get_document_metadata(self, eo_number: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ API publication.pravo.gov.ru"""
        try:
            api_url = f"http://publication.pravo.gov.ru/api/Document?eoNumber={eo_number}"
            
            async with self.session.get(api_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata = {
                        "eo_number": data.get("eoNumber"),
                        "name": data.get("name"),
                        "complex_name": data.get("complexName"),
                        "title": data.get("title"),
                        "number": data.get("number"),
                        "document_date": data.get("documentDate"),
                        "publish_date": data.get("publishDateShort"),
                        "view_date": data.get("viewDate"),
                        "pages_count": data.get("pagesCount"),
                        "pdf_file_length": data.get("pdfFileLength"),
                        "document_type": data.get("documentType", {}).get("name") if data.get("documentType") else None,
                        "signatory_authorities": [
                            auth.get("name") for auth in data.get("signatoryAuthorities", [])
                        ],
                        "document_id": data.get("id"),
                        "source_url": f"http://publication.pravo.gov.ru/Document/View/{eo_number}",
                        "api_metadata_retrieved_at": datetime.now().isoformat()
                    }
                    
                    logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {eo_number}: {metadata.get('name', 'unknown')}")
                    return metadata
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {eo_number}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è {eo_number}: {e}")
            return None
    
    def _extract_text_from_html(self, html_content: bytes) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for encoding in ['utf-8', 'windows-1251', 'cp1251', 'iso-8859-1']:
                try:
                    html_text = html_content.decode(encoding)
                    soup = BeautifulSoup(html_text, 'html.parser')
                    
                    # –£–¥–∞–ª—è–µ–º script –∏ style —Ç–µ–≥–∏
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = soup.get_text()
                    
                    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                    lines = (line.strip() for line in text.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text = '\n'.join(chunk for chunk in chunks if chunk)
                    
                    if len(text) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
                        return text
                except (UnicodeDecodeError, Exception):
                    continue
            
            return None
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {e}")
            return None

    async def download_pdf(self, url: str, filename: str, eo_number: str = None) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç PDF —Ñ–∞–π–ª –∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML"""
        try:
            # –ï—Å–ª–∏ –µ—Å—Ç—å eo_number, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π URL –Ω–∞ –ø–æ–ª–Ω—ã–π PDF
            if eo_number:
                pdf_url = f"http://publication.pravo.gov.ru/file/pdf?eoNumber={eo_number}"
                logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ PDF —á–µ—Ä–µ–∑ API: {filename}")
            else:
                pdf_url = url
                logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {filename}")
            
            async with self.session.get(pdf_url) as response:
                response.raise_for_status()
                content = await response.read()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç HTML
            if self._is_html_content(content):
                logger.info(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω HTML –≤–º–µ—Å—Ç–æ PDF, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç...")
                
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ iframe —Å doc_itself - —Ç–∞–º —Ä–µ–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                try:
                    html_text = content.decode('windows-1251', errors='ignore')
                    soup = BeautifulSoup(html_text, 'html.parser')
                    
                    # –ò—â–µ–º iframe —Å doc_itself
                    iframes = soup.find_all('iframe', src=True)
                    doc_iframe_url = None
                    for iframe in iframes:
                        src = iframe.get('src', '')
                        if 'doc_itself' in src:
                            # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—ã–π URL –¥–ª—è iframe
                            from urllib.parse import urljoin
                            doc_iframe_url = urljoin(url, src)
                            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω iframe —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º: {doc_iframe_url}")
                            break
                    
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ iframe —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º, –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–≥–æ
                    if doc_iframe_url:
                        async with self.session.get(doc_iframe_url) as iframe_response:
                            iframe_response.raise_for_status()
                            iframe_content = await iframe_response.read()
                            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ iframe: {len(iframe_content)} bytes")
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ iframe –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                            text = self._extract_text_from_html(iframe_content)
                        if text and len(text) > 500:  # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ä–æ—à–∏–π —Ç–µ–∫—Å—Ç –∏–∑ iframe
                            txt_filename = filename.replace('.pdf', '.txt')
                            filepath = self.output_dir / txt_filename
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(text)
                            logger.info(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ iframe: {txt_filename} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                            return True
                except Exception as iframe_err:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å iframe: {iframe_err}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç")
                
                # –ï—Å–ª–∏ iframe –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                text = self._extract_text_from_html(content)
                
                if text:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (DocumentService –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç .txt)
                    txt_filename = filename.replace('.pdf', '.txt')
                    filepath = self.output_dir / txt_filename
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(text)
                    logger.info(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ HTML: {txt_filename} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    # HTML —Ñ–∞–π–ª—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º - –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã, —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ HTML –¥–ª—è {filename}")
                    # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ TXT —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                    txt_filename = filename.replace('.pdf', '.txt')
                    filepath = self.output_dir / txt_filename
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ö–æ—Ç—è –±—ã —á—Ç–æ-—Ç–æ –∏–∑ HTML
                    try:
                        html_text = content.decode('windows-1251', errors='ignore')
                        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ BeautifulSoup
                        import re
                        text = re.sub(r'<[^>]+>', '', html_text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        if len(text) > 100:
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(text)
                            logger.info(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –ø—Ä–æ—Å—Ç—ã–º –º–µ—Ç–æ–¥–æ–º: {txt_filename}")
                            return True
                    except:
                        pass
                    return False
            else:
                # –≠—Ç–æ PDF –∏–ª–∏ –¥—Ä—É–≥–æ–π –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª
                filepath = self.output_dir / filename
                with open(filepath, 'wb') as f:
                    f.write(content)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ PDF
                if content[:4] == b'%PDF':
                    logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω PDF: {filename} ({len(content)} –±–∞–π—Ç)")
                else:
                    logger.warning(f"‚ö†Ô∏è –°–∫–∞—á–∞–Ω —Ñ–∞–π–ª, –Ω–æ –Ω–µ PDF: {filename} ({len(content)} –±–∞–π—Ç)")
                
                return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {filename}: {e}")
            return False

    async def download_codex(self, name: str, url: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–æ–¥–µ–∫—Å –ø–æ URL –∏ –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ API"""
        logger.info(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–µ–∫—Å–∞: {name}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º eo_number (ID) –∏–∑ URL –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        link_param = query_params.get('link', [None])[0] if query_params.get('link') else None
        
        if not link_param:
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ —Å—Ç—Ä–æ–∫–∏ –Ω–∞–ø—Ä—è–º—É—é
            link_param = url.split('link=')[-1].split('&')[0] if 'link=' in url else 'unknown'
        
        eo_number = link_param
        filename = f"{eo_number}.pdf"
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ API
        metadata = await self._get_document_metadata(eo_number)
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π URL –Ω–∞ –ø–æ–ª–Ω—ã–π PDF)
        download_success = await self.download_pdf(url, filename, eo_number=eo_number)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª
        if download_success and metadata:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –±—ã—Ç—å .txt –µ—Å–ª–∏ —ç—Ç–æ HTML)
            downloaded_files = list(self.output_dir.glob(f"{eo_number}.*"))
            actual_file = None
            if downloaded_files:
                actual_file = downloaded_files[0]
                metadata_file = actual_file.with_suffix('.json')
            else:
                metadata_file = self.output_dir / f"{eo_number}.json"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è –∫–æ–¥–µ–∫—Å–∞ –∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            metadata["codex_name"] = name
            if actual_file:
                metadata["file_path"] = str(actual_file)
                metadata["file_name"] = actual_file.name
            else:
                metadata["file_path"] = None
                metadata["file_name"] = filename
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_file.name}")
        
        return download_success

    async def download_all_codexes(self) -> Tuple[int, int]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –∫–æ–¥–µ–∫—Å—ã"""
        logger.info("üöÄ –ù–∞—á–∞–ª–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–æ–¥–µ–∫—Å–æ–≤")
        logger.info(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.output_dir}")
        
        success_count = 0
        total_count = len(self.codexes)
        
        async with self:
            for name, url in self.codexes.items():
                if await self.download_codex(name, url):
                    success_count += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(2)
        
        logger.info(f"‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {success_count}/{total_count} –∫–æ–¥–µ–∫—Å–æ–≤")
        return success_count, total_count

    def get_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        # –ò—â–µ–º PDF, TXT –∏ HTML —Ñ–∞–π–ª—ã
        pdf_files = list(self.output_dir.glob("*.pdf"))
        txt_files = list(self.output_dir.glob("*.txt"))
        html_files = list(self.output_dir.glob("*.html"))
        all_files = pdf_files + txt_files + html_files
        
        return {
            "total_files": len(all_files),
            "files": [f.name for f in all_files],
            "pdf_files": len(pdf_files),
            "txt_files": len(txt_files),
            "html_files": len(html_files),
            "total_size": sum(f.stat().st_size for f in all_files),
            "output_dir": str(self.output_dir)
        }

    def get_downloaded_files(self) -> List[Path]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (PDF, TXT, HTML)"""
        pdf_files = list(self.output_dir.glob("*.pdf"))
        txt_files = list(self.output_dir.glob("*.txt"))
        html_files = list(self.output_dir.glob("*.html"))
        return pdf_files + txt_files + html_files

    def cleanup_old_files(self, days: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã"""
        cutoff_time = datetime.now().timestamp() - (days * 24 * 60 * 60)
        cleaned_count = 0
        
        for file_path in self.output_dir.glob("*.pdf"):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                cleaned_count += 1
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {file_path.name}")
        
        if cleaned_count > 0:
            logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–æ {cleaned_count} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤")
        else:
            logger.info("‚ÑπÔ∏è –°—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")



