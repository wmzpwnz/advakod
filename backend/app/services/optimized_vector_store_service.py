"""
Optimized Vector Store Service - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ vector_store_service.py —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import logging
import chromadb
from chromadb.config import Settings
import os
from typing import List, Dict, Any, Optional, Tuple, Union
import uuid
import json
import asyncio
from datetime import datetime, date
from ..core.date_utils import DateUtils
from ..core.cache import cache_service

logger = logging.getLogger(__name__)

class OptimizedVectorStoreService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.client = None
        self.collection = None
        self.collection_name = os.getenv("CHROMA_COLLECTION_NAME", "legal_documents")
        self.db_path = os.getenv("CHROMA_DB_PATH", os.path.join(os.getcwd(), "backend", "data", "chroma_db"))
        self.is_initialized = False
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self._search_cache = {}
        self._cache_ttl = 300  # 5 –º–∏–Ω—É—Ç
        self._max_cache_size = 100
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_searches": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_documents": 0,
            "average_search_time": 0.0,
            "last_search_time": 0.0
        }
        
        # –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self._connection_pool_size = 5
        self._active_connections = 0
        
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OptimizedVectorStoreService...")
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            os.makedirs(self.db_path, exist_ok=True)
            
            logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ChromaDB –≤ {self.db_path}")
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç ChromaDB —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True,
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    chroma_db_impl="duckdb+parquet",
                    persist_directory=self.db_path
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                self.collection = self.client.get_collection(name=self.collection_name)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")
            except Exception:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG"},
                    embedding_function=None
                )
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")
            
            self.is_initialized = True
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            try:
                count = self.collection.count()
                self._stats["total_documents"] = count
                logger.info(f"üìä –í –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            
            logger.info("‚úÖ OptimizedVectorStoreService —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OptimizedVectorStoreService: {e}")
            self.is_initialized = False
            raise
    
    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≥–æ—Ç–æ–≤–∞ –ª–∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∫ —Ä–∞–±–æ—Ç–µ"""
        return self.is_initialized and self.client is not None and self.collection is not None
    
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        count = 0
        if self.is_ready():
            try:
                count = self.collection.count()
                self._stats["total_documents"] = count
            except:
                count = 0
                
        return {
            "initialized": self.is_initialized,
            "db_path": self.db_path,
            "collection_name": self.collection_name,
            "documents_count": count,
            "cache_size": len(self._search_cache),
            "max_cache_size": self._max_cache_size,
            "active_connections": self._active_connections,
            "stats": self._stats.copy()
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        cache_hit_rate = 0.0
        if self._stats["total_searches"] > 0:
            cache_hit_rate = self._stats["cache_hits"] / self._stats["total_searches"]
            
        return {
            "total_searches": self._stats["total_searches"],
            "cache_hit_rate": cache_hit_rate,
            "average_search_time": self._stats["average_search_time"],
            "total_documents": self._stats["total_documents"],
            "cache_size": len(self._search_cache)
        }
    
    def _generate_cache_key(self, query: str, limit: int, min_similarity: float, situation_date: Optional[str] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        import hashlib
        key_data = f"{query}:{limit}:{min_similarity}:{situation_date or 'none'}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _clean_cache(self):
        """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞"""
        current_time = datetime.now().timestamp()
        expired_keys = []
        
        for key, (timestamp, _) in self._search_cache.items():
            if current_time - timestamp > self._cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self._search_cache[key]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if len(self._search_cache) > self._max_cache_size:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
            sorted_items = sorted(self._search_cache.items(), key=lambda x: x[1][0])
            items_to_remove = len(self._search_cache) - self._max_cache_size
            for i in range(items_to_remove):
                del self._search_cache[sorted_items[i][0]]
    
    def _validate_embedding(self, embedding) -> list:
        """Validates embedding data before storing"""
        import numpy as np
        
        if embedding is None:
            raise ValueError("Embedding cannot be None")
            
        try:
            arr = np.asarray(embedding, dtype=float)
            
            if arr.ndim != 1:
                raise ValueError(f"Embedding must be 1-dimensional, got {arr.ndim}D")
                
            if np.isnan(arr).any():
                raise ValueError("Embedding contains NaN values")
                
            if np.isinf(arr).any():
                raise ValueError("Embedding contains infinite values")
                
            if len(arr) < 50 or len(arr) > 5000:
                raise ValueError(f"Embedding dimension {len(arr)} seems unreasonable")
                
            return arr.tolist()
            
        except (ValueError, TypeError) as e:
            logger.error(f"Embedding validation failed: {e}")
            raise ValueError(f"Invalid embedding data: {e}")
    
    def _validate_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Validates and sanitizes metadata"""
        if not isinstance(metadata, dict):
            raise ValueError("Metadata must be a dictionary")
            
        ALLOWED_KEYS = {
            "source", "article", "valid_from", "valid_to", "edition",
            "title", "filename", "content_length", "added_at", "part", "item"
        }
        
        sanitized = {}
        for key, value in metadata.items():
            if key in ALLOWED_KEYS:
                if isinstance(value, (str, int, float, bool, type(None))):
                    sanitized[key] = value
                else:
                    sanitized[key] = str(value)
                    
        return sanitized 
    
    async def add_document(self, 
                          content: str, 
                          metadata: Dict[str, Any],
                          document_id: Optional[str] = None,
                          embedding: Optional[List[float]] = None) -> bool:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        if not self.is_ready():
            logger.info("üîÑ Optimized Vector store –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é...")
            await self.initialize()
        
        if not self.is_ready():
            logger.warning("OptimizedVectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            # Validate inputs
            if not content or not content.strip():
                raise ValueError("Document content cannot be empty")
                
            sanitized_metadata = self._validate_metadata(metadata)
            
            if embedding is not None:
                validated_embedding = self._validate_embedding(embedding)
            else:
                validated_embedding = None
            
            if not document_id:
                document_id = str(uuid.uuid4())
            
            sanitized_metadata.update({
                "added_at": datetime.now().isoformat(),
                "content_length": len(content)
            })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            if validated_embedding:
                self.collection.add(
                    documents=[content],
                    metadatas=[sanitized_metadata],
                    ids=[document_id],
                    embeddings=[validated_embedding]
                )
            else:
                self.collection.add(
                    documents=[content],
                    metadatas=[sanitized_metadata],
                    ids=[document_id]
                )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._stats["total_documents"] += 1
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–æ–∏—Å–∫–∞, —Ç–∞–∫ –∫–∞–∫ –¥–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            self._search_cache.clear()
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {document_id} (–¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return True
            
        except ValueError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
            return False
    
    async def search_similar(self, 
                           query: str, 
                           limit: int = 5,
                           min_similarity: float = 0.5,
                           situation_date: Optional[Union[str, date, datetime]] = None) -> List[Dict[str, Any]]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not self.is_ready():
            logger.warning("OptimizedVectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return []
        
        start_time = datetime.now().timestamp()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = self._generate_cache_key(query, limit, min_similarity, str(situation_date) if situation_date else None)
            
            # –û—á–∏—â–∞–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏
            self._clean_cache()
            
            if cache_key in self._search_cache:
                timestamp, cached_results = self._search_cache[cache_key]
                if datetime.now().timestamp() - timestamp < self._cache_ttl:
                    self._stats["cache_hits"] += 1
                    self._stats["total_searches"] += 1
                    logger.info(f"üéØ –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")
                    return cached_results
            
            # –ö—ç—à –ø—Ä–æ–º–∞—Ö - –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            self._stats["cache_misses"] += 1
            
            # Create date filter if situation_date is provided
            where_filter = None
            if situation_date:
                where_filter = DateUtils.create_date_filter(situation_date)
                logger.info(f"üìÖ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ: {situation_date} -> {where_filter}")
            
            logger.info(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫: '{query[:50]}...' (limit={limit}, min_similarity={min_similarity})")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ
            search_kwargs = {
                "query_texts": [query],
                "n_results": limit,
                "include": ['documents', 'metadatas', 'distances']
            }
            
            if where_filter:
                search_kwargs["where"] = where_filter
                
            results = self.collection.query(**search_kwargs)
            
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {len(results.get('documents', [[]])[0])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            documents = []
            if results['documents'] and len(results['documents']) > 0:
                docs = results['documents'][0]
                metadatas = results['metadatas'][0] if results['metadatas'] else []
                distances = results['distances'][0] if results['distances'] else []
                
                for i, content in enumerate(docs):
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = 1.0 - distance
                    
                    if similarity >= min_similarity:
                        documents.append({
                            "content": content,
                            "metadata": metadatas[i] if i < len(metadatas) else {},
                            "similarity": similarity,
                            "distance": distance
                        })
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._search_cache[cache_key] = (datetime.now().timestamp(), documents)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            search_time = datetime.now().timestamp() - start_time
            self._stats["total_searches"] += 1
            self._stats["last_search_time"] = search_time
            
            if self._stats["total_searches"] > 0:
                total_time = self._stats["average_search_time"] * (self._stats["total_searches"] - 1)
                self._stats["average_search_time"] = (total_time + search_time) / self._stats["total_searches"]
            
            logger.info(f"üîç –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤—Ä–µ–º—è: {search_time:.3f}—Å)")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not self.is_ready():
            logger.warning("OptimizedVectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return None
            
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_cache_key = f"doc_{document_id}"
            if doc_cache_key in self._search_cache:
                timestamp, cached_doc = self._search_cache[doc_cache_key]
                if datetime.now().timestamp() - timestamp < self._cache_ttl:
                    return cached_doc
            
            results = self.collection.get(
                ids=[document_id],
                include=['documents', 'metadatas']
            )
            
            if results['documents'] and len(results['documents']) > 0:
                doc = {
                    "id": document_id,
                    "content": results['documents'][0],
                    "metadata": results['metadatas'][0] if results['metadatas'] else {}
                }
                
                # –ö—ç—à–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                self._search_cache[doc_cache_key] = (datetime.now().timestamp(), doc)
                return doc
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            
        return None
    
    async def delete_document(self, document_id: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID"""
        if not self.is_ready():
            logger.warning("OptimizedVectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            self.collection.delete(ids=[document_id])
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._stats["total_documents"] = max(0, self._stats["total_documents"] - 1)
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à
            self._search_cache.clear()
            
            logger.info(f"üóëÔ∏è –î–æ–∫—É–º–µ–Ω—Ç —É–¥–∞–ª–µ–Ω –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
    
    async def clear_collection(self) -> bool:
        """–û—á–∏—â–∞–µ—Ç –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""
        if not self.is_ready():
            logger.warning("OptimizedVectorStore –Ω–µ –≥–æ—Ç–æ–≤")
            return False
            
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG"}
            )
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –∫—ç—à
            self._stats["total_documents"] = 0
            self._search_cache.clear()
            
            logger.info("üóëÔ∏è –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –æ—á–∏—â–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "status": "healthy" if self.is_ready() else "unhealthy",
            "initialized": self.is_initialized,
            "documents_count": self._stats["total_documents"],
            "cache_size": len(self._search_cache),
            "performance": self.get_performance_summary()
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
optimized_vector_store_service = OptimizedVectorStoreService()