"""
Unified LLM Service - –µ–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å saiga_service.py, saiga_service_improved.py, optimized_saiga_service.py
"""

import logging
import time
import threading
import asyncio
import uuid
from typing import Optional, AsyncGenerator, Any, Dict, List
from queue import Queue
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from ..core.config import settings

# –í–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å llama_cpp
from llama_cpp import Llama

logger = logging.getLogger(__name__)


class RequestPriority(Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3


@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
    id: str
    prompt: str
    context: Optional[str]
    user_id: str
    timestamp: datetime
    priority: RequestPriority = RequestPriority.NORMAL
    stream: bool = True
    max_tokens: int = 1024
    temperature: float = 0.3
    top_p: float = 0.8


@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    request_id: str
    content: str
    processing_time: float
    tokens_generated: int
    queue_time: float
    success: bool
    error: Optional[str] = None


@dataclass
class ServiceHealth:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    status: str  # "healthy", "degraded", "unhealthy"
    last_check: datetime
    response_time: float
    error_rate: float
    memory_usage: float
    cpu_usage: float
    queue_length: int
    active_requests: int


@dataclass
class LLMMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM"""
    requests_per_minute: float
    average_response_time: float
    p95_response_time: float
    error_rate: float
    queue_length: int
    concurrent_requests: int
    memory_usage_mb: float
    cpu_usage_percent: float
    total_requests: int
    successful_requests: int
    failed_requests: int


def _redact_for_logs(text: str, max_len: int = 120) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –∏ –º–∞—Å–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–æ–≥–æ–≤."""
    if not text:
        return ""
    s = text.strip()
    if len(s) <= max_len:
        return s.replace("\n", " ")
    return (s[:max_len//2] + " ... " + s[-max_len//2:]).replace("\n", " ")


def _estimate_tokens(text: str) -> int:
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: 1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞."""
    if not text:
        return 0
    return max(1, len(text) // 4)


class UnifiedLLMService:
    """–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ Vistral"""
    
    def __init__(self):
        self.model: Optional[Llama] = None
        self._model_loaded: bool = False
        self._load_lock = threading.Lock()
        self._async_semaphore: Optional[asyncio.Semaphore] = None
        self._configured_n_batch = getattr(settings, "VISTRAL_N_BATCH", 512)
        self._current_n_batch: Optional[int] = None
        self._long_prompt_threshold = 1500
        self._long_prompt_n_batch = min(512, self._configured_n_batch)
        self._use_mlock = getattr(settings, "VISTRAL_USE_MLOCK", False)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._inference_timeout = getattr(settings, "VISTRAL_INFERENCE_TIMEOUT", 900)
        self._max_concurrency = getattr(settings, "VISTRAL_MAX_CONCURRENCY", 3)
        self._queue_size = getattr(settings, "VISTRAL_QUEUE_SIZE", 50)
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        self._request_queue = asyncio.PriorityQueue(maxsize=self._queue_size)
        self._active_requests: Dict[str, LLMRequest] = {}
        self._request_history: List[LLMResponse] = []
        self._max_history = 1000
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "p95_response_time": 0.0,
            "last_response_time": 0.0,
            "queue_length": 0,
            "concurrent_requests": 0,
            "requests_per_minute": 0.0,
            "error_rate": 0.0,
            "memory_usage_mb": 0.0,
            "cpu_usage_percent": 0.0
        }
        
        # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        self._last_metrics_update = time.time()
        self._response_times: List[float] = []
        
        # –§–ª–∞–≥ –¥–ª—è graceful shutdown
        self._shutdown_requested = False
        
        # Background tasks
        self._background_tasks: List[asyncio.Task] = []
        
    def _ensure_semaphore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self._async_semaphore is None:
            try:
                loop = asyncio.get_running_loop()
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
            except RuntimeError:
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
    
    def _load_model(self, force_n_batch: Optional[int] = None):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Vistral —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        if self._model_loaded and self.model is not None:
            return

        with self._load_lock:
            if self._model_loaded and self.model is not None:
                return
            try:
                import os
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ VISTRAL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                model_path = getattr(settings, "VISTRAL_MODEL_PATH", "")
                n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
                n_threads = getattr(settings, "VISTRAL_N_THREADS", 10)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π n_batch –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (512 –¥–ª—è CPU)
                n_batch = force_n_batch or getattr(settings, "VISTRAL_N_BATCH", 512)
                n_gpu_layers = getattr(settings, "VISTRAL_N_GPU_LAYERS", 0)
                # use_mlock –∏ f16_kv —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è CPU)
                use_mlock = getattr(settings, "VISTRAL_USE_MLOCK", False)
                use_f16_kv = use_mlock  # f16_kv —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ use_mlock –≤–∫–ª—é—á–µ–Ω
                
                if not model_path or not os.path.exists(model_path):
                    raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                
                logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Vistral –∏–∑ %s", model_path)
                logger.info("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_ctx=%s, n_threads=%s, n_batch=%s, use_mlock=%s, max_concurrency=%s, queue_size=%s", 
                          n_ctx, n_threads, n_batch, use_mlock, self._max_concurrency, self._queue_size)

                self.model = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_batch=n_batch,  # 512 –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è CPU, —É—Å–∫–æ—Ä—è–µ—Ç –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
                    n_gpu_layers=n_gpu_layers,
                    logits_all=False,
                    use_mmap=True,
                    use_mlock=use_mlock,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è CPU
                    verbose=False,
                    f16_kv=use_f16_kv,  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ use_mlock –≤–∫–ª—é—á–µ–Ω
                )
                self._current_n_batch = n_batch
                self._model_loaded = True
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vistral —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Vistral: %s", e)
                raise

    def _reload_model_with_batch(self, target_batch: int):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º n_batch"""
        logger.info("‚öôÔ∏è –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ n_batch=%s", target_batch)
        self._model_loaded = False
        self.model = None
        self._load_model(force_n_batch=target_batch)

    async def _ensure_batch_for_prompt(self, prompt_len: int):
        """–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ n_batch –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"""
        if prompt_len <= self._long_prompt_threshold:
            return
        desired_batch = self._long_prompt_n_batch
        if self._current_n_batch is not None and self._current_n_batch <= desired_batch:
            return
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._reload_model_with_batch, desired_batch)

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedLLMService...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self.ensure_model_loaded_async()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            await self._start_background_tasks()
            
            logger.info("‚úÖ UnifiedLLMService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ UnifiedLLMService: %s", e)
            raise

    async def _start_background_tasks(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏"""
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        queue_processor = asyncio.create_task(self._process_request_queue())
        self._background_tasks.append(queue_processor)
        
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        metrics_updater = asyncio.create_task(self._update_metrics_periodically())
        self._background_tasks.append(metrics_updater)
        
        logger.info("üîÑ –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∑–∞–ø—É—â–µ–Ω—ã")

    async def _process_request_queue(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
        while not self._shutdown_requested:
            try:
                # –ñ–¥–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                try:
                    priority, request = await asyncio.wait_for(
                        self._request_queue.get(), timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
                await self._process_single_request(request)
                
                # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
                self._request_queue.task_done()
                
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ –æ—á–µ—Ä–µ–¥–∏: %s", e)
                await asyncio.sleep(1)

    async def _process_single_request(self, request: LLMRequest):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            self._active_requests[request.id] = request
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            if request.stream:
                # –î–ª—è streaming –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
                logger.info(f"üîÑ Processing streaming request {request.id}")
                # Streaming –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ _stream_response_internal
                pass
            else:
                response_text = await self._generate_response_internal(
                    request.prompt,
                    request.max_tokens,
                    request.temperature,
                    request.top_p
                )
                
                # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç
                processing_time = time.time() - start_time
                response = LLMResponse(
                    request_id=request.id,
                    content=response_text,
                    processing_time=processing_time,
                    tokens_generated=_estimate_tokens(response_text),
                    queue_time=start_time - request.timestamp.timestamp(),
                    success=True
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self._add_to_history(response)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_response = LLMResponse(
                request_id=request.id,
                content="",
                processing_time=processing_time,
                tokens_generated=0,
                queue_time=start_time - request.timestamp.timestamp(),
                success=False,
                error=str(e)
            )
            self._add_to_history(error_response)
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ %s: %s", request.id, e)
            
        finally:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self._active_requests.pop(request.id, None)

    async def ensure_model_loaded_async(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        if self.is_model_loaded():
            return True
            
        loop = asyncio.get_running_loop()
        try:
            await loop.run_in_executor(None, self._load_model)
            if not self.is_model_loaded():
                raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ _load_model")
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ ensure_model_loaded_async")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _compute_max_gen_tokens(self, prompt: str, requested_max: int) -> int:
        """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç n_ctx –∏ –¥–ª–∏–Ω—ã prompt"""
        n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
        prompt_tokens = _estimate_tokens(prompt)
        safety_margin = getattr(settings, "VISTRAL_TOKEN_MARGIN", 32)
        available = max(1, n_ctx - prompt_tokens - safety_margin)
        if requested_max > available:
            logger.debug("requested_max (%s) > available (%s) -> limiting to available", 
                        requested_max, available)
            return available
        return requested_max

    async def generate_response(
        self,
        prompt: str,
        context: Optional[str] = None,
        stream: bool = True,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8,
        user_id: str = "anonymous",
        priority: RequestPriority = RequestPriority.NORMAL
    ) -> AsyncGenerator[str, None]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π streaming"""
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
        # –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ, –µ—Å–ª–∏ –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω (–∏–∑ chat.py)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—Ä–æ–º–ø—Ç —É–∂–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —é—Ä–∏—Å—Ç–∞
        if "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç" in prompt:
            # –ü—Ä–æ–º–ø—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            processed_prompt = prompt
        else:
            # –ü—Ä–æ–º–ø—Ç –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            processed_prompt = self._prepare_prompt(prompt, context)
        
        request = LLMRequest(
            id=str(uuid.uuid4()),
            prompt=processed_prompt,
            context=context,
            user_id=user_id,
            timestamp=datetime.now(),
            priority=priority,
            stream=stream,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        if stream:
            # Streaming —Ä–µ–∂–∏–º
            async for chunk in self._stream_response_internal(request):
                yield chunk
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            response = await self._generate_response_internal(
                request.prompt, max_tokens, temperature, top_p
            )
            yield response

    async def _generate_response_internal(
        self,
        prompt: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8
    ) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        self._ensure_semaphore()
        await self.ensure_model_loaded_async()
        await self._ensure_batch_for_prompt(len(prompt))
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        assert self._async_semaphore is not None
        async with self._async_semaphore:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º max tokens
            allowed_max = self._compute_max_gen_tokens(prompt, max_tokens)

            if getattr(settings, "LOG_PROMPTS", False):
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) prompt=%s... max_tokens=%s temp=%s", 
                          _redact_for_logs(prompt, 120), allowed_max, temperature)
            else:
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) max_tokens=%s temp=%s", allowed_max, temperature)

            loop = asyncio.get_running_loop()

            def _blocking_call():
                try:
                    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º chat-completion, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å instruct-–º–æ–¥–µ–ª—è–º–∏
                    try:
                        chat_res = self.model.create_chat_completion(
                            messages=[
                                {"role": "user", "content": prompt},
                            ],
                            max_tokens=allowed_max,
                            temperature=temperature,
                            top_p=top_p,
                            stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                            repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                        )
                        return {"_mode": "chat", **chat_res}
                    except Exception:
                        # –§–æ–ª–ª–±—ç–∫ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                        pass

                    result = self.model(
                        prompt,
                        max_tokens=allowed_max,
                        temperature=temperature,
                        top_p=top_p,
                        stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                        repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                    )
                    return {"_mode": "text", **(result if isinstance(result, dict) else {"raw": result})}
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ blocking_call –º–æ–¥–µ–ª–∏: %s", e)
                    raise

            try:
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, _blocking_call), 
                    timeout=self._inference_timeout
                )
            except asyncio.TimeoutError:
                logger.error("‚è∞ Inference timeout after %s seconds", self._inference_timeout)
                raise RuntimeError(f"Inference timeout after {self._inference_timeout} seconds")

            try:
                text = ""
                if isinstance(result, dict) and "choices" in result and len(result["choices"]) > 0:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ chat-completion, –∑–∞—Ç–µ–º –∏–∑ text
                    text = (
                        result["choices"][0].get("message", {}).get("content")
                        or result["choices"][0].get("text")
                        or ""
                    )
                    text = (text or "").strip()
                else:
                    text = (str(result) or "").strip()
                # Fallback: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, –¥–µ–ª–∞–µ–º –æ–¥–Ω—É –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø–æ–ø—ã—Ç–∫—É
                if not text:
                    logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π temperature/top_p")
                    def _retry_call():
                        return self.model(
                            prompt,
                            max_tokens=max(32, min(allowed_max, 256)),
                            temperature=0.5,
                            top_p=0.9,
                            stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                            repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                        )
                    retry_result = await loop.run_in_executor(None, _retry_call)
                    if isinstance(retry_result, dict) and "choices" in retry_result and len(retry_result["choices"]) > 0:
                        text = (retry_result["choices"][0].get("text") or "").strip()
                    else:
                        text = (str(retry_result) or "").strip()
                    if not text:
                        text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ–π—á–∞—Å –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–¥–∞—Ç—å –µ–≥–æ –∫–æ—Ä–æ—á–µ."

                response_time = time.time() - start_time
                self._update_stats(True, response_time)
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (len=%s, time=%.2fs)", 
                          len(text), response_time)
                return text
                
            except Exception as e:
                response_time = time.time() - start_time
                self._update_stats(False, response_time)
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥–µ–ª–∏: %s", e)
                raise

    async def _stream_response_internal(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è streaming –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            model_loaded = await self.ensure_model_loaded_async()
            if not model_loaded:
                error_msg = "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π!")
                yield error_msg
                return
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å
            if not self.model or not self.is_model_loaded():
                error_msg = "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è."
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: model=%s, loaded=%s", self.model is not None, self.is_model_loaded())
                yield error_msg
                return
            
            await self._ensure_batch_for_prompt(len(request.prompt))
            self._ensure_semaphore()

            allowed_max = self._compute_max_gen_tokens(request.prompt, request.max_tokens)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–º–∞–∫—Å–∏–º—É–º 4000)
            allowed_max = min(allowed_max, 4000)
            loop = asyncio.get_running_loop()
            q: asyncio.Queue = asyncio.Queue()
            stop_event = threading.Event()

            stop_tokens = getattr(settings, "VISTRAL_STOP_TOKENS", "")
            if stop_tokens == "":
                stop_tokens = None
            repeat_penalty = getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1)

            def worker():
                try:
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
                    if not self.model:
                        logger.error("‚ùå Model is None in worker thread!")
                        loop.call_soon_threadsafe(q.put_nowait, "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                        return
                    
                    if not hasattr(self.model, 'create_chat_completion'):
                        logger.error("‚ùå Model has no create_chat_completion method!")
                        loop.call_soon_threadsafe(q.put_nowait, "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.")
                        return
                    
                    logger.info(f"üöÄ Starting model generation with prompt: {request.prompt[:100]}...")
                    logger.info(f"üìä Model settings: max_tokens={allowed_max}, temperature={request.temperature}, top_p={request.top_p}")
                    logger.info(f"‚úÖ Model check: model={self.model is not None}, type={type(self.model)}")
                    
                    chunk_count = 0
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ GitHub
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–∫–∞–∫ –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)
                    optimized_max_tokens = min(allowed_max, 4000)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–∫ –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ë–ï–ó –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≤ CHAT_MODE_CONFIG –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞ (basic/expert)
                    # –ù–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º top_p, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏
                    generation_params = {
                        "max_tokens": optimized_max_tokens,
                        "temperature": request.temperature,
                        "top_p": request.top_p,
                        "stream": True
                    }

                    acceleration_enabled = (not request.context or not request.context.strip()) and len(self._active_requests) <= 1
                    if acceleration_enabled:
                        fast_token_limit = min(1500, generation_params["max_tokens"])
                        generation_params["max_tokens"] = fast_token_limit
                        generation_params["top_p"] = max(0.5, generation_params["top_p"] - 0.15)
                        logger.info("‚ö° Fast-start —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω: max_tokens=%s, top_p=%s", fast_token_limit, generation_params["top_p"])

                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if stop_tokens:
                        generation_params["stop"] = stop_tokens
                    
                    logger.info(f"üîß Using optimized generation params: {generation_params}")
                    
                    start_time = time.time()
                    first_token_time = [None]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
                    timeout_triggered = [False]  # –§–ª–∞–≥ —Ç–∞–π–º–∞—É—Ç–∞
                    accumulated_text = ""  # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–≤—É—Ö—Ñ–∞–∑–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    quick_response_sent = [False]  # –§–ª–∞–≥ –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                    QUICK_RESPONSE_THRESHOLD = 256  # –ü–æ—Ä–æ–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200-300 —Å–∏–º–≤–æ–ª–æ–≤)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º create_chat_completion (–∫–∞–∫ –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –Ω–∞ GitHub)
                    logger.info(f"üîß Calling create_chat_completion with params: {generation_params}")
                    try:
                        stream_iter = self.model.create_chat_completion(
                            messages=[
                                {"role": "user", "content": request.prompt},
                            ],
                            stream=True,
                            **{k: v for k, v in generation_params.items() if k != "stream"}
                        )
                        logger.info(f"‚úÖ create_chat_completion returned iterator: {stream_iter is not None}")
                    except Exception as e:
                        logger.error(f"‚ùå Error calling create_chat_completion: {e}", exc_info=True)
                        loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
                        return
                    
                    # –†–µ–∞–ª—å–Ω—ã–π watchdog —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º stop_event –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    FIRST_TOKEN_TIMEOUT = 25  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (25 —Å–µ–∫—É–Ω–¥)
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º watchdog –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–π–º–∞—É—Ç–∞
                    def watchdog():
                        while not stop_event.is_set() and first_token_time[0] is None:
                            time.sleep(1.0)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
                            elapsed = time.time() - start_time
                            if elapsed > FIRST_TOKEN_TIMEOUT:
                                logger.error(f"‚ùå FIRST TOKEN TIMEOUT after {FIRST_TOKEN_TIMEOUT}s! Stopping generation...")
                                timeout_triggered[0] = True
                                stop_event.set()  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                                loop.call_soon_threadsafe(q.put_nowait, f"[TIMEOUT] –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {FIRST_TOKEN_TIMEOUT} —Å–µ–∫—É–Ω–¥. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∏—Å—Ç–æ—Ä–∏—é.")
                                break
                    
                    watchdog_thread = threading.Thread(target=watchdog, daemon=True)
                    watchdog_thread.start()
                    
                    logger.info("üîÑ Starting iteration over stream_iter...")
                    logger.info(f"üîç stream_iter type: {type(stream_iter)}, is iterable: {hasattr(stream_iter, '__iter__')}")
                    
                    iteration_started = False
                    for chunk in stream_iter:
                        if not iteration_started:
                            iteration_started = True
                            elapsed_iter = time.time() - start_time
                            logger.info(f"‚úÖ First iteration started after {elapsed_iter:.2f}s - chunk type: {type(chunk)}")
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
                        if stop_event.is_set():
                            logger.warning("üõë Generation stopped by watchdog")
                            break
                        
                        elapsed = time.time() - start_time
                        chunk_count += 1
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–ø–µ—Ä–≤—ã–µ 10, –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–µ 50)
                        if chunk_count <= 10 or chunk_count % 50 == 0:
                            logger.info(f"üîç Received chunk {chunk_count}: has_chunk={bool(chunk)}, elapsed={elapsed:.2f}s")
                        
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –±–µ–∑ —á–∞–Ω–∫–æ–≤
                        if chunk_count > 10 and elapsed > 120:  # 2 –º–∏–Ω—É—Ç—ã
                            logger.warning(f"‚ö†Ô∏è Generation taking too long: {chunk_count} chunks in {elapsed:.2f}s")
                        
                        if not chunk:
                            if chunk_count <= 10:
                                logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} is empty/None")
                            continue
                        
                        choices = chunk.get("choices") or []
                        if not choices:
                            if chunk_count <= 10:
                                logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} has no choices: chunk={chunk}")
                            continue
                        
                        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –≤—Ä–µ–º—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è)
                        if chunk_count == 1 and first_token_time[0] is None:
                            first_token_time[0] = elapsed
                            logger.info(f"‚ö° First token generated in {first_token_time[0]:.2f}s")
                            # –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é - —Ç–æ–ª—å–∫–æ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤—Ä–µ–º—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                            # Watchdog —Å–∞–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è, –∫–æ–≥–¥–∞ —É–≤–∏–¥–∏—Ç, —á—Ç–æ first_token_time[0] –Ω–µ None
                        
                        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ chat-—Å—Ç—Ä–∏–º–∞ (delta.content) –∏ text-—Å—Ç—Ä–∏–º–∞ (text)
                        delta = (
                            choices[0].get("delta", {}).get("content")  # Chat completion
                            or choices[0].get("text", "")  # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞
                        )
                        if delta:
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ —Ç–∞–π–º–∞—É—Ç–∞
                            if not timeout_triggered[0]:
                                loop.call_soon_threadsafe(q.put_nowait, delta)
                                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                                if chunk_count <= 3:
                                    logger.info(f"üì§ Chunk {chunk_count} sent: {delta[:50]}...")
                                
                                # –î–≤—É—Ö—Ñ–∞–∑–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                                accumulated_text += delta
                                # –ü—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω ‚âà 0.75 —Å–∏–º–≤–æ–ª–∞, –ø–æ—ç—Ç–æ–º—É 256 —Ç–æ–∫–µ–Ω–æ–≤ ‚âà 200 —Å–∏–º–≤–æ–ª–æ–≤
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ—Å–ª–µ ~200 —Å–∏–º–≤–æ–ª–æ–≤
                                if not quick_response_sent[0] and len(accumulated_text) >= 200:
                                    quick_response_sent[0] = True
                                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ (–Ω–µ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞)
                                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –≤ chat.py
                                    loop.call_soon_threadsafe(q.put_nowait, "__QUICK_RESPONSE_READY__")
                                    logger.info(f"‚úÖ Quick response ready after {len(accumulated_text)} characters ({chunk_count} chunks)")
                        else:
                            # –õ–æ–≥–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                            if chunk_count <= 5:
                                logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} has no delta: choices={choices[0] if choices else 'empty'}")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞ –∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
                        if chunk_count > 0 and chunk_count % 10 == 0:
                            if elapsed > self._inference_timeout:
                                logger.error(f"‚ùå MODEL GENERATION TIMEOUT after {self._inference_timeout}s! Force stopping...")
                                stop_event.set()
                                loop.call_soon_threadsafe(q.put_nowait, f"[TIMEOUT] Model generation exceeded {self._inference_timeout} seconds")
                                break
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ü–∏–∫–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    logger.info(f"‚úÖ Stream iteration loop completed. Processed {chunk_count} chunks")
                    
                    elapsed_time = time.time() - start_time
                    first_token_str = f"{first_token_time[0]:.2f}s" if first_token_time[0] else "N/A"
                    logger.info(f"üèÅ Model generation completed. Total chunks: {chunk_count}, Time: {elapsed_time:.2f}s, First token: {first_token_str}")
                    if chunk_count == 0:
                        logger.warning("‚ö†Ô∏è No chunks received from model!")
                    elif chunk_count == 1:
                        logger.warning("‚ö†Ô∏è Only 1 chunk received - generation may have stopped early")
                    loop.call_soon_threadsafe(q.put_nowait, None)
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ streaming worker: %s", e)
                    loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] {str(e)}")
                    loop.call_soon_threadsafe(q.put_nowait, None)

            t = threading.Thread(target=worker, daemon=True)
            t.start()

            while True:
                token = await q.get()
                if token is None:
                    break
                if isinstance(token, str) and token.startswith("[ERROR]"):
                    raise RuntimeError(token)
                yield token
                
        except Exception as e:
            logger.error(f"‚ùå Critical error in streaming: {e}")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –∫–ª–∏–µ–Ω—Ç—É
            yield f"[ERROR] –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
            return

    def _prepare_prompt(self, question: str, context: Optional[str] = None, chat_mode: str = "basic") -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        return self.create_legal_prompt(question, context, chat_mode)

    def create_legal_prompt(self, question: str, context: Optional[str] = None, chat_mode: str = "basic") -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–µ–∂–∏–º–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
        if chat_mode == "expert":
            system_prompt = "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞."
        else:  # basic
            system_prompt = "–¢—ã —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û–±—ä—è—Å–Ω—è–π –ø—Ä–æ—Å—Ç–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ."
        
        # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏—Å—Ç–æ—Ä–∏–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        history_part = ""
        if context and context.strip():
            # –ò—Å—Ç–æ—Ä–∏—è —É–∂–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: ... / –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ..."
            # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            history_part = context.strip() + "\n\n"
        
        # –ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        if history_part:
            prompt = f"{system_prompt}\n\n{history_part}–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        else:
            prompt = f"{system_prompt}\n\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        
        return prompt

    async def get_queue_position(self, request_id: str) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—á–µ—Ä–µ–¥–∏"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
        return self._request_queue.qsize()

    async def health_check(self) -> ServiceHealth:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        current_time = datetime.now()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        status = "healthy"
        if not self.is_model_loaded():
            status = "unhealthy"
        elif self._stats["error_rate"] > 0.1:  # –ë–æ–ª–µ–µ 10% –æ—à–∏–±–æ–∫
            status = "degraded"
        elif len(self._active_requests) >= self._max_concurrency:
            status = "degraded"
        
        return ServiceHealth(
            status=status,
            last_check=current_time,
            response_time=self._stats["last_response_time"],
            error_rate=self._stats["error_rate"],
            memory_usage=self._stats["memory_usage_mb"],
            cpu_usage=self._stats["cpu_usage_percent"],
            queue_length=self._request_queue.qsize(),
            active_requests=len(self._active_requests)
        )

    def get_metrics(self) -> LLMMetrics:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return LLMMetrics(
            requests_per_minute=self._stats["requests_per_minute"],
            average_response_time=self._stats["average_response_time"],
            p95_response_time=self._stats["p95_response_time"],
            error_rate=self._stats["error_rate"],
            queue_length=self._request_queue.qsize(),
            concurrent_requests=len(self._active_requests),
            memory_usage_mb=self._stats["memory_usage_mb"],
            cpu_usage_percent=self._stats["cpu_usage_percent"],
            total_requests=self._stats["total_requests"],
            successful_requests=self._stats["successful_requests"],
            failed_requests=self._stats["failed_requests"]
        )

    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._model_loaded and self.model is not None

    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å API: –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    def is_model_ready(self) -> bool:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤."""
        return self.is_model_loaded()

    async def get_model_status(self) -> dict:
        """–ö—Ä–∞—Ç–∫–∏–π —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ API."""
        return {
            "model_loaded": self.is_model_loaded(),
            "active_requests": len(self._active_requests),
            "max_concurrency": self._max_concurrency,
        }

    async def _update_metrics_periodically(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
        while not self._shutdown_requested:
            try:
                await self._update_metrics()
                await asyncio.sleep(30)  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: %s", e)
                await asyncio.sleep(30)

    async def _update_metrics(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        current_time = time.time()
        time_diff = current_time - self._last_metrics_update
        
        if time_diff > 0:
            # –û–±–Ω–æ–≤–ª—è–µ–º requests per minute
            recent_requests = len([r for r in self._request_history 
                                 if current_time - r.processing_time < 60])
            self._stats["requests_per_minute"] = recent_requests
            
            # –û–±–Ω–æ–≤–ª—è–µ–º error rate
            if self._stats["total_requests"] > 0:
                self._stats["error_rate"] = self._stats["failed_requests"] / self._stats["total_requests"]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º P95 response time
            if self._response_times:
                sorted_times = sorted(self._response_times)
                p95_index = int(len(sorted_times) * 0.95)
                self._stats["p95_response_time"] = sorted_times[p95_index] if p95_index < len(sorted_times) else 0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∑–∞–≥–ª—É—à–∫–∞)
            try:
                import psutil
                process = psutil.Process()
                self._stats["memory_usage_mb"] = process.memory_info().rss / 1024 / 1024
                self._stats["cpu_usage_percent"] = process.cpu_percent()
            except ImportError:
                pass
        
        self._last_metrics_update = current_time

    def _add_to_history(self, response: LLMResponse):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        self._request_history.append(response)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self._request_history) > self._max_history:
            self._request_history = self._request_history[-self._max_history:]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω –æ—Ç–≤–µ—Ç–æ–≤
        self._response_times.append(response.processing_time)
        if len(self._response_times) > 1000:
            self._response_times = self._response_times[-1000:]

    def _update_stats(self, success: bool, response_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self._stats["total_requests"] += 1
        if success:
            self._stats["successful_requests"] += 1
        else:
            self._stats["failed_requests"] += 1
        
        self._stats["last_response_time"] = response_time
        
        if self._stats["successful_requests"] > 0:
            total_time = self._stats["average_response_time"] * (self._stats["successful_requests"] - 1)
            self._stats["average_response_time"] = (total_time + response_time) / self._stats["successful_requests"]

    async def graceful_shutdown(self):
        """Graceful shutdown —Å–µ—Ä–≤–∏—Å–∞"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown UnifiedLLMService...")
        
        self._shutdown_requested = True
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–º–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥)
        shutdown_timeout = 30
        start_time = time.time()
        
        while self._active_requests and (time.time() - start_time) < shutdown_timeout:
            logger.info("‚è≥ –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è %d –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...", len(self._active_requests))
            await asyncio.sleep(1)
        
        # –û—Ç–º–µ–Ω—è–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
        for task in self._background_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        
        logger.info("‚úÖ UnifiedLLMService —É—Å–ø–µ—à–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
unified_llm_service = UnifiedLLMService()