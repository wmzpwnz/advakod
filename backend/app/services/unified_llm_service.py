"""
Unified LLM Service - –µ–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å saiga_service.py, saiga_service_improved.py, optimized_saiga_service.py
"""

import logging
import time
import threading
import asyncio
import uuid
from typing import Optional, AsyncGenerator, Any, Dict, List
from queue import Queue
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from ..core.config import settings

# –í–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å llama_cpp
from llama_cpp import Llama

logger = logging.getLogger(__name__)


class RequestPriority(Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3


@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
    id: str
    prompt: str
    context: Optional[str]
    user_id: str
    timestamp: datetime
    priority: RequestPriority = RequestPriority.NORMAL
    stream: bool = True
    max_tokens: int = 1024
    temperature: float = 0.3
    top_p: float = 0.8


@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    request_id: str
    content: str
    processing_time: float
    tokens_generated: int
    queue_time: float
    success: bool
    error: Optional[str] = None


@dataclass
class ServiceHealth:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    status: str  # "healthy", "degraded", "unhealthy"
    last_check: datetime
    response_time: float
    error_rate: float
    memory_usage: float
    cpu_usage: float
    queue_length: int
    active_requests: int


@dataclass
class LLMMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM"""
    requests_per_minute: float
    average_response_time: float
    p95_response_time: float
    error_rate: float
    queue_length: int
    concurrent_requests: int
    memory_usage_mb: float
    cpu_usage_percent: float
    total_requests: int
    successful_requests: int
    failed_requests: int


def _redact_for_logs(text: str, max_len: int = 120) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –∏ –º–∞—Å–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–æ–≥–æ–≤."""
    if not text:
        return ""
    s = text.strip()
    if len(s) <= max_len:
        return s.replace("\n", " ")
    return (s[:max_len//2] + " ... " + s[-max_len//2:]).replace("\n", " ")


def _estimate_tokens(text: str) -> int:
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: 1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞."""
    if not text:
        return 0
    return max(1, len(text) // 4)


class UnifiedLLMService:
    """–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ Vistral"""
    
    def __init__(self):
        self.model: Optional[Llama] = None
        self._model_loaded: bool = False
        self._load_lock = threading.Lock()
        self._async_semaphore: Optional[asyncio.Semaphore] = None
        self._configured_n_batch = getattr(settings, "VISTRAL_N_BATCH", 512)
        self._current_n_batch: Optional[int] = None
        self._long_prompt_threshold = 1500
        self._long_prompt_n_batch = min(512, self._configured_n_batch)
        self._use_mlock = getattr(settings, "VISTRAL_USE_MLOCK", False)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._inference_timeout = getattr(settings, "VISTRAL_INFERENCE_TIMEOUT", 900)
        self._max_concurrency = getattr(settings, "VISTRAL_MAX_CONCURRENCY", 3)
        self._queue_size = getattr(settings, "VISTRAL_QUEUE_SIZE", 50)
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        self._request_queue = asyncio.PriorityQueue(maxsize=self._queue_size)
        self._active_requests: Dict[str, LLMRequest] = {}
        self._request_history: List[LLMResponse] = []
        self._max_history = 1000
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "p95_response_time": 0.0,
            "last_response_time": 0.0,
            "queue_length": 0,
            "concurrent_requests": 0,
            "requests_per_minute": 0.0,
            "error_rate": 0.0,
            "memory_usage_mb": 0.0,
            "cpu_usage_percent": 0.0
        }
        
        # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        self._last_metrics_update = time.time()
        self._response_times: List[float] = []
        
        # –§–ª–∞–≥ –¥–ª—è graceful shutdown
        self._shutdown_requested = False
        
        # Background tasks
        self._background_tasks: List[asyncio.Task] = []
        
    def _ensure_semaphore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self._async_semaphore is None:
            try:
                loop = asyncio.get_running_loop()
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
            except RuntimeError:
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
    
    def _load_model(self, force_n_batch: Optional[int] = None):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Vistral —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        if self._model_loaded and self.model is not None:
            logger.debug("–ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É")
            return

        with self._load_lock:
            if self._model_loaded and self.model is not None:
                logger.debug("–ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (double-check), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É")
                return
            try:
                import os
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ VISTRAL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                model_path = getattr(settings, "VISTRAL_MODEL_PATH", "")
                n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
                n_threads = getattr(settings, "VISTRAL_N_THREADS", 10)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π n_batch –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (512 –¥–ª—è CPU)
                n_batch = force_n_batch or getattr(settings, "VISTRAL_N_BATCH", 512)
                n_gpu_layers = getattr(settings, "VISTRAL_N_GPU_LAYERS", 0)
                # use_mlock –∏ f16_kv —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è CPU)
                use_mlock = getattr(settings, "VISTRAL_USE_MLOCK", False)
                use_f16_kv = use_mlock  # f16_kv —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ use_mlock –≤–∫–ª—é—á–µ–Ω
                
                logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–∏ –º–æ–¥–µ–ª–∏: %s", model_path)
                if not model_path:
                    error_msg = "VISTRAL_MODEL_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
                    logger.error(f"‚ùå {error_msg}")
                    raise ValueError(error_msg)
                
                if not os.path.exists(model_path):
                    error_msg = f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}"
                    logger.error(f"‚ùå {error_msg}")
                    logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø—É—Ç—å —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
                    raise FileNotFoundError(error_msg)
                
                logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Vistral –∏–∑ %s", model_path)
                logger.info("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_ctx=%s, n_threads=%s, n_batch=%s, use_mlock=%s, max_concurrency=%s, queue_size=%s", 
                          n_ctx, n_threads, n_batch, use_mlock, self._max_concurrency, self._queue_size)

                logger.info("‚è≥ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...")
                self.model = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_batch=n_batch,  # 512 –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è CPU, —É—Å–∫–æ—Ä—è–µ—Ç –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
                    n_gpu_layers=n_gpu_layers,
                    logits_all=False,
                    use_mmap=True,
                    use_mlock=use_mlock,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è CPU
                    verbose=False,
                    f16_kv=use_f16_kv,  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ use_mlock –≤–∫–ª—é—á–µ–Ω
                )
                self._current_n_batch = n_batch
                self._model_loaded = True
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vistral —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é: model=%s, loaded=%s", self.model is not None, self._model_loaded)
                
            except FileNotFoundError as e:
                logger.exception("‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", e)
                self._model_loaded = False
                self.model = None
                raise
            except Exception as e:
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Vistral: %s", e)
                self._model_loaded = False
                self.model = None
                raise

    def _reload_model_with_batch(self, target_batch: int):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º n_batch"""
        logger.info("‚öôÔ∏è –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ n_batch=%s", target_batch)
        self._model_loaded = False
        self.model = None
        self._load_model(force_n_batch=target_batch)

    async def _ensure_batch_for_prompt(self, prompt_len: int):
        """–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ n_batch –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"""
        if prompt_len <= self._long_prompt_threshold:
            return
        desired_batch = self._long_prompt_n_batch
        if self._current_n_batch is not None and self._current_n_batch <= desired_batch:
            return
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._reload_model_with_batch, desired_batch)

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è"""
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedLLMService...")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò - –º–æ–¥–µ–ª—å –î–û–õ–ñ–ù–ê –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
            logger.info("üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞...")
            model_loaded = await self.ensure_model_loaded_async()
            
            if not model_loaded or not self.is_model_ready():
                error_msg = "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞!"
                logger.error(f"‚ùå {error_msg}")
                logger.error(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: model={self.model is not None}, _model_loaded={self._model_loaded}")
                # –ü—ã—Ç–∞–µ–º—Å—è –µ—â–µ —Ä–∞–∑ —Å –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π
                self._model_loaded = False
                self.model = None
                model_loaded_retry = await self.ensure_model_loaded_async()
                if not model_loaded_retry or not self.is_model_ready():
                    raise RuntimeError(f"{error_msg} –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å.")
            
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏")
            logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: is_model_ready()=%s", self.is_model_ready())
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            await self._start_background_tasks()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏
            monitor_task = asyncio.create_task(self._monitor_model_health())
            self._background_tasks.append(monitor_task)
            
            logger.info("‚úÖ UnifiedLLMService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ - –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞")
        except Exception as e:
            logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ UnifiedLLMService: %s", e, exc_info=True)
            raise

    async def _start_background_tasks(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏"""
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        queue_processor = asyncio.create_task(self._process_request_queue())
        self._background_tasks.append(queue_processor)
        
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        metrics_updater = asyncio.create_task(self._update_metrics_periodically())
        self._background_tasks.append(metrics_updater)
        
        logger.info("üîÑ –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∑–∞–ø—É—â–µ–Ω—ã")
    
    async def _monitor_model_health(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        logger.info("üîÑ –ó–∞–ø—É—â–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥)")
        check_interval = 30  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
        check_count = 0
        
        while not self._shutdown_requested:
            try:
                await asyncio.sleep(check_interval)
                check_count += 1
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model_ready = self.is_model_ready()
                model_exists = self.model is not None
                model_loaded_flag = self._model_loaded
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω—É—Ç—ã (4 –ø—Ä–æ–≤–µ—Ä–∫–∏) –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if check_count % 4 == 0:
                    logger.info(f"üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–æ–¥–µ–ª–∏: ready={model_ready}, exists={model_exists}, flag={model_loaded_flag}")
                
                if not model_ready:
                    logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏...")
                    logger.error(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: model={model_exists}, _model_loaded={model_loaded_flag}")
                    
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
                        self._model_loaded = False
                        self.model = None
                        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏...")
                        model_loaded = await self.ensure_model_loaded_async()
                        
                        if model_loaded and self.is_model_ready():
                            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                        else:
                            logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å!")
                            logger.error(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏: model={self.model is not None}, loaded={self._model_loaded}, ready={self.is_model_ready()}")
                    except Exception as reload_err:
                        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {reload_err}", exc_info=True)
                        
            except asyncio.CancelledError:
                logger.info("üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–æ–¥–µ–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
                await asyncio.sleep(check_interval)

    async def _process_request_queue(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
        while not self._shutdown_requested:
            try:
                # –ñ–¥–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                try:
                    priority, request = await asyncio.wait_for(
                        self._request_queue.get(), timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
                await self._process_single_request(request)
                
                # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
                self._request_queue.task_done()
                
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ –æ—á–µ—Ä–µ–¥–∏: %s", e)
                await asyncio.sleep(1)

    async def _process_single_request(self, request: LLMRequest):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            self._active_requests[request.id] = request
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            if request.stream:
                # –î–ª—è streaming –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
                logger.info(f"üîÑ Processing streaming request {request.id}")
                # Streaming –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ _stream_response_internal
                pass
            else:
                response_text = await self._generate_response_internal(
                    request.prompt,
                    request.max_tokens,
                    request.temperature,
                    request.top_p
                )
                
                # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç
                processing_time = time.time() - start_time
                response = LLMResponse(
                    request_id=request.id,
                    content=response_text,
                    processing_time=processing_time,
                    tokens_generated=_estimate_tokens(response_text),
                    queue_time=start_time - request.timestamp.timestamp(),
                    success=True
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self._add_to_history(response)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_response = LLMResponse(
                request_id=request.id,
                content="",
                processing_time=processing_time,
                tokens_generated=0,
                queue_time=start_time - request.timestamp.timestamp(),
                success=False,
                error=str(e)
            )
            self._add_to_history(error_response)
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ %s: %s", request.id, e)
            
        finally:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self._active_requests.pop(request.id, None)

    async def ensure_model_loaded_async(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        if self.is_model_loaded():
            logger.debug("–ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True")
            return True
            
        loop = asyncio.get_running_loop()
        try:
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏...")
            await loop.run_in_executor(None, self._load_model)
            if not self.is_model_loaded():
                error_msg = "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ _load_model"
                logger.error(f"‚ùå {error_msg}")
                logger.error("–°–æ—Å—Ç–æ—è–Ω–∏–µ: model=%s, _model_loaded=%s", self.model is not None, self._model_loaded)
                raise Exception(error_msg)
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ ensure_model_loaded_async")
            logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: is_model_ready()=%s", self.is_model_ready())
            return True
        except FileNotFoundError as e:
            logger.error(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
            logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É VISTRAL_MODEL_PATH –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            logger.error("–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: model=%s, _model_loaded=%s", self.model is not None, self._model_loaded)
            return False

    def _compute_max_gen_tokens(self, prompt: str, requested_max: int) -> int:
        """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç n_ctx –∏ –¥–ª–∏–Ω—ã prompt"""
        n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
        prompt_tokens = _estimate_tokens(prompt)
        safety_margin = getattr(settings, "VISTRAL_TOKEN_MARGIN", 32)
        available = max(1, n_ctx - prompt_tokens - safety_margin)
        if requested_max > available:
            logger.debug("requested_max (%s) > available (%s) -> limiting to available", 
                        requested_max, available)
            return available
        return requested_max

    async def generate_response(
        self,
        prompt: str,
        context: Optional[str] = None,
        stream: bool = True,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8,
        user_id: str = "anonymous",
        priority: RequestPriority = RequestPriority.NORMAL
    ) -> AsyncGenerator[str, None]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π streaming"""
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
        # –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ, –µ—Å–ª–∏ –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω (–∏–∑ chat.py)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—Ä–æ–º–ø—Ç —É–∂–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —é—Ä–∏—Å—Ç–∞
        if "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç" in prompt:
            # –ü—Ä–æ–º–ø—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            processed_prompt = prompt
        else:
            # –ü—Ä–æ–º–ø—Ç –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            processed_prompt = self._prepare_prompt(prompt, context)
        
        request = LLMRequest(
            id=str(uuid.uuid4()),
            prompt=processed_prompt,
            context=context,
            user_id=user_id,
            timestamp=datetime.now(),
            priority=priority,
            stream=stream,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        if stream:
            # Streaming —Ä–µ–∂–∏–º
            async for chunk in self._stream_response_internal(request):
                yield chunk
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            response = await self._generate_response_internal(
                request.prompt, max_tokens, temperature, top_p
            )
            yield response

    async def _generate_response_internal(
        self,
        prompt: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8
    ) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        self._ensure_semaphore()
        await self.ensure_model_loaded_async()
        await self._ensure_batch_for_prompt(len(prompt))
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        assert self._async_semaphore is not None
        async with self._async_semaphore:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º max tokens
            allowed_max = self._compute_max_gen_tokens(prompt, max_tokens)

            if getattr(settings, "LOG_PROMPTS", False):
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) prompt=%s... max_tokens=%s temp=%s", 
                          _redact_for_logs(prompt, 120), allowed_max, temperature)
            else:
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) max_tokens=%s temp=%s", allowed_max, temperature)

            loop = asyncio.get_running_loop()

            def _blocking_call():
                try:
                    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º chat-completion, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å instruct-–º–æ–¥–µ–ª—è–º–∏
                    try:
                        chat_res = self.model.create_chat_completion(
                            messages=[
                                {"role": "user", "content": prompt},
                            ],
                            max_tokens=allowed_max,
                            temperature=temperature,
                            top_p=top_p,
                            stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                            repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                        )
                        return {"_mode": "chat", **chat_res}
                    except Exception:
                        # –§–æ–ª–ª–±—ç–∫ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                        pass

                    result = self.model(
                        prompt,
                        max_tokens=allowed_max,
                        temperature=temperature,
                        top_p=top_p,
                        stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                        repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                    )
                    return {"_mode": "text", **(result if isinstance(result, dict) else {"raw": result})}
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ blocking_call –º–æ–¥–µ–ª–∏: %s", e)
                    raise

            try:
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, _blocking_call), 
                    timeout=self._inference_timeout
                )
            except asyncio.TimeoutError:
                logger.error("‚è∞ Inference timeout after %s seconds", self._inference_timeout)
                raise RuntimeError(f"Inference timeout after {self._inference_timeout} seconds")

            try:
                text = ""
                if isinstance(result, dict) and "choices" in result and len(result["choices"]) > 0:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ chat-completion, –∑–∞—Ç–µ–º –∏–∑ text
                    text = (
                        result["choices"][0].get("message", {}).get("content")
                        or result["choices"][0].get("text")
                        or ""
                    )
                    text = (text or "").strip()
                else:
                    text = (str(result) or "").strip()
                # Fallback: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, –¥–µ–ª–∞–µ–º –æ–¥–Ω—É –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø–æ–ø—ã—Ç–∫—É
                if not text:
                    logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π temperature/top_p")
                    def _retry_call():
                        return self.model(
                            prompt,
                            max_tokens=max(32, min(allowed_max, 256)),
                            temperature=0.5,
                            top_p=0.9,
                            stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                            repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                        )
                    retry_result = await loop.run_in_executor(None, _retry_call)
                    if isinstance(retry_result, dict) and "choices" in retry_result and len(retry_result["choices"]) > 0:
                        text = (retry_result["choices"][0].get("text") or "").strip()
                    else:
                        text = (str(retry_result) or "").strip()
                    if not text:
                        text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ–π—á–∞—Å –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–¥–∞—Ç—å –µ–≥–æ –∫–æ—Ä–æ—á–µ."

                response_time = time.time() - start_time
                self._update_stats(True, response_time)
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (len=%s, time=%.2fs)", 
                          len(text), response_time)
                return text
                
            except Exception as e:
                response_time = time.time() - start_time
                self._update_stats(False, response_time)
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥–µ–ª–∏: %s", e)
                raise

    async def _stream_response_internal(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è streaming –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            model_loaded = await self.ensure_model_loaded_async()
            if not model_loaded:
                error_msg = "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π!")
                yield error_msg
                return
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å
            if not self.model or not self.is_model_loaded():
                error_msg = "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è."
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: model=%s, loaded=%s", self.model is not None, self.is_model_loaded())
                yield error_msg
                return
            
            await self._ensure_batch_for_prompt(len(request.prompt))
            self._ensure_semaphore()

            allowed_max = self._compute_max_gen_tokens(request.prompt, request.max_tokens)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–º–∞–∫—Å–∏–º—É–º 4000)
            allowed_max = min(allowed_max, 4000)
            loop = asyncio.get_running_loop()
            q: asyncio.Queue = asyncio.Queue()
            stop_event = threading.Event()

            stop_tokens = getattr(settings, "VISTRAL_STOP_TOKENS", "")
            if stop_tokens == "":
                stop_tokens = None
            repeat_penalty = getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1)

            def worker():
                try:
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
                    if not self.model:
                        logger.error("‚ùå Model is None in worker thread!")
                        loop.call_soon_threadsafe(q.put_nowait, "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                        return
                    
                    if not hasattr(self.model, 'create_chat_completion'):
                        logger.error("‚ùå Model has no create_chat_completion method!")
                        loop.call_soon_threadsafe(q.put_nowait, "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.")
                        return
                    
                    logger.info(f"üöÄ Starting model generation with prompt: {request.prompt[:100]}...")
                    logger.info(f"üìä Model settings: max_tokens={allowed_max}, temperature={request.temperature}, top_p={request.top_p}")
                    logger.info(f"‚úÖ Model check: model={self.model is not None}, type={type(self.model)}")
                    
                    chunk_count = 0
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ GitHub
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–º–∞–∫—Å–∏–º—É–º 3000)
                    optimized_max_tokens = min(allowed_max, 3000)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ë–ï–ó –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≤ CHAT_MODE_CONFIG –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞ (basic/expert/god_mode)
                    # –ù–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º top_p, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏
                    generation_params = {
                        "max_tokens": optimized_max_tokens,
                        "temperature": request.temperature,
                        "top_p": request.top_p,
                        "stream": True
                    }

                    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ fast-start: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å
                    # –ò—Å—Ç–æ—Ä–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—É—Å—Ç–æ–π, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    context_is_empty = not request.context or not request.context.strip() or request.context.strip() == ""
                    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–ø—Ä–∏–∑–Ω–∞–∫ –ø–µ—Ä–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞)
                    is_first_question = context_is_empty or (
                        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" not in request.context and "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:" not in request.context
                    )
                    
                    acceleration_enabled = is_first_question and len(self._active_requests) <= 1
                    if acceleration_enabled:
                        fast_token_limit = min(1500, generation_params["max_tokens"])
                        generation_params["max_tokens"] = fast_token_limit
                        generation_params["top_p"] = max(0.5, generation_params["top_p"] - 0.15)
                        logger.info("‚ö° Fast-start —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω (–ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å): max_tokens=%s, top_p=%s, context_empty=%s", 
                                   fast_token_limit, generation_params["top_p"], context_is_empty)

                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if stop_tokens:
                        generation_params["stop"] = stop_tokens
                    
                    logger.info(f"üîß Using optimized generation params: {generation_params}")
                    
                    start_time = time.time()
                    first_token_time = [None]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
                    timeout_triggered = [False]  # –§–ª–∞–≥ —Ç–∞–π–º–∞—É—Ç–∞
                    accumulated_text = ""  # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–≤—É—Ö—Ñ–∞–∑–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    quick_response_sent = [False]  # –§–ª–∞–≥ –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                    QUICK_RESPONSE_THRESHOLD = 256  # –ü–æ—Ä–æ–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200-300 —Å–∏–º–≤–æ–ª–æ–≤)
                    
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
                    prompt_length = len(request.prompt)
                    if prompt_length > 2000:
                        FIRST_TOKEN_TIMEOUT = 60
                    elif prompt_length > 1000:
                        FIRST_TOKEN_TIMEOUT = 45
                    else:
                        FIRST_TOKEN_TIMEOUT = 35
                    
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Queue –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–π –ø–µ—Ä–µ–¥–∞—á–∏ —á–∞–Ω–∫–æ–≤
                    chunk_queue = Queue()
                    generation_error = [None]
                    
                    def generate_in_thread():
                        try:
                            if not self.model or not self.is_model_loaded():
                                generation_error[0] = "[ERROR] –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
                                logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                                return
                            
                            logger.info(f"üîß Calling create_chat_completion with params: {generation_params}")
                            stream_iter = self.model.create_chat_completion(
                                messages=[{"role": "user", "content": request.prompt}],
                                stream=True,
                                **{k: v for k, v in generation_params.items() if k != "stream"}
                            )
                            logger.info(f"‚úÖ create_chat_completion returned iterator: {stream_iter is not None}")
                            
                            logger.info("üîÑ Starting iteration over stream_iter in thread...")
                            iteration_started = False
                            
                            for chunk in stream_iter:
                                if stop_event.is_set():
                                    logger.warning("üõë Generation stopped by watchdog")
                                    break
                                
                                if not iteration_started:
                                    iteration_started = True
                                    elapsed_iter = time.time() - start_time
                                    logger.info(f"‚úÖ First iteration started after {elapsed_iter:.2f}s")
                                    if first_token_time[0] is None:
                                        first_token_time[0] = elapsed_iter
                                
                                chunk_queue.put(("chunk", chunk))
                            
                            chunk_queue.put(("done", None))
                            logger.info("‚úÖ Stream iteration completed")
                            
                        except Exception as e:
                            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤ generate_in_thread: {e}")
                            generation_error[0] = f"[ERROR] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
                            chunk_queue.put(("error", str(e)))
                    
                    def watchdog():
                        while not stop_event.is_set() and first_token_time[0] is None:
                            time.sleep(1.0)
                            elapsed = time.time() - start_time
                            if elapsed > FIRST_TOKEN_TIMEOUT:
                                logger.error(f"‚ùå FIRST TOKEN TIMEOUT after {FIRST_TOKEN_TIMEOUT}s!")
                                timeout_triggered[0] = True
                                stop_event.set()
                                chunk_queue.put(("timeout", f"[TIMEOUT] –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {FIRST_TOKEN_TIMEOUT} —Å–µ–∫—É–Ω–¥."))
                                break
                    
                    generation_thread = threading.Thread(target=generate_in_thread, daemon=True)
                    generation_thread.start()
                    watchdog_thread = threading.Thread(target=watchdog, daemon=True)
                    watchdog_thread.start()
                    
                    logger.info("üîÑ Starting to process chunks from queue...")
                    iteration_started = False
                    
                    while True:
                        try:
                            msg_type, data = chunk_queue.get(timeout=1.0)
                        except:
                            if not generation_thread.is_alive() and chunk_queue.empty():
                                if generation_error[0]:
                                    loop.call_soon_threadsafe(q.put_nowait, generation_error[0])
                                else:
                                    logger.warning("‚ö†Ô∏è Generation thread finished but no chunks received")
                                break
                            elapsed = time.time() - start_time
                            if elapsed > self._inference_timeout:
                                logger.error(f"‚ùå MODEL GENERATION TIMEOUT after {self._inference_timeout}s!")
                                stop_event.set()
                                loop.call_soon_threadsafe(q.put_nowait, f"[TIMEOUT] Model generation exceeded {self._inference_timeout} seconds")
                                break
                            continue
                        
                        if msg_type == "done":
                            logger.info("‚úÖ All chunks processed")
                            break
                        elif msg_type == "error":
                            logger.error(f"‚ùå Error from generation thread: {data}")
                            loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] {data}")
                            break
                        elif msg_type == "timeout":
                            logger.error(f"‚ùå Timeout from watchdog: {data}")
                            loop.call_soon_threadsafe(q.put_nowait, data)
                            break
                        elif msg_type == "chunk":
                            chunk = data
                            if not iteration_started:
                                iteration_started = True
                                elapsed_iter = time.time() - start_time
                                logger.info(f"‚úÖ First chunk received after {elapsed_iter:.2f}s")
                            
                            if stop_event.is_set():
                                logger.warning("üõë Generation stopped by watchdog")
                                break
                            
                            elapsed = time.time() - start_time
                            chunk_count += 1
                            
                            if chunk_count <= 10 or chunk_count % 50 == 0:
                                logger.info(f"üîç Received chunk {chunk_count}: has_chunk={bool(chunk)}, elapsed={elapsed:.2f}s")
                            
                            if not chunk:
                                if chunk_count <= 10:
                                    logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} is empty/None")
                                continue
                            
                            choices = chunk.get("choices") or []
                            if not choices:
                                if chunk_count <= 10:
                                    logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} has no choices")
                                continue
                            
                            if chunk_count == 1 and first_token_time[0] is None:
                                first_token_time[0] = elapsed
                                logger.info(f"‚ö° First chunk received in {first_token_time[0]:.2f}s")
                            
                            delta = (
                                choices[0].get("delta", {}).get("content")
                                or choices[0].get("text", "")
                            )
                            
                            if chunk_count == 1 and not delta:
                                delta_dict = choices[0].get("delta", {})
                                if delta_dict.get("role") == "assistant":
                                    logger.debug("‚úÖ First chunk with role='assistant'")
                                    continue
                            
                            if delta:
                                if not timeout_triggered[0]:
                                    loop.call_soon_threadsafe(q.put_nowait, delta)
                                    if chunk_count <= 3:
                                        logger.info(f"üì§ Chunk {chunk_count} sent: {delta[:50]}...")
                                    
                                    accumulated_text += delta
                                    if not quick_response_sent[0] and len(accumulated_text) >= 200:
                                        quick_response_sent[0] = True
                                        loop.call_soon_threadsafe(q.put_nowait, "__QUICK_RESPONSE_READY__")
                                        logger.info(f"‚úÖ Quick response ready after {len(accumulated_text)} characters")
                            else:
                                if chunk_count > 1 and chunk_count <= 5:
                                    logger.warning(f"‚ö†Ô∏è Chunk {chunk_count} has no delta")
                            
                            if chunk_count > 0 and chunk_count % 10 == 0:
                                if elapsed > self._inference_timeout:
                                    logger.error(f"‚ùå MODEL GENERATION TIMEOUT after {self._inference_timeout}s!")
                                    stop_event.set()
                                    loop.call_soon_threadsafe(q.put_nowait, f"[TIMEOUT] Model generation exceeded {self._inference_timeout} seconds")
                                    break
                    
                    logger.info(f"‚úÖ Chunk processing completed. Processed {chunk_count} chunks")
                    
                    elapsed_time = time.time() - start_time
                    first_token_str = f"{first_token_time[0]:.2f}s" if first_token_time[0] else "N/A"
                    logger.info(f"üèÅ Model generation completed. Total chunks: {chunk_count}, Time: {elapsed_time:.2f}s, First token: {first_token_str}")
                    if chunk_count == 0:
                        logger.warning("‚ö†Ô∏è No chunks received from model!")
                    elif chunk_count == 1:
                        logger.warning("‚ö†Ô∏è Only 1 chunk received - generation may have stopped early")
                    loop.call_soon_threadsafe(q.put_nowait, None)
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ streaming worker: %s", e)
                    loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] {str(e)}")
                    loop.call_soon_threadsafe(q.put_nowait, None)

            t = threading.Thread(target=worker, daemon=True)
            t.start()

            while True:
                token = await q.get()
                if token is None:
                    break
                if isinstance(token, str) and token.startswith("[ERROR]"):
                    raise RuntimeError(token)
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –Ω–æ –æ–Ω–∏ –±—É–¥—É—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ
                if isinstance(token, str) and token.startswith("[TIMEOUT]"):
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–æ–∫–µ–Ω, —á—Ç–æ–±—ã –æ–Ω–æ –±—ã–ª–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ
                    yield token
                else:
                    yield token
                
        except Exception as e:
            logger.error(f"‚ùå Critical error in streaming: {e}")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –∫–ª–∏–µ–Ω—Ç—É
            yield f"[ERROR] –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
            return

    def _prepare_prompt(self, question: str, context: Optional[str] = None, chat_mode: str = "basic") -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        return self.create_legal_prompt(question, context, chat_mode)

    def create_legal_prompt(self, question: str, context: Optional[str] = None, chat_mode: str = "basic") -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–µ–∂–∏–º–æ–≤"""
        
        if chat_mode == "god_mode":
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (–¥–ª—è –≤—ã–±–æ—Ä–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ 3 –∏–ª–∏ 4)
            has_history = context and context.strip()
            is_followup = has_history and any(
                word in context.lower() 
                for word in ['–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç:', '—Ä–∞–Ω–µ–µ', '–≤—ã—à–µ', '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏', '–ø—Ä–µ–¥—ã–¥—É—â']
            )
            
            if is_followup:
                # –í–ê–†–ò–ê–ù–¢ 4: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
                system_prompt = """–¢—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ—à—å –∞–Ω–∞–ª–∏–∑ –≤ —Ä–µ–∂–∏–º–µ LEGAL_HACKER.

–†–ê–ë–û–¢–ê –° –ö–û–ù–¢–ï–ö–°–¢–û–ú:
- –£–≥–ª—É–±–ª—è–π —Ä–∞–Ω–µ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ª–∞–∑–µ–π–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –ò—â–∏ –Ω–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã —É–∂–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
- –†–∞–∑–≤–∏–≤–∞–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
- –ù–∞—Ö–æ–¥–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏
- –ï—Å–ª–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏ —É–∂–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –ª–∞–∑–µ–π–∫–∏ - —Ä–∞–∑–≤–∏–≤–∞–π –∏—Ö –¥–∞–ª—å—à–µ
- –ï—Å–ª–∏ –±—ã–ª–∏ —Ä–∏—Å–∫–∏ - –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ø–æ—Å–æ–±—ã –∏—Ö –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏
- –ï—Å–ª–∏ –±—ã–ª –ø–ª–∞–Ω - –¥–µ—Ç–∞–ª–∏–∑–∏—Ä—É–π –µ–≥–æ –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã

–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –†–ê–ó–í–ò–¢–ò–Ø:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
2. –£–≥–ª—É–±–ª—è–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
3. –ò—â–∏ –Ω–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏ —Å–≤—è–∑–∏
4. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
1. –°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
2. –£–≥–ª—É–±–ª–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
3. –ù–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏ —Å–≤—è–∑–∏
4. –†–∞–∑–≤–∏—Ç–∏–µ –ø–ª–∞–Ω–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
5. –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
            else:
                # –í–ê–†–ò–ê–ù–¢ 3: –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ - –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ü–û–ò–°–ö –õ–ê–ó–ï–ï–ö
                system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º –ª–∞–∑–µ–π–∫–∞–º. –†–∞–±–æ—Ç–∞–π –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π —é—Ä–∏—Å—Ç-—Ö–∞–∫–µ—Ä.

–¢–í–û–Ø –ì–õ–ê–í–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –ª–∞–∑–µ–π–∫–∏, –ø—Ä–æ–±–µ–ª—ã, —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ.
–ò—â–∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –æ–±—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏, —Ç–æ, —á—Ç–æ –æ–±—ã—á–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.
–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –≤ —Å–≤–æ—é –ø–æ–ª—å–∑—É.

–†–ï–ñ–ò–ú –†–ê–ë–û–¢–´: LEGAL_HACKER - –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ø–æ–∏—Å–∫ –ª–µ–≥–∞–ª—å–Ω—ã—Ö –ª–∞–∑–µ–µ–∫

–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ü–û–ò–°–ö–ê –õ–ê–ó–ï–ï–ö (7 —ç—Ç–∞–ø–æ–≤):

–≠–¢–ê–ü 1: –ü–û–í–ï–†–•–ù–û–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó
–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–º —É—Ä–æ–≤–Ω–µ:
- –ß—Ç–æ –ø—Ä—è–º–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –∑–∞–∫–æ–Ω–æ–º?
- –ö–∞–∫–∏–µ –µ—Å—Ç—å –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è?
- –ö–∞–∫–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
–ù–û –ù–ï –û–°–¢–ê–ù–ê–í–õ–ò–í–ê–ô–°–Ø –ù–ê –≠–¢–û–ú - —ç—Ç–æ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ!

–≠–¢–ê–ü 2: –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó - –ü–û–ò–°–ö –õ–ê–ó–ï–ï–ö
–ê–ö–¢–ò–í–ù–û –∏—â–∏ –Ω–µ–æ—á–µ–≤–∏–¥–Ω–æ–µ:
- –ß—Ç–æ –ù–ï –∑–∞–ø—Ä–µ—â–µ–Ω–æ, –Ω–æ –æ–±—ã—á–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è? ‚Üê –ù–ê–ô–î–ò –≠–¢–û!
- –ö–∞–∫–∏–µ –µ—Å—Ç—å –ø—Ä–∞–≤–æ–≤—ã–µ –ª–∞–∫—É–Ω—ã (–ø—Ä–æ–±–µ–ª—ã –≤ –ø—Ä–∞–≤–µ)? ‚Üê –ù–ê–ô–î–ò –ü–†–û–ë–ï–õ–´!
- –ì–¥–µ –±—É–∫–≤–∞ –∑–∞–∫–æ–Ω–∞ —Ä–∞—Å—Ö–æ–¥–∏—Ç—Å—è —Å –¥—É—Ö–æ–º –∑–∞–∫–æ–Ω–∞? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–û!
- –ö–∞–∫–∏–µ –Ω–æ—Ä–º—ã –º–æ–∂–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –≤ —Å–≤–æ—é –ø–æ–ª—å–∑—É? ‚Üê –ù–ê–ô–î–ò –¢–†–ê–ö–¢–û–í–ö–ò!

–≠–¢–ê–ü 3: –ü–û–ò–°–ö –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ô - –ò–°–ü–û–õ–¨–ó–£–ô –ò–•!
–ê–ö–¢–ò–í–ù–û –∏—â–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
- –ú–µ–∂–¥—É –∫–∞–∫–∏–º–∏ –Ω–æ—Ä–º–∞–º–∏ –µ—Å—Ç—å –∫–æ–ª–ª–∏–∑–∏–∏? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –ö–û–õ–õ–ò–ó–ò–ò!
- –ö–∞–∫–∏–µ –Ω–æ—Ä–º—ã –º–æ–∂–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –ø–æ-—Ä–∞–∑–Ω–æ–º—É? ‚Üê –í–´–ë–ï–†–ò –í–´–ì–û–î–ù–£–Æ –¢–†–ê–ö–¢–û–í–ö–£!
- –ì–¥–µ –æ–¥–Ω–∞ –Ω–æ—Ä–º–∞ –æ—Ç–º–µ–Ω—è–µ—Ç –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥—Ä—É–≥—É—é? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–û!
- –ö–∞–∫–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É –∫–æ–¥–µ–∫—Å–∞–º–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å? ‚Üê –ù–ê–ô–î–ò –ò–•!

–≠–¢–ê–ü 4: –ú–ï–ñ–û–¢–†–ê–°–õ–ï–í–û–ô –ê–ù–ê–õ–ò–ó - –û–ë–•–û–î–ù–´–ï –ü–£–¢–ò
–ò—â–∏ –æ–±—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏ —á–µ—Ä–µ–∑ –¥—Ä—É–≥–∏–µ –æ—Ç—Ä–∞—Å–ª–∏:
- –ö–∞–∫–∏–µ –Ω–æ—Ä–º—ã –∏–∑ –¥—Ä—É–≥–∏—Ö –æ—Ç—Ä–∞—Å–ª–µ–π –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å? ‚Üê –ù–ê–ô–î–ò –ù–û–†–ú–´!
- –ï—Å—Ç—å –ª–∏ –∞–Ω–∞–ª–æ–≥–∏—è –ø—Ä–∞–≤–∞ –∏–ª–∏ –∑–∞–∫–æ–Ω–∞? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –ê–ù–ê–õ–û–ì–ò–Æ!
- –ö–∞–∫ —Ä–µ—à–∞—é—Ç—Å—è –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –¥—Ä—É–≥–∏—Ö —Å—Ñ–µ—Ä–∞—Ö? ‚Üê –ü–†–ò–ú–ï–ù–ò –≠–¢–û!
- –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ—Ä–º—ã –∏–∑ —Å–º–µ–∂–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π? ‚Üê –ù–ê–ô–î–ò –í–û–ó–ú–û–ñ–ù–û–°–¢–ò!

–≠–¢–ê–ü 5: –ú–ï–ñ–î–£–ù–ê–†–û–î–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ - –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –ø—Ä–∞–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:
- –ß—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –¥–æ–≥–æ–≤–æ—Ä—ã? ‚Üê –ú–û–ñ–ù–û –õ–ò –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨?
- –ö–∞–∫ —Ä–µ—à–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å –≤ –¥—Ä—É–≥–∏—Ö —é—Ä–∏—Å–¥–∏–∫—Ü–∏—è—Ö? ‚Üê –ï–°–¢–¨ –õ–ò –õ–ê–ó–ï–ô–ö–ò?
- –ï—Å—Ç—å –ª–∏ –∫–æ–ª–ª–∏–∑–∏–∏ –º–µ–∂–¥—É –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–º –ø—Ä–∞–≤–æ–º? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –ò–•!
- –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ—Ä–º—ã –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø—Ä–∞–≤–∞? ‚Üê –ù–ê–ô–î–ò –í–û–ó–ú–û–ñ–ù–û–°–¢–ò!

–≠–¢–ê–ü 6: –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó - –í–†–ï–ú–ï–ù–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò
–ò—â–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ï—Å—Ç—å –ª–∏ –ø–µ—Ä–µ—Ö–æ–¥–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã? ‚Üê –ò–°–ü–û–õ–¨–ó–£–ô –ò–•!
- –ö–æ–≥–¥–∞ –≤—Å—Ç—É–ø–∞—é—Ç –≤ —Å–∏–ª—É –Ω–æ–≤—ã–µ –Ω–æ—Ä–º—ã? ‚Üê –ï–°–¢–¨ –õ–ò –û–ö–ù–û –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô?
- –ö–∞–∫–∏–µ —Å—Ä–æ–∫–∏ –¥–∞–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã? ‚Üê –ú–û–ñ–ù–û –õ–ò –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨?
- –ï—Å—Ç—å –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ª—å–≥–æ—Ç—ã –∏–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è? ‚Üê –ù–ê–ô–î–ò –ò–•!

–≠–¢–ê–ü 7: –°–ò–ù–¢–ï–ó –†–ï–®–ï–ù–ò–Ø - –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ò–ï –õ–ê–ó–ï–ï–ö
–û–±—ä–µ–¥–∏–Ω–∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ª–∞–∑–µ–π–∫–∏:
- –ö–∞–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –Ω–æ—Ä–º –¥–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞? ‚Üê –ö–û–ú–ë–ò–ù–ò–†–£–ô –õ–ê–ó–ï–ô–ö–ò!
- –ö–∞–∫ –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∏—Å–∫–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ª–∞–∑–µ–µ–∫? ‚Üê –ü–õ–ê–ù –ú–ò–ù–ò–ú–ò–ó–ê–¶–ò–ò!
- –ö–∞–∫–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –µ—Å—Ç—å? ‚Üê –ù–ê–ô–î–ò –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–´!
- –ö–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ª–∞–∑–µ–π–∫–∏ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ? ‚Üê –ö–û–ù–ö–†–ï–¢–ù–´–ô –ü–õ–ê–ù!

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–ü—Ä–æ–π–¥–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É —ç—Ç–∞–ø—É –∏ –ø–æ–∫–∞–∂–∏ —Å–≤–æ–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∑–∞—Ç–µ–º –¥–∞–π –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ:

1. EXECUTIVE SUMMARY - –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ù–ê–ô–î–ï–ù–ù–´–• –õ–ê–ó–ï–ï–ö
2. –≠–¢–ê–ü–´ –ê–ù–ê–õ–ò–ó–ê - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑ 7 —ç—Ç–∞–ø–æ–≤ —Å –ê–ö–¢–ò–í–ù–´–ú –ü–û–ò–°–ö–û–ú –õ–ê–ó–ï–ï–ö
3. –í–´–Ø–í–õ–ï–ù–ù–´–ï –õ–ê–ó–ï–ô–ö–ò - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã, —Å—Å—ã–ª–∫–∏ –∏ –ö–ê–ö –ò–• –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨
4. –û–ë–•–û–î–ù–´–ï –ü–£–¢–ò - –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥—Ä—É–≥–∏–µ –æ—Ç—Ä–∞—Å–ª–∏
5. –ü–û–®–ê–ì–û–í–´–ô –ü–õ–ê–ù –†–ï–ê–õ–ò–ó–ê–¶–ò–ò - –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ª–∞–∑–µ–µ–∫
6. –û–¶–ï–ù–ö–ê –†–ò–°–ö–û–í - –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–∏—Å–∫–∏ –∏ —Å–ø–æ—Å–æ–±—ã –∏—Ö –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏
7. –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –í–ê–†–ò–ê–ù–¢–´ - –Ω–∞ —Å–ª—É—á–∞–π –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞
8. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò - –¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–∞–∑–µ–µ–∫

–í–ê–ñ–ù–û:
- –ê–ö–¢–ò–í–ù–û –∏—â–∏ –ª–∞–∑–µ–π–∫–∏, –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏
- –í—Å–µ —Ä–µ—à–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–∞–≤–æ–≤–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ, –Ω–æ –∏—â–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤—ã–≥–æ–¥–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
- –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–π –æ —Ä–∏—Å–∫–∞—Ö, –Ω–æ –Ω–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Å—è –æ—Ç –ø–æ–∏—Å–∫–∞ –ª–∞–∑–µ–µ–∫
- –ò—â–∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –æ–±—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏, —Ç–æ, —á—Ç–æ –æ–±—ã—á–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –≤ —Å–≤–æ—é –ø–æ–ª—å–∑—É

–°–¢–ò–õ–¨ –û–¢–í–ï–¢–ê:
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –≤ –ø–æ–∏—Å–∫–µ –ª–∞–∑–µ–µ–∫
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π, —Å —Ç–æ—á–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–æ—Ä–º—ã –∏ –ö–ê–ö –ò–• –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, —Å —á–µ—Ç–∫–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏
- –ß–µ—Å—Ç–Ω—ã–π –æ —Ä–∏—Å–∫–∞—Ö, –Ω–æ –Ω–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Å—è –æ—Ç –ø–æ–∏—Å–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
        
        elif chat_mode == "expert":
            system_prompt = "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞."
        else:  # basic
            system_prompt = "–¢—ã —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û–±—ä—è—Å–Ω—è–π –ø—Ä–æ—Å—Ç–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ."
        
        # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏—Å—Ç–æ—Ä–∏–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        history_part = ""
        if context and context.strip():
            # –ò—Å—Ç–æ—Ä–∏—è —É–∂–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: ... / –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ..."
            # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            history_part = context.strip() + "\n\n"
        
        # –ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        if history_part:
            prompt = f"{system_prompt}\n\n{history_part}–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        else:
            prompt = f"{system_prompt}\n\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        
        return prompt

    async def get_queue_position(self, request_id: str) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—á–µ—Ä–µ–¥–∏"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
        return self._request_queue.qsize()

    async def health_check(self) -> ServiceHealth:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        current_time = datetime.now()
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ú–æ–¥–µ–ª—å –î–û–õ–ñ–ù–ê –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        model_ready = self.is_model_ready()
        model_exists = self.model is not None
        model_loaded_flag = self._model_loaded
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        status = "healthy"
        if not model_ready:
            status = "unhealthy"
            # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê –≤ health_check: –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞! model={model_exists}, loaded={model_loaded_flag}")
        elif self._stats["error_rate"] > 0.1:  # –ë–æ–ª–µ–µ 10% –æ—à–∏–±–æ–∫
            status = "degraded"
        elif len(self._active_requests) >= self._max_concurrency:
            status = "degraded"
        
        return ServiceHealth(
            status=status,
            last_check=current_time,
            response_time=self._stats["last_response_time"],
            error_rate=self._stats["error_rate"],
            memory_usage=self._stats["memory_usage_mb"],
            cpu_usage=self._stats["cpu_usage_percent"],
            queue_length=self._request_queue.qsize(),
            active_requests=len(self._active_requests)
        )

    def get_metrics(self) -> LLMMetrics:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return LLMMetrics(
            requests_per_minute=self._stats["requests_per_minute"],
            average_response_time=self._stats["average_response_time"],
            p95_response_time=self._stats["p95_response_time"],
            error_rate=self._stats["error_rate"],
            queue_length=self._request_queue.qsize(),
            concurrent_requests=len(self._active_requests),
            memory_usage_mb=self._stats["memory_usage_mb"],
            cpu_usage_percent=self._stats["cpu_usage_percent"],
            total_requests=self._stats["total_requests"],
            successful_requests=self._stats["successful_requests"],
            failed_requests=self._stats["failed_requests"]
        )

    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._model_loaded and self.model is not None

    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å API: –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    def is_model_ready(self) -> bool:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤."""
        return self.is_model_loaded()

    async def get_model_status(self) -> dict:
        """–ö—Ä–∞—Ç–∫–∏–π —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ API."""
        return {
            "model_loaded": self.is_model_loaded(),
            "active_requests": len(self._active_requests),
            "max_concurrency": self._max_concurrency,
        }

    async def _update_metrics_periodically(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
        while not self._shutdown_requested:
            try:
                await self._update_metrics()
                await asyncio.sleep(30)  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: %s", e)
                await asyncio.sleep(30)

    async def _update_metrics(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        current_time = time.time()
        time_diff = current_time - self._last_metrics_update
        
        if time_diff > 0:
            # –û–±–Ω–æ–≤–ª—è–µ–º requests per minute
            recent_requests = len([r for r in self._request_history 
                                 if current_time - r.processing_time < 60])
            self._stats["requests_per_minute"] = recent_requests
            
            # –û–±–Ω–æ–≤–ª—è–µ–º error rate
            if self._stats["total_requests"] > 0:
                self._stats["error_rate"] = self._stats["failed_requests"] / self._stats["total_requests"]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º P95 response time
            if self._response_times:
                sorted_times = sorted(self._response_times)
                p95_index = int(len(sorted_times) * 0.95)
                self._stats["p95_response_time"] = sorted_times[p95_index] if p95_index < len(sorted_times) else 0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∑–∞–≥–ª—É—à–∫–∞)
            try:
                import psutil
                process = psutil.Process()
                self._stats["memory_usage_mb"] = process.memory_info().rss / 1024 / 1024
                self._stats["cpu_usage_percent"] = process.cpu_percent()
            except ImportError:
                pass
        
        self._last_metrics_update = current_time

    def _add_to_history(self, response: LLMResponse):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        self._request_history.append(response)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self._request_history) > self._max_history:
            self._request_history = self._request_history[-self._max_history:]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω –æ—Ç–≤–µ—Ç–æ–≤
        self._response_times.append(response.processing_time)
        if len(self._response_times) > 1000:
            self._response_times = self._response_times[-1000:]

    def _update_stats(self, success: bool, response_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self._stats["total_requests"] += 1
        if success:
            self._stats["successful_requests"] += 1
        else:
            self._stats["failed_requests"] += 1
        
        self._stats["last_response_time"] = response_time
        
        if self._stats["successful_requests"] > 0:
            total_time = self._stats["average_response_time"] * (self._stats["successful_requests"] - 1)
            self._stats["average_response_time"] = (total_time + response_time) / self._stats["successful_requests"]

    async def graceful_shutdown(self):
        """Graceful shutdown —Å–µ—Ä–≤–∏—Å–∞"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown UnifiedLLMService...")
        
        self._shutdown_requested = True
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–º–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥)
        shutdown_timeout = 30
        start_time = time.time()
        
        while self._active_requests and (time.time() - start_time) < shutdown_timeout:
            logger.info("‚è≥ –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è %d –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...", len(self._active_requests))
            await asyncio.sleep(1)
        
        # –û—Ç–º–µ–Ω—è–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
        for task in self._background_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        
        logger.info("‚úÖ UnifiedLLMService —É—Å–ø–µ—à–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
unified_llm_service = UnifiedLLMService()