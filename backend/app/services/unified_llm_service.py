"""
Unified LLM Service - –µ–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å saiga_service.py, saiga_service_improved.py, optimized_saiga_service.py
"""

import logging
import time
import threading
import asyncio
import uuid
from typing import Optional, AsyncGenerator, Any, Dict, List
from queue import Queue
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from ..core.config import settings

# –í–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å llama_cpp
from llama_cpp import Llama

logger = logging.getLogger(__name__)


class RequestPriority(Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3


@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
    id: str
    prompt: str
    context: Optional[str]
    user_id: str
    timestamp: datetime
    priority: RequestPriority = RequestPriority.NORMAL
    stream: bool = True
    max_tokens: int = 1024
    temperature: float = 0.3
    top_p: float = 0.8


@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    request_id: str
    content: str
    processing_time: float
    tokens_generated: int
    queue_time: float
    success: bool
    error: Optional[str] = None


@dataclass
class ServiceHealth:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    status: str  # "healthy", "degraded", "unhealthy"
    last_check: datetime
    response_time: float
    error_rate: float
    memory_usage: float
    cpu_usage: float
    queue_length: int
    active_requests: int


@dataclass
class LLMMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM"""
    requests_per_minute: float
    average_response_time: float
    p95_response_time: float
    error_rate: float
    queue_length: int
    concurrent_requests: int
    memory_usage_mb: float
    cpu_usage_percent: float
    total_requests: int
    successful_requests: int
    failed_requests: int


def _redact_for_logs(text: str, max_len: int = 120) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –∏ –º–∞—Å–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–æ–≥–æ–≤."""
    if not text:
        return ""
    s = text.strip()
    if len(s) <= max_len:
        return s.replace("\n", " ")
    return (s[:max_len//2] + " ... " + s[-max_len//2:]).replace("\n", " ")


def _estimate_tokens(text: str) -> int:
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: 1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞."""
    if not text:
        return 0
    return max(1, len(text) // 4)


class UnifiedLLMService:
    """–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ Vistral"""
    
    def __init__(self):
        self.model: Optional[Llama] = None
        self._model_loaded: bool = False
        self._load_lock = threading.Lock()
        self._async_semaphore: Optional[asyncio.Semaphore] = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._inference_timeout = getattr(settings, "VISTRAL_INFERENCE_TIMEOUT", 900)
        self._max_concurrency = getattr(settings, "VISTRAL_MAX_CONCURRENCY", 3)
        self._queue_size = getattr(settings, "VISTRAL_QUEUE_SIZE", 50)
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        self._request_queue = asyncio.PriorityQueue(maxsize=self._queue_size)
        self._active_requests: Dict[str, LLMRequest] = {}
        self._request_history: List[LLMResponse] = []
        self._max_history = 1000
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "p95_response_time": 0.0,
            "last_response_time": 0.0,
            "queue_length": 0,
            "concurrent_requests": 0,
            "requests_per_minute": 0.0,
            "error_rate": 0.0,
            "memory_usage_mb": 0.0,
            "cpu_usage_percent": 0.0
        }
        
        # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        self._last_metrics_update = time.time()
        self._response_times: List[float] = []
        
        # –§–ª–∞–≥ –¥–ª—è graceful shutdown
        self._shutdown_requested = False
        
        # Background tasks
        self._background_tasks: List[asyncio.Task] = []
        
    def _ensure_semaphore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self._async_semaphore is None:
            try:
                loop = asyncio.get_running_loop()
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
            except RuntimeError:
                self._async_semaphore = asyncio.Semaphore(self._max_concurrency)
    
    def _load_model(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Vistral"""
        if self._model_loaded and self.model is not None:
            return

        with self._load_lock:
            if self._model_loaded and self.model is not None:
                return
            try:
                import os
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ VISTRAL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                model_path = getattr(settings, "VISTRAL_MODEL_PATH", "")
                n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
                n_threads = getattr(settings, "VISTRAL_N_THREADS", 8)
                n_gpu_layers = getattr(settings, "VISTRAL_N_GPU_LAYERS", 0)
                
                if not model_path or not os.path.exists(model_path):
                    raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                
                logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Vistral –∏–∑ %s", model_path)
                logger.info("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_ctx=%s, n_threads=%s, max_concurrency=%s, queue_size=%s", 
                          n_ctx, n_threads, self._max_concurrency, self._queue_size)

                self.model = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_gpu_layers=n_gpu_layers,
                    logits_all=False,
                    use_mmap=True,
                    use_mlock=False,
                    verbose=False
                )
                self._model_loaded = True
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vistral —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Vistral: %s", e)
                raise

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedLLMService...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self.ensure_model_loaded_async()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            await self._start_background_tasks()
            
            logger.info("‚úÖ UnifiedLLMService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ UnifiedLLMService: %s", e)
            raise

    async def _start_background_tasks(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏"""
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        queue_processor = asyncio.create_task(self._process_request_queue())
        self._background_tasks.append(queue_processor)
        
        # –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        metrics_updater = asyncio.create_task(self._update_metrics_periodically())
        self._background_tasks.append(metrics_updater)
        
        logger.info("üîÑ –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∑–∞–ø—É—â–µ–Ω—ã")

    async def _process_request_queue(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
        while not self._shutdown_requested:
            try:
                # –ñ–¥–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                try:
                    priority, request = await asyncio.wait_for(
                        self._request_queue.get(), timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
                await self._process_single_request(request)
                
                # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
                self._request_queue.task_done()
                
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ –æ—á–µ—Ä–µ–¥–∏: %s", e)
                await asyncio.sleep(1)

    async def _process_single_request(self, request: LLMRequest):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            self._active_requests[request.id] = request
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            if request.stream:
                # –î–ª—è streaming –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
                logger.info(f"üîÑ Processing streaming request {request.id}")
                # Streaming –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ _stream_response_internal
                pass
            else:
                response_text = await self._generate_response_internal(
                    request.prompt,
                    request.max_tokens,
                    request.temperature,
                    request.top_p
                )
                
                # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç
                processing_time = time.time() - start_time
                response = LLMResponse(
                    request_id=request.id,
                    content=response_text,
                    processing_time=processing_time,
                    tokens_generated=_estimate_tokens(response_text),
                    queue_time=start_time - request.timestamp.timestamp(),
                    success=True
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self._add_to_history(response)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_response = LLMResponse(
                request_id=request.id,
                content="",
                processing_time=processing_time,
                tokens_generated=0,
                queue_time=start_time - request.timestamp.timestamp(),
                success=False,
                error=str(e)
            )
            self._add_to_history(error_response)
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ %s: %s", request.id, e)
            
        finally:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self._active_requests.pop(request.id, None)

    async def ensure_model_loaded_async(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        if self.is_model_loaded():
            return True
            
        loop = asyncio.get_running_loop()
        try:
            await loop.run_in_executor(None, self._load_model)
            if not self.is_model_loaded():
                raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ _load_model")
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ ensure_model_loaded_async")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _compute_max_gen_tokens(self, prompt: str, requested_max: int) -> int:
        """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç n_ctx –∏ –¥–ª–∏–Ω—ã prompt"""
        n_ctx = getattr(settings, "VISTRAL_N_CTX", 8192)
        prompt_tokens = _estimate_tokens(prompt)
        safety_margin = getattr(settings, "VISTRAL_TOKEN_MARGIN", 32)
        available = max(1, n_ctx - prompt_tokens - safety_margin)
        if requested_max > available:
            logger.debug("requested_max (%s) > available (%s) -> limiting to available", 
                        requested_max, available)
            return available
        return requested_max

    async def generate_response(
        self,
        prompt: str,
        context: Optional[str] = None,
        stream: bool = True,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8,
        user_id: str = "anonymous",
        priority: RequestPriority = RequestPriority.NORMAL
    ) -> AsyncGenerator[str, None]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π streaming"""
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
        request = LLMRequest(
            id=str(uuid.uuid4()),
            prompt=self._prepare_prompt(prompt, context),
            context=context,
            user_id=user_id,
            timestamp=datetime.now(),
            priority=priority,
            stream=stream,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        if stream:
            # Streaming —Ä–µ–∂–∏–º
            async for chunk in self._stream_response_internal(request):
                yield chunk
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            response = await self._generate_response_internal(
                request.prompt, max_tokens, temperature, top_p
            )
            yield response

    async def _generate_response_internal(
        self,
        prompt: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
        top_p: float = 0.8
    ) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        self._ensure_semaphore()
        await self.ensure_model_loaded_async()
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        assert self._async_semaphore is not None
        async with self._async_semaphore:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º max tokens
            allowed_max = self._compute_max_gen_tokens(prompt, max_tokens)

            if getattr(settings, "LOG_PROMPTS", False):
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) prompt=%s... max_tokens=%s temp=%s", 
                          _redact_for_logs(prompt, 120), allowed_max, temperature)
            else:
                logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (unified) max_tokens=%s temp=%s", allowed_max, temperature)

            loop = asyncio.get_running_loop()

            def _blocking_call():
                try:
                    result = self.model(
                        prompt,
                        max_tokens=allowed_max,
                        temperature=temperature,
                        top_p=top_p,
                        stop=getattr(settings, "VISTRAL_STOP_TOKENS", None),
                        repeat_penalty=getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1),
                    )
                    return result
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ blocking_call –º–æ–¥–µ–ª–∏: %s", e)
                    raise

            try:
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, _blocking_call), 
                    timeout=self._inference_timeout
                )
            except asyncio.TimeoutError:
                logger.error("‚è∞ Inference timeout after %s seconds", self._inference_timeout)
                raise RuntimeError(f"Inference timeout after {self._inference_timeout} seconds")

            try:
                text = ""
                if isinstance(result, dict) and "choices" in result and len(result["choices"]) > 0:
                    text = (result["choices"][0].get("text") or "").strip()
                else:
                    text = str(result)
                
                response_time = time.time() - start_time
                self._update_stats(True, response_time)
                logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (len=%s, time=%.2fs)", 
                          len(text), response_time)
                return text
                
            except Exception as e:
                response_time = time.time() - start_time
                self._update_stats(False, response_time)
                logger.exception("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥–µ–ª–∏: %s", e)
                raise

    async def _stream_response_internal(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è streaming –æ—Ç–≤–µ—Ç–∞"""
        try:
            await self.ensure_model_loaded_async()
            self._ensure_semaphore()

            allowed_max = self._compute_max_gen_tokens(request.prompt, request.max_tokens)
            loop = asyncio.get_running_loop()
            q: asyncio.Queue = asyncio.Queue()

            stop_tokens = getattr(settings, "VISTRAL_STOP_TOKENS", "")
            if stop_tokens == "":
                stop_tokens = None
            repeat_penalty = getattr(settings, "VISTRAL_REPEAT_PENALTY", 1.1)

            def worker():
                try:
                    logger.info(f"üöÄ Starting model generation with prompt: {request.prompt[:100]}...")
                    logger.info(f"üìä Model settings: max_tokens={allowed_max}, temperature={request.temperature}, top_p={request.top_p}")
                    
                    chunk_count = 0
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Vistral 24B
                    generation_params = {
                        "max_tokens": min(allowed_max, 2000),  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
                        "temperature": getattr(settings, "VISTRAL_TEMPERATURE", 0.3),
                        "top_p": getattr(settings, "VISTRAL_TOP_P", 0.8),
                        "stream": True
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if stop_tokens:
                        generation_params["stop"] = stop_tokens
                    
                    logger.info(f"üîß Using optimized generation params: {generation_params}")
                    
                    start_time = time.time()
                    for chunk in self.model(request.prompt, **generation_params):
                        # –¢–∞–π–º–∞—É—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                        if time.time() - start_time > self._inference_timeout:
                            logger.error(f"‚ùå MODEL GENERATION TIMEOUT after {self._inference_timeout}s! Force stopping...")
                            loop.call_soon_threadsafe(q.put_nowait, f"[TIMEOUT] Model generation exceeded {self._inference_timeout} seconds")
                            break
                            
                        chunk_count += 1
                        if not chunk:
                            logger.warning(f"‚ö†Ô∏è Empty chunk #{chunk_count}")
                            continue
                        choices = chunk.get("choices") or []
                        if not choices:
                            logger.warning(f"‚ö†Ô∏è No choices in chunk #{chunk_count}")
                            continue
                        delta = choices[0].get("text", "")
                        if delta:
                            logger.info(f"‚úÖ Generated token #{chunk_count}: '{delta[:50]}...'")
                            loop.call_soon_threadsafe(q.put_nowait, delta)
                        else:
                            logger.warning(f"‚ö†Ô∏è Empty delta in chunk #{chunk_count}")
                    
                    logger.info(f"üèÅ Model generation completed. Total chunks: {chunk_count}")
                    loop.call_soon_threadsafe(q.put_nowait, None)
                except Exception as e:
                    logger.exception("‚ùå –û—à–∏–±–∫–∞ –≤ streaming worker: %s", e)
                    loop.call_soon_threadsafe(q.put_nowait, f"[ERROR] {str(e)}")
                    loop.call_soon_threadsafe(q.put_nowait, None)

            t = threading.Thread(target=worker, daemon=True)
            t.start()

            while True:
                token = await q.get()
                if token is None:
                    break
                if isinstance(token, str) and token.startswith("[ERROR]"):
                    raise RuntimeError(token)
                yield token
                
        except Exception as e:
            logger.error(f"‚ùå Critical error in streaming: {e}")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –∫–ª–∏–µ–Ω—Ç—É
            yield f"[ERROR] –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
            return

    def _prepare_prompt(self, question: str, context: Optional[str] = None) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        return self.create_legal_prompt(question, context)

    def create_legal_prompt(self, question: str, context: Optional[str] = None) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        special_instructions = ""
        question_lower = (question or "").lower()
        
        if "–∏–ø" in question_lower or "–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å" in question_lower:
            special_instructions = "\n# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: —Å–º. —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ (–Ω–µ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å –≤ –∫–æ–¥–µ)\n"
        
        prompt = f"""–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ, —Ç–æ—á–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –¢–û–ß–ù–û–°–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–ò:
- –ù–ò–ö–û–ì–î–ê –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π –∑–∞–∫–æ–Ω—ã, —Å—Ç–∞—Ç—å–∏, —Å—Ä–æ–∫–∏ –∏–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ "—É—Ç–æ—á–Ω–∏—Ç–µ –≤ –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∞—Ö"
- –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 2024 –≥–æ–¥
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–∫–æ–Ω—ã –†–§
- –£–∫–∞–∑—ã–≤–∞–π —Ç–æ—á–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–≤
- –°—Ä–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ (–¥–Ω–∏, –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü—ã, –ù–ï –≥–æ–¥—ã)
- –ù–ï —É–ø–æ–º–∏–Ω–∞–π –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (consultant.ru, garant.ru –∏ —Ç.–¥.)
- –ù–ï –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–µ–±-—Å–∞–π—Ç—ã –≤ –æ—Ç–≤–µ—Ç
- –ù–ï –ü–£–¢–ê–ô –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π!
- –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ - —Å–∫–∞–∂–∏ "—Å—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"

{special_instructions}

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:
- –î–∞–≤–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (1000-2000 —Å–ª–æ–≤)
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —è–∑—ã–∫, –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É
- –ü—Ä–∏–≤–æ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–æ–≤ –∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ –Ω–æ–º–µ—Ä–∞–º–∏
- –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å —á–µ—Ç–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
- –û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
- –í–∫–ª—é—á–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ—á–Ω—ã—Ö —Å—Ä–æ–∫–∞—Ö, —à—Ç—Ä–∞—Ñ–∞—Ö, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
- –î–∞–≤–∞–π –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏
- –ù–ï –ü–û–í–¢–û–†–Ø–ô–°–Ø - –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
- –ó–ê–í–ï–†–®–ê–ô –æ—Ç–≤–µ—Ç –∫—Ä–∞—Ç–∫–∏–º —Ä–µ–∑—é–º–µ

–ü–æ–º–Ω–∏: –ª—É—á—à–µ —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∑–∞—Ç—å "—É—Ç–æ—á–Ω–∏—Ç–µ –≤ –Ω–∞–ª–æ–≥–æ–≤–æ–π/—Å—É–¥–µ/–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–µ", —á–µ–º –¥–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
        
        if context:
            context_section = f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ===\n{context}\n=== –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ===\n\n"
            prompt = prompt.replace(f"–í–æ–ø—Ä–æ—Å: {question}", f"{context_section}–í–æ–ø—Ä–æ—Å: {question}")
        
        return prompt

    async def get_queue_position(self, request_id: str) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—á–µ—Ä–µ–¥–∏"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
        return self._request_queue.qsize()

    async def health_check(self) -> ServiceHealth:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        current_time = datetime.now()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        status = "healthy"
        if not self.is_model_loaded():
            status = "unhealthy"
        elif self._stats["error_rate"] > 0.1:  # –ë–æ–ª–µ–µ 10% –æ—à–∏–±–æ–∫
            status = "degraded"
        elif len(self._active_requests) >= self._max_concurrency:
            status = "degraded"
        
        return ServiceHealth(
            status=status,
            last_check=current_time,
            response_time=self._stats["last_response_time"],
            error_rate=self._stats["error_rate"],
            memory_usage=self._stats["memory_usage_mb"],
            cpu_usage=self._stats["cpu_usage_percent"],
            queue_length=self._request_queue.qsize(),
            active_requests=len(self._active_requests)
        )

    def get_metrics(self) -> LLMMetrics:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return LLMMetrics(
            requests_per_minute=self._stats["requests_per_minute"],
            average_response_time=self._stats["average_response_time"],
            p95_response_time=self._stats["p95_response_time"],
            error_rate=self._stats["error_rate"],
            queue_length=self._request_queue.qsize(),
            concurrent_requests=len(self._active_requests),
            memory_usage_mb=self._stats["memory_usage_mb"],
            cpu_usage_percent=self._stats["cpu_usage_percent"],
            total_requests=self._stats["total_requests"],
            successful_requests=self._stats["successful_requests"],
            failed_requests=self._stats["failed_requests"]
        )

    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._model_loaded and self.model is not None

    async def _update_metrics_periodically(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
        while not self._shutdown_requested:
            try:
                await self._update_metrics()
                await asyncio.sleep(30)  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: %s", e)
                await asyncio.sleep(30)

    async def _update_metrics(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        current_time = time.time()
        time_diff = current_time - self._last_metrics_update
        
        if time_diff > 0:
            # –û–±–Ω–æ–≤–ª—è–µ–º requests per minute
            recent_requests = len([r for r in self._request_history 
                                 if current_time - r.processing_time < 60])
            self._stats["requests_per_minute"] = recent_requests
            
            # –û–±–Ω–æ–≤–ª—è–µ–º error rate
            if self._stats["total_requests"] > 0:
                self._stats["error_rate"] = self._stats["failed_requests"] / self._stats["total_requests"]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º P95 response time
            if self._response_times:
                sorted_times = sorted(self._response_times)
                p95_index = int(len(sorted_times) * 0.95)
                self._stats["p95_response_time"] = sorted_times[p95_index] if p95_index < len(sorted_times) else 0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∑–∞–≥–ª—É—à–∫–∞)
            try:
                import psutil
                process = psutil.Process()
                self._stats["memory_usage_mb"] = process.memory_info().rss / 1024 / 1024
                self._stats["cpu_usage_percent"] = process.cpu_percent()
            except ImportError:
                pass
        
        self._last_metrics_update = current_time

    def _add_to_history(self, response: LLMResponse):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        self._request_history.append(response)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self._request_history) > self._max_history:
            self._request_history = self._request_history[-self._max_history:]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω –æ—Ç–≤–µ—Ç–æ–≤
        self._response_times.append(response.processing_time)
        if len(self._response_times) > 1000:
            self._response_times = self._response_times[-1000:]

    def _update_stats(self, success: bool, response_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self._stats["total_requests"] += 1
        if success:
            self._stats["successful_requests"] += 1
        else:
            self._stats["failed_requests"] += 1
        
        self._stats["last_response_time"] = response_time
        
        if self._stats["successful_requests"] > 0:
            total_time = self._stats["average_response_time"] * (self._stats["successful_requests"] - 1)
            self._stats["average_response_time"] = (total_time + response_time) / self._stats["successful_requests"]

    async def graceful_shutdown(self):
        """Graceful shutdown —Å–µ—Ä–≤–∏—Å–∞"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown UnifiedLLMService...")
        
        self._shutdown_requested = True
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–º–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥)
        shutdown_timeout = 30
        start_time = time.time()
        
        while self._active_requests and (time.time() - start_time) < shutdown_timeout:
            logger.info("‚è≥ –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è %d –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...", len(self._active_requests))
            await asyncio.sleep(1)
        
        # –û—Ç–º–µ–Ω—è–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
        for task in self._background_tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        
        logger.info("‚úÖ UnifiedLLMService —É—Å–ø–µ—à–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
unified_llm_service = UnifiedLLMService()