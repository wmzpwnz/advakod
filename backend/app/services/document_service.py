"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
–ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã
"""

import logging
import os
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
import hashlib
import uuid
from datetime import datetime
import tempfile
import requests
from urllib.parse import urlparse
import mimetypes

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ —Ñ–∞–π–ª–æ–≤
import pypdf
import pdfplumber
from docx import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from .embeddings_service import embeddings_service
from .vector_store_service import vector_store_service
from .document_validator import document_validator
from .ai_document_validator import ai_document_validator
from .hybrid_document_validator import hybrid_document_validator
from .document_versioning import document_versioning_service
from .simple_expert_rag import simple_expert_rag
from .pdf_ocr_service import pdf_ocr_service

logger = logging.getLogger(__name__)

class DocumentService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap=200,  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            length_function=len,
            separators=["\n\n", "\n", ". ", "? ", "! ", " ", ""]
        )
        self.supported_extensions = {'.pdf', '.docx', '.txt', '.md'}
        self.validation_method = "none"  # hybrid, ai, rules, none
        
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "supported_extensions": list(self.supported_extensions),
            "chunk_size": self.text_splitter._chunk_size,
            "chunk_overlap": self.text_splitter._chunk_overlap,
            "validation_method": self.validation_method,
            "hybrid_stats": hybrid_document_validator.get_validation_stats()
        }
    
    def set_validation_method(self, method: str):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if method not in ["hybrid", "ai", "rules", "none"]:
            raise ValueError("–ú–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å: hybrid, ai, rules –∏–ª–∏ none")
        
        self.validation_method = method
        logger.info(f"–ú–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {method}")
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def _get_pdf_page_count(self, file_path: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ PDF —Ñ–∞–π–ª–µ"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                return len(pdf_reader.pages)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF {file_path}: {e}")
            return 0
    
    def _get_docx_page_count(self, file_path: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ DOCX —Ñ–∞–π–ª–µ"""
        try:
            doc = Document(file_path)
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –∏ –¥–µ–ª–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            paragraphs = len(doc.paragraphs)
            # –ü—Ä–∏–º–µ—Ä–Ω–æ 20-30 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            estimated_pages = max(1, paragraphs // 25)
            return estimated_pages
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã DOCX {file_path}: {e}")
            return 0
    
    def _extract_text_from_pdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤"""
        try:
            # –ú–µ—Ç–æ–¥ 1: pdfplumber (–±–æ–ª–µ–µ –º–æ—â–Ω—ã–π)
            text = self._extract_text_with_pdfplumber(file_path)
            if text and len(text.strip()) > 100:
                logger.info(f"pdfplumber –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text.strip()
            
            # –ú–µ—Ç–æ–¥ 2: pypdf (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
            text = self._extract_text_with_pypdf(file_path)
            if text and len(text.strip()) > 100:
                logger.info(f"pypdf –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text.strip()
            
            # –ú–µ—Ç–æ–¥ 3: OCR (–µ—Å–ª–∏ –¥—Ä—É–≥–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏)
            logger.info("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–±—É–µ–º OCR")
            ocr_text = pdf_ocr_service.extract_text_from_pdf(file_path)
            
            if ocr_text and len(ocr_text.strip()) > 50:
                logger.info(f"OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ: {len(ocr_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return ocr_text.strip()
            else:
                logger.warning("–í—Å–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                return text.strip() if text else ""
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {file_path}: {e}")
            return ""
    
    def _extract_text_with_pdfplumber(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –ø–æ–º–æ—â—å—é pdfplumber"""
        try:
            text_parts = []
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(page_text)
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (pdfplumber): {e}")
                        continue
            return '\n\n'.join(text_parts) if text_parts else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ pdfplumber –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return ""
    
    def _extract_text_with_pypdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –ø–æ–º–æ—â—å—é pypdf"""
        try:
            text_parts = []
            with open(file_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(page_text)
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (pypdf): {e}")
                        continue
            return '\n\n'.join(text_parts) if text_parts else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ pypdf –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return ""
    
    def _extract_text_from_docx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        try:
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX: {file_path}")
            doc = Document(file_path)
            text = ""
            paragraph_count = 0
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                    text += paragraph.text + "\n"
                    paragraph_count += 1
            
            logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {paragraph_count} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            result = text.strip()
            logger.info(f"‚úÖ DOCX –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX {file_path}: {e}")
            logger.error(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return ""
    
    def _extract_text_from_txt(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ TXT —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except UnicodeDecodeError:
            # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for encoding in ['cp1251', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        return file.read().strip()
                except:
                    continue
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ñ–∞–π–ª–∞ {file_path}")
            return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è TXT —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return ""
    
    def extract_text_from_file(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ç–∏–ø–∞"""
        if not os.path.exists(file_path):
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return ""
        
        file_extension = Path(file_path).suffix.lower()
        logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file_path} (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {file_extension})")
        
        if file_extension not in self.supported_extensions:
            logger.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_extension}")
            logger.error(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {self.supported_extensions}")
            return ""
        
        logger.info(f"üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
        
        if file_extension == '.pdf':
            result = self._extract_text_from_pdf(file_path)
        elif file_extension == '.docx':
            result = self._extract_text_from_docx(file_path)
        elif file_extension in ['.txt', '.md']:
            result = self._extract_text_from_txt(file_path)
        else:
            logger.error(f"–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è {file_extension} –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
            return ""
        
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
        return result
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        if not text.strip():
            return []
        
        try:
            chunks = self.text_splitter.split_text(text)
            logger.info(f"üìù –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return chunks
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏: {e}")
            return [text]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫
    
    async def process_file(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_path}")
        
        if not os.path.exists(file_path):
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return {"success": False, "error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"}
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            logger.info(f"üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞...")
            text = self.extract_text_from_file(file_path)
            logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if not text:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞")
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞"}
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ
            file_info = Path(file_path)
            logger.info(f"üîç –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –º–µ—Ç–æ–¥–æ–º: {self.validation_method}")
            
            if self.validation_method == "none":
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –ø—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                validation_result = {
                    "is_valid": True,
                    "document_type": "unknown",
                    "confidence": 1.0,
                    "reason": "–í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞",
                    "legal_score": 1.0,
                    "invalid_score": 0.0
                }
            elif self.validation_method == "hybrid":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = await hybrid_document_validator.validate_document(text, file_info.name)
            elif self.validation_method == "ai":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = await ai_document_validator.validate_document(text, file_info.name)
            else:  # rules
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = document_validator.validate_document(text, file_info.name)
            
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {validation_result}")
            
            if not validation_result["is_valid"]:
                logger.warning(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: {validation_result['reason']}")
                return {
                    "success": False, 
                    "error": f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ: {validation_result['reason']}",
                    "validation_details": validation_result,
                    "suggestions": validation_result.get("suggestions", [])
                }
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            logger.info(f"‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏...")
            chunks = self.split_text_into_chunks(text)
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            
            if not chunks:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏")
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            file_info = Path(file_path)
            file_hash = self._calculate_file_hash(file_path)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            pages_count = 0
            if file_info.suffix.lower() == '.pdf':
                pages_count = self._get_pdf_page_count(file_path)
            elif file_info.suffix.lower() == '.docx':
                pages_count = self._get_docx_page_count(file_path)
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_id = f"doc_{file_hash[:8]}"
            version = document_versioning_service.create_document_version(
                document_id=document_id,
                text=text,
                filename=file_info.name,
                file_hash=file_hash,
                metadata={"original_filename": file_info.name}
            )
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è ChromaDB (—Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ —Ç–∏–ø—ã)
            base_metadata = {
                "filename": file_info.name,
                "file_extension": file_info.suffix.lower(),
                "file_size": int(os.path.getsize(file_path)),
                "file_hash": file_hash,
                "processed_at": datetime.now().isoformat(),
                "chunks_count": int(len(chunks)),
                "total_length": int(len(text)),
                "document_type": str(validation_result["document_type"]),
                "validation_confidence": float(validation_result["confidence"]),
                "legal_score": float(validation_result.get("legal_score", 0.0)),
                "is_validated": True,
                "document_id": document_id,
                "version": str(version.version),
                "status": str(version.status),
                "is_draft": version.status == "draft",
                "pages": int(pages_count) if pages_count > 0 else None
            }
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            base_metadata = {k: v for k, v in base_metadata.items() if v is not None}
            
            if metadata:
                # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                filtered_metadata = {k: v for k, v in metadata.items() if v is not None}
                base_metadata.update(filtered_metadata)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ –≥–æ—Ç–æ–≤–∞
            if not vector_store_service.is_ready():
                logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
                vector_store_service.initialize()
                logger.info(f"Vector store ready after init: {vector_store_service.is_ready()}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            added_count = 0
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î")
            for i, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": int(i),
                    "chunk_length": int(len(chunk)),
                    "is_chunk": True
                })
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                chunk_metadata = {k: v for k, v in chunk_metadata.items() if v is not None}
                
                chunk_id = f"{file_hash}_{i}"
                logger.info(f"–î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ {i+1}/{len(chunks)}: {chunk_id}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º
                if not vector_store_service.is_ready():
                    logger.warning(f"Vector store –Ω–µ –≥–æ—Ç–æ–≤ –¥–ª—è —á–∞–Ω–∫–∞ {i+1}, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º...")
                    vector_store_service.initialize()
                
                success = vector_store_service.add_document(
                    content=chunk,
                    metadata=chunk_metadata,
                    document_id=chunk_id
                )
                logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞ {i+1}: {success}")
                
                if success:
                    added_count += 1
            
            logger.info(f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path} ({added_count}/{len(chunks)} —á–∞–Ω–∫–æ–≤)")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ simple_expert_rag
            try:
                from .simple_expert_rag import LegalMetadata
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è simple_expert_rag
                legal_metadata = LegalMetadata(
                    source=file_info.name,
                    ingested_at=datetime.now()
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ simple_expert_rag
                simple_rag_result = await simple_expert_rag.add_document_with_text(
                    text_content=text,
                    metadata=legal_metadata
                )
                
                if simple_rag_result.get('success'):
                    logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ simple_expert_rag: {simple_rag_result.get('chunks_created', 0)} —á–∞–Ω–∫–æ–≤")
                else:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ simple_expert_rag: {simple_rag_result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å simple_expert_rag: {e}")
            
            return {
                "success": True,
                "file_path": file_path,
                "chunks_added": added_count,
                "total_chunks": len(chunks),
                "file_hash": file_hash,
                "text_length": len(text),
                "document_id": document_id
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return {"success": False, "error": str(e)}
    
    async def process_directory(self, directory_path: str, recursive: bool = True) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        if not os.path.exists(directory_path):
            return {"success": False, "error": f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory_path}"}
        
        files_found = []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã
        if recursive:
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if Path(file_path).suffix.lower() in self.supported_extensions:
                        files_found.append(file_path)
        else:
            for file in os.listdir(directory_path):
                file_path = os.path.join(directory_path, file)
                if os.path.isfile(file_path) and Path(file_path).suffix.lower() in self.supported_extensions:
                    files_found.append(file_path)
        
        if not files_found:
            return {"success": False, "error": "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}
        
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(files_found)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
        results = []
        for file_path in files_found:
            result = await self.process_file(file_path)
            results.append(result)
        
        successful = [r for r in results if r.get("success", False)]
        failed = [r for r in results if not r.get("success", False)]
        
        return {
            "success": True,
            "directory_path": directory_path,
            "files_found": len(files_found),
            "files_processed": len(successful),
            "files_failed": len(failed),
            "results": results
        }
    
    async def add_text_document(self, 
                              title: str, 
                              content: str, 
                              metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é"""
        if not content.strip():
            return {"success": False, "error": "–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç"}
        
        try:
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ
            if self.validation_method == "none":
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –ø—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                validation_result = {
                    "is_valid": True,
                    "document_type": "unknown",
                    "confidence": 1.0,
                    "reason": "–í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞",
                    "legal_score": 1.0,
                    "invalid_score": 0.0
                }
            elif self.validation_method == "hybrid":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = await hybrid_document_validator.validate_document(content, title)
            elif self.validation_method == "ai":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = await ai_document_validator.validate_document(content, title)
            else:  # rules
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                validation_result = document_validator.validate_document(content, title)
            
            if not validation_result["is_valid"]:
                return {
                    "success": False, 
                    "error": f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ: {validation_result['reason']}",
                    "validation_details": validation_result,
                    "suggestions": validation_result.get("suggestions", [])
                }
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.split_text_into_chunks(content)
            if not chunks:
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            doc_id = str(uuid.uuid4())
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            base_metadata = {
                "title": title,
                "document_id": doc_id,
                "content_hash": content_hash,
                "added_at": datetime.now().isoformat(),
                "chunks_count": len(chunks),
                "total_length": len(content),
                "source": "direct_input",
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                "document_type": validation_result["document_type"],
                "validation_confidence": validation_result["confidence"],
                "legal_score": validation_result.get("legal_score", 0.0),
                "is_validated": True
            }
            
            if metadata:
                base_metadata.update(metadata)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ –≥–æ—Ç–æ–≤–∞
            if not vector_store_service.is_ready():
                logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
                vector_store_service.initialize()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            added_count = 0
            for i, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": i,
                    "chunk_length": len(chunk),
                    "is_chunk": True
                })
                
                chunk_id = f"{doc_id}_{i}"
                success = vector_store_service.add_document(
                    content=chunk,
                    metadata=chunk_metadata,
                    document_id=chunk_id
                )
                
                if success:
                    added_count += 1
            
            logger.info(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {title} ({added_count}/{len(chunks)} —á–∞–Ω–∫–æ–≤)")
            
            return {
                "success": True,
                "title": title,
                "document_id": doc_id,
                "chunks_added": added_count,
                "total_chunks": len(chunks),
                "content_hash": content_hash
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {"success": False, "error": str(e)}

    async def process_url(self, url: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ URL"""
        try:
            logger.info(f"üåê –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É URL: {url}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return {"success": False, "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL"}
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ URL
            content_type = self._get_content_type_from_url(url)
            logger.info(f"üìÑ –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_type}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            if content_type == "html":
                text = await self._extract_text_from_html_url(url)
            else:
                text = await self._extract_text_from_file_url(url)
            
            if not text:
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ URL"}
            
            logger.info(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ URL")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as tmp_file:
                tmp_file.write(text)
                tmp_file_path = tmp_file.name
            
            try:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                result = await self.add_text_document(
                    title=f"url_document_{uuid.uuid4().hex[:8]}.txt",
                    content=text,
                    metadata={
                        "source_type": "url",
                        "source_url": url,
                        "content_type": content_type,
                        "downloaded_at": datetime.now().isoformat()
                    }
                )
                
                if result.get('success'):
                    result['source_url'] = url
                    result['content_type'] = content_type
                
                return result
                
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL {url}: {e}")
            return {"success": False, "error": str(e)}
    
    def _get_content_type_from_url(self, url: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ URL"""
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        if path.endswith('.pdf'):
            return 'pdf'
        elif path.endswith(('.doc', '.docx')):
            return 'docx'
        elif path.endswith(('.txt', '.text')):
            return 'txt'
        elif path.endswith(('.md', '.markdown')):
            return 'md'
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º HTML
            return 'html'
    
    async def _extract_text_from_html_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            logger.info(f"üåê –ó–∞–≥—Ä—É–∂–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
            response.encoding = response.apparent_encoding or 'utf-8'
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML
            import re
            html_content = response.text
            
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
            html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
            
            # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
            clean_text = re.sub(r'<[^>]+>', '', html_content)
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
            import html
            clean_text = html.unescape(clean_text)
            
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            clean_text = re.sub(r'\s+', ' ', clean_text)
            clean_text = clean_text.strip()
            
            logger.info(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ HTML")
            return clean_text
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML {url}: {e}")
            return ""
    
    async def _extract_text_from_file_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ URL"""
        try:
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            content_type = response.headers.get('content-type', '').lower()
            
            if 'pdf' in content_type or url.lower().endswith('.pdf'):
                return await self._extract_text_from_pdf_bytes(response.content)
            elif 'msword' in content_type or 'officedocument' in content_type or url.lower().endswith(('.doc', '.docx')):
                return await self._extract_text_from_docx_bytes(response.content)
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∫–∞–∫ —Ç–µ–∫—Å—Ç
                try:
                    text = response.text
                    logger.info(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                    return text
                except:
                    return ""
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {url}: {e}")
            return ""
    
    async def _extract_text_from_pdf_bytes(self, pdf_bytes: bytes) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –±–∞–π—Ç–æ–≤"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp_file:
                tmp_file.write(pdf_bytes)
                tmp_file_path = tmp_file.name
            
            try:
                return await self._extract_text_from_pdf(tmp_file_path)
            finally:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF –±–∞–π—Ç–æ–≤: {e}")
            return ""
    
    async def _extract_text_from_docx_bytes(self, docx_bytes: bytes) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX –±–∞–π—Ç–æ–≤"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as tmp_file:
                tmp_file.write(docx_bytes)
                tmp_file_path = tmp_file.name
            
            try:
                return await self._extract_text_from_docx(tmp_file_path)
            finally:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX –±–∞–π—Ç–æ–≤: {e}")
            return ""

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
document_service = DocumentService()
