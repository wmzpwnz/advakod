"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π
"""

import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import json

from .document_service import DocumentService
from .vector_store_service import VectorStoreService

logger = logging.getLogger(__name__)

class CodesRAGIntegration:
    def __init__(self, codes_dir: str = "data/codes_downloads"):
        self.codes_dir = Path(codes_dir)
        self.document_service = DocumentService()
        self.vector_store = VectorStoreService()
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –∏–ª–∏ /app/data
        import os
        base_dir = os.getenv("DATA_DIR", "/app/data")
        self.metadata_dir = Path(base_dir) / "rag_integration" / "metadata"
        try:
            self.metadata_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError) as e:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            import tempfile
            temp_base = Path(tempfile.gettempdir()) / "rag_integration" / "metadata"
            temp_base.mkdir(parents=True, exist_ok=True)
            self.metadata_dir = temp_base
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ {base_dir}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é: {temp_base}")

    def _load_metadata(self, file_path: Path) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            metadata_file = file_path.with_suffix('.json')
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {file_path.name}: {e}")
        return None
    
    async def integrate_codex(self, file_path: Path) -> Dict:
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –∫–æ–¥–µ–∫—Å –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            logger.info(f"üìÑ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–¥–µ–∫—Å–∞: {file_path.name}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞
            metadata = self._load_metadata(file_path)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è DocumentService
            doc_metadata = {
                'source': str(file_path),
                'type': 'codex',
                'processed_at': datetime.now().isoformat()
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ API –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if metadata:
                doc_metadata.update({
                    'codex_name': metadata.get('codex_name'),
                    'document_name': metadata.get('name'),
                    'document_title': metadata.get('title'),
                    'document_number': metadata.get('number'),
                    'document_date': metadata.get('document_date'),
                    'publish_date': metadata.get('publish_date'),
                    'view_date': metadata.get('view_date'),
                    'document_type': metadata.get('document_type'),
                    'pages_count': metadata.get('pages_count'),
                    'signatory_authorities': metadata.get('signatory_authorities', []),
                    'eo_number': metadata.get('eo_number'),
                    'source_url': metadata.get('source_url'),
                    'filename': metadata.get('file_name', file_path.name)
                })
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ DocumentService.process_file (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥)
            result = await self.document_service.process_file(
                str(file_path),
                metadata=doc_metadata
            )
            
            if not result.get('success', False):
                error_msg = result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {error_msg}")
                return {
                    'success': False,
                    'error': error_msg,
                    'file': str(file_path)
                }
            
            # process_file —É–∂–µ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            document_id = result.get('document_id', '')
            chunks_count = result.get('chunks_count', 0)
            
            logger.info(f"‚úÖ –ö–æ–¥–µ–∫—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω: {file_path.name} (ID: {document_id}, —á–∞–Ω–∫–æ–≤: {chunks_count})")
            
            return {
                'success': True,
                'file': str(file_path),
                'document_id': document_id,
                'chunks_count': chunks_count,
                'vector_ids': result.get('vector_ids', [])
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ {file_path.name}: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'file': str(file_path)
            }

    async def integrate_all_codexes(self) -> Dict:
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–¥–µ–∫—Å—ã –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        logger.info("üîó –ù–∞—á–∞–ª–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π")
        
        if not self.codes_dir.exists():
            return {
                'success': False,
                'error': f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–æ–¥–µ–∫—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.codes_dir}"
            }
        
        # –ò—â–µ–º PDF, TXT –∏ HTML —Ñ–∞–π–ª—ã
        pdf_files = list(self.codes_dir.glob("*.pdf"))
        txt_files = list(self.codes_dir.glob("*.txt"))
        html_files = list(self.codes_dir.glob("*.html"))
        all_files = pdf_files + txt_files + html_files
        
        if not all_files:
            return {
                'success': False,
                'error': "–§–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (PDF/TXT/HTML)"
            }
        
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(pdf_files)} PDF, {len(txt_files)} TXT, {len(html_files)} HTML (–≤—Å–µ–≥–æ: {len(all_files)})")
        
        results = []
        total_chunks = 0
        successful_files = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–∏—Å—Ç–µ–º—É)
        for file_path in all_files:
            result = await self.integrate_codex(file_path)
            results.append(result)
            
            if result['success']:
                successful_files += 1
                total_chunks += result.get('chunks_count', 0)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(all_files),
            'pdf_files': len(pdf_files),
            'txt_files': len(txt_files),
            'html_files': len(html_files),
            'successful_files': successful_files,
            'total_chunks': total_chunks,
            'results': results
        }
        
        report_file = self.metadata_dir / f"integration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üìä –û—Ç—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        success = successful_files > 0
        
        logger.info(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_files}/{len(all_files)} —Ñ–∞–π–ª–æ–≤, {total_chunks} —á–∞–Ω–∫–æ–≤")
        
        return {
            'success': success,
            'processed_files': successful_files,
            'total_files': len(all_files),
            'total_chunks': total_chunks,
            'report_file': str(report_file),
            'results': results
        }

    def get_integration_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if not self.codes_dir.exists():
            return {
                'available_files': 0,
                'integrated_files': 0,
                'total_chunks': 0
            }
        
        # –ò—â–µ–º PDF, TXT –∏ HTML —Ñ–∞–π–ª—ã
        pdf_files = list(self.codes_dir.glob("*.pdf"))
        txt_files = list(self.codes_dir.glob("*.txt"))
        html_files = list(self.codes_dir.glob("*.html"))
        all_files = pdf_files + txt_files + html_files
        
        # –ü–æ–¥—Å—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        integrated_count = 0
        total_chunks = 0
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            metadata_dir = Path("rag_integration/metadata")
            if metadata_dir.exists():
                # –°—á–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
                metadata_files = list(metadata_dir.glob("*.json"))
                integrated_count = len(metadata_files)
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                for metadata_file in metadata_files:
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                            total_chunks += metadata.get('total_chunks', 0)
                    except Exception:
                        pass
        
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
        
        return {
            'available_files': len(all_files),
            'pdf_files': len(pdf_files),
            'txt_files': len(txt_files),
            'html_files': len(html_files),
            'integrated_files': integrated_count,
            'total_chunks': total_chunks,
            'codes_dir': str(self.codes_dir)
        }



