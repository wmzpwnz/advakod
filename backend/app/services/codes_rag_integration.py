"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import json

from .document_service import DocumentService
from .vector_store_service import VectorStoreService

logger = logging.getLogger(__name__)

class CodesRAGIntegration:
    def __init__(self, codes_dir: str = "downloaded_codexes"):
        self.codes_dir = Path(codes_dir)
        self.document_service = DocumentService()
        self.vector_store = VectorStoreService()
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        self.metadata_dir = Path("rag_integration/metadata")
        self.metadata_dir.mkdir(parents=True, exist_ok=True)

    def integrate_codex(self, file_path: Path) -> Dict:
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –∫–æ–¥–µ–∫—Å –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            logger.info(f"üìÑ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–¥–µ–∫—Å–∞: {file_path.name}")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            result = self.document_service.process_document(file_path)
            
            if not result['success']:
                return {
                    'success': False,
                    'error': result['error'],
                    'file': str(file_path)
                }
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            chunks = result['chunks']
            embeddings = result['embeddings']
            
            vector_result = self.vector_store.add_documents(
                documents=chunks,
                embeddings=embeddings,
                metadata={
                    'source': str(file_path),
                    'type': 'codex',
                    'processed_at': datetime.now().isoformat()
                }
            )
            
            if not vector_result['success']:
                return {
                    'success': False,
                    'error': f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {vector_result['error']}",
                    'file': str(file_path)
                }
            
            logger.info(f"‚úÖ –ö–æ–¥–µ–∫—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω: {file_path.name} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
            
            return {
                'success': True,
                'file': str(file_path),
                'chunks_count': len(chunks),
                'vector_ids': vector_result['ids']
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ {file_path.name}: {e}")
            return {
                'success': False,
                'error': str(e),
                'file': str(file_path)
            }

    def integrate_all_codexes(self) -> Dict:
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–¥–µ–∫—Å—ã –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        logger.info("üîó –ù–∞—á–∞–ª–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–µ–∫—Å–æ–≤ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π")
        
        if not self.codes_dir.exists():
            return {
                'success': False,
                'error': f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–æ–¥–µ–∫—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.codes_dir}"
            }
        
        pdf_files = list(self.codes_dir.glob("*.pdf"))
        
        if not pdf_files:
            return {
                'success': False,
                'error': "PDF —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            }
        
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤")
        
        results = []
        total_chunks = 0
        successful_files = 0
        
        for file_path in pdf_files:
            result = self.integrate_codex(file_path)
            results.append(result)
            
            if result['success']:
                successful_files += 1
                total_chunks += result['chunks_count']
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(pdf_files),
            'successful_files': successful_files,
            'total_chunks': total_chunks,
            'results': results
        }
        
        report_file = self.metadata_dir / f"integration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üìä –û—Ç—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        success = successful_files > 0
        
        logger.info(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_files}/{len(pdf_files)} —Ñ–∞–π–ª–æ–≤, {total_chunks} —á–∞–Ω–∫–æ–≤")
        
        return {
            'success': success,
            'processed_files': successful_files,
            'total_files': len(pdf_files),
            'total_chunks': total_chunks,
            'report_file': str(report_file),
            'results': results
        }

    def get_integration_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if not self.codes_dir.exists():
            return {
                'available_files': 0,
                'integrated_files': 0,
                'total_chunks': 0
            }
        
        pdf_files = list(self.codes_dir.glob("*.pdf"))
        
        # –ü–æ–¥—Å—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        integrated_count = 0
        total_chunks = 0
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            all_docs = self.vector_store.get_all_documents()
            
            for doc in all_docs:
                if doc.get('metadata', {}).get('type') == 'codex':
                    integrated_count += 1
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    chunks = self.vector_store.search_similar(
                        query=doc.get('content', ''),
                        top_k=1000,
                        filter_metadata={'source': doc.get('metadata', {}).get('source')}
                    )
                    total_chunks += len(chunks.get('documents', []))
        
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
        
        return {
            'available_files': len(pdf_files),
            'integrated_files': integrated_count,
            'total_chunks': total_chunks,
            'codes_dir': str(self.codes_dir)
        }


