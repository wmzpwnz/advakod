"""
API –¥–ª—è –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª–∏
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏, –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π –∏ —Å–∏—Å—Ç–µ–º–æ–π
"""

import logging
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form, Query, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import func
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import json
import io
import os
import tempfile

from ..core.database import get_db
from ..models.user import User
from ..models.chat import ChatSession, ChatMessage
from ..models.audit_log import AuditLog, ActionType, SeverityLevel
from ..schemas.user import User as UserSchema, UserCreate
from ..schemas.admin import (
    UserUpdate, AdminDashboard, DocumentList, DocumentSearchResponse,
    AuditLogsResponse, UserAnalytics, FileUploadResponse
)
from ..services.auth_service import auth_service
from ..services.document_service import document_service
from ..services.vector_store_service import vector_store_service
from ..services.embeddings_service import embeddings_service
from ..services.rag_service import rag_service
from ..services.smart_document_processor import smart_document_processor
from ..services.audit_service import get_audit_service
from ..services.ai_document_validator import ai_document_validator
from ..services.document_versioning import document_versioning_service
from ..core.admin_security import get_secure_admin, require_admin_action_validation
from ..core.cache import cache_service

logger = logging.getLogger(__name__)

router = APIRouter()

def get_current_admin(current_user: User = Depends(auth_service.get_current_active_user)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")
    return current_user

# ==================== –£–ü–†–ê–í–õ–ï–ù–ò–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø–ú–ò ====================

@router.get("/users", response_model=List[UserSchema])
async def get_users(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    search: Optional[str] = None,
    is_active: Optional[bool] = None,
    is_premium: Optional[bool] = None,
    is_admin: Optional[bool] = None,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    try:
        query = db.query(User)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if search:
            query = query.filter(
                (User.username.contains(search)) |
                (User.email.contains(search)) |
                (User.full_name.contains(search))
            )
        
        if is_active is not None:
            query = query.filter(User.is_active == is_active)
        
        if is_premium is not None:
            query = query.filter(User.is_premium == is_premium)
            
        if is_admin is not None:
            query = query.filter(User.is_admin == is_admin)
        
        users = query.offset(skip).limit(limit).all()
        return users
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/users/{user_id}", response_model=UserSchema)
async def get_user(
    user_id: int,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ ID"""
    try:
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        return user
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/users/{user_id}", response_model=UserSchema)
async def update_user(
    user_id: int,
    user_update: UserUpdate,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–û–±–Ω–æ–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—è
        update_data = user_update.dict(exclude_unset=True)
        for field, value in update_data.items():
            setattr(user, field, value)
        
        user.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(user)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="user",
            resource_id=str(user_id),
            description=f"Updated user {user.username}",
            details={"updated_fields": list(update_data.keys())}
        )
        
        return user
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/users/{user_id}")
@require_admin_action_validation("delete_user", "user")
async def delete_user(
    user_id: int,
    request: Request,
    current_admin: User = Depends(get_secure_admin),
    db: Session = Depends(get_db)
):
    """–£–¥–∞–ª–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–º—è–≥–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ)"""
    try:
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # –ú—è–≥–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ - –¥–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user.is_active = False
        user.updated_at = datetime.utcnow()
        db.commit()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="user",
            resource_id=str(user_id),
            description=f"Deactivated user {user.username}",
            severity=SeverityLevel.HIGH
        )
        
        return {"message": "User deactivated successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/users/{user_id}/toggle-admin")
async def toggle_admin_status(
    user_id: int,
    request: Request,
    current_admin: User = Depends(get_secure_admin),
    db: Session = Depends(get_db)
):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
    try:
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # –ù–µ–ª—å–∑—è —É–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∞ –∞–¥–º–∏–Ω–∞ —É —Å–∞–º–æ–≥–æ —Å–µ–±—è
        if user_id == current_admin.id:
            raise HTTPException(status_code=400, detail="Cannot change your own admin status")
        
        user.is_admin = not user.is_admin
        user.updated_at = datetime.utcnow()
        db.commit()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="user",
            resource_id=str(user_id),
            description=f"Toggled admin status for {user.username} to {user.is_admin}",
            severity=SeverityLevel.HIGH
        )
        
        return {
            "message": f"Admin status {'granted' if user.is_admin else 'revoked'}",
            "is_admin": user.is_admin
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∞–¥–º–∏–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== –ê–ù–ê–õ–ò–¢–ò–ö–ê –ò –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================

@router.get("/dashboard", response_model=AdminDashboard)
async def get_admin_dashboard(
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–∞—à–±–æ—Ä–¥ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
    try:
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        total_users = db.query(User).count()
        active_users = db.query(User).filter(User.is_active == True).count()
        premium_users = db.query(User).filter(User.is_premium == True).count()
        admin_users = db.query(User).filter(User.is_admin == True).count()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        new_users_30d = db.query(User).filter(User.created_at >= thirty_days_ago).count()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Ç–æ–≤
        total_sessions = db.query(ChatSession).count()
        total_messages = db.query(ChatMessage).count()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã
        rag_status = rag_service.get_status()
        vector_store_status = vector_store_service.get_status()
        
        return {
            "users": {
                "total": total_users,
                "active": active_users,
                "premium": premium_users,
                "admins": admin_users,
                "new_last_30d": new_users_30d
            },
            "chats": {
                "total_sessions": total_sessions,
                "total_messages": total_messages
            },
            "system": {
                "rag_status": rag_status,
                "vector_store_status": vector_store_status,
                "timestamp": datetime.utcnow().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/analytics/users")
async def get_user_analytics(
    days: int = Query(30, ge=1, le=365),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    try:
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–Ω—è–º
        daily_registrations = db.query(
            func.date(User.created_at).label('date'),
            func.count(User.id).label('count')
        ).filter(
            User.created_at >= start_date
        ).group_by(
            func.date(User.created_at)
        ).all()
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –ø–æ–¥–ø–∏—Å–∫–∏
        subscription_stats = db.query(
            User.subscription_type,
            func.count(User.id).label('count')
        ).group_by(User.subscription_type).all()
        
        return {
            "period": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat(),
                "days": days
            },
            "daily_registrations": [
                {"date": str(reg.date), "count": reg.count}
                for reg in daily_registrations
            ],
            "subscription_distribution": [
                {"type": stat.subscription_type, "count": stat.count}
                for stat in subscription_stats
            ]
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== –£–ü–†–ê–í–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò ====================

@router.get("/documents", response_model=Dict)
async def get_documents(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º–µ"""
    from fastapi import Response
    try:
        if not vector_store_service.is_ready():
            return {"documents": [], "total": 0, "message": "Vector store not ready"}
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection = vector_store_service.collection
        total_docs = collection.count()
        
        if total_docs == 0:
            return {"documents": [], "total": 0, "skip": skip, "limit": limit}
        
        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑-–∑–∞ –±–∞–≥–∞ –≤ ChromaDB _decode_seq_id
        results = None
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π get
            results = collection.get(
                limit=total_docs,
                include=['metadatas', 'documents']
            )
        except (TypeError, ValueError) as e:
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å _decode_seq_id, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            logger.warning(f"–û—à–∏–±–∫–∞ get() –∏–∑ ChromaDB (–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã): {e}")
            try:
                # –ü—Ä–æ–±—É–µ–º peek (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
                try:
                    peek_results = collection.peek(limit=min(1000, total_docs))
                    if peek_results and 'ids' in peek_results:
                        results = peek_results
                        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(peek_results.get('ids', []))} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ peek()")
                except Exception as peek_err:
                    logger.warning(f"–û—à–∏–±–∫–∞ peek(): {peek_err}")
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ query —Å dummy –≤–µ–∫—Ç–æ—Ä–æ–º
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º embeddings_service –∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
                        # –°–æ–∑–¥–∞–µ–º dummy embedding
                        dummy_embedding = embeddings_service.get_embedding("test") if hasattr(embeddings_service, 'get_embedding') else [0.0] * 384
                        query_results = collection.query(
                            query_embeddings=[dummy_embedding],
                            n_results=min(1000, total_docs),
                            include=['metadatas', 'documents']
                        )
                        if query_results and query_results.get('ids'):
                            # Query –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
                            results = {
                                'ids': query_results.get('ids', [[]])[0] if query_results.get('ids') else [],
                                'documents': query_results.get('documents', [[]])[0] if query_results.get('documents') else [],
                                'metadatas': query_results.get('metadatas', [[]])[0] if query_results.get('metadatas') else []
                            }
                            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(results['ids'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ query()")
                    except Exception as query_err:
                        logger.error(f"–û—à–∏–±–∫–∞ query(): {query_err}")
                
                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
                if not results:
                    logger.warning("–ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã")
                    try:
                        from pathlib import Path
                        from datetime import datetime
                        import uuid as uuid_lib
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π
                        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: codes-system –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å downloaded_codexes
                        # (–∫–æ–¥ –∫–æ–ø–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –æ–±—Ä–∞–∑–∞, –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ bind mount)
                        possible_paths = [
                            "/app/data/codes_downloads",  # –ù–æ–≤—ã–π –ø—É—Ç—å (–æ–±—â–∏–π volume)
                            "/app/downloaded_codexes",  # –°—Ç–∞—Ä—ã–π –ø—É—Ç—å (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è codes-system)
                            "/app/data/downloaded_codexes"  # –ï—â–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç
                        ]
                        codes_dir = None
                        for path_str in possible_paths:
                            path = Path(path_str)
                            if path.exists() and path.is_dir():
                                codes_dir = path
                                break
                        
                        file_docs = []
                        if codes_dir and codes_dir.exists():
                            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                            try:
                                from ..services.deleted_documents_cache import deleted_documents_cache
                            except ImportError:
                                deleted_documents_cache = None
                            
                            # –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ä—ã—Ö –∑–∞–≥–ª—É—à–µ–∫ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                            old_stub_ids = [
                                "0001201412140001", "0001201412140002", "0001201412140003",
                                "0001201412140004", "0001201412140005", "0001201412140006",
                                "0001201410140002", "0001201905010039", "0001202203030006",
                                "198", "289-11"
                            ]
                            
                            for file_path in codes_dir.rglob("*.txt"):
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                                if deleted_documents_cache:
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                                    if deleted_documents_cache.is_deleted(filename=file_path.name):
                                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —É–¥–∞–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ –∏–º–µ–Ω–∏): {file_path.name}")
                                        continue
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ ID –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                                    try:
                                        json_path = file_path.with_suffix('.json')
                                        if json_path.exists():
                                            with open(json_path, 'r', encoding='utf-8') as f:
                                                meta = json.load(f)
                                                doc_id = meta.get('document_id') or meta.get('id') or file_path.stem
                                                if deleted_documents_cache.is_deleted(document_id=doc_id):
                                                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —É–¥–∞–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ ID): {file_path.name} (ID: {doc_id})")
                                                    continue
                                    except Exception:
                                        pass
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (stem)
                                    if deleted_documents_cache.is_deleted(document_id=file_path.stem):
                                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —É–¥–∞–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ stem): {file_path.name}")
                                        continue
                                try:
                                    stat = file_path.stat()
                                    
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã (–∑–∞–≥–ª—É—à–∫–∏ <= 2000 –±–∞–π—Ç - –∑–∞–≥–ª—É—à–∫–∏ –æ–±—ã—á–Ω–æ 500-1500 –±–∞–π—Ç)
                                    if stat.st_size <= 2000:
                                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª: {file_path.name} ({stat.st_size} –±–∞–π—Ç)")
                                        continue
                                    
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å ID —Å—Ç–∞—Ä—ã—Ö –∑–∞–≥–ª—É—à–µ–∫
                                    file_stem = file_path.stem
                                    if any(old_id in file_stem for old_id in old_stub_ids):
                                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª –∑–∞–≥–ª—É—à–∫–∏: {file_path.name} (ID: {file_stem})")
                                        continue
                                    
                                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/–∑–∞–≥–ª—É—à–∫—É
                                    try:
                                        content_check = file_path.read_text(encoding='utf-8', errors='ignore')[:500].lower()
                                        if len(content_check) < 100 or any(keyword in content_check for keyword in ['–Ω–∞–≤–∏–≥–∞—Ü–∏—è', 'navigation', '–∑–∞–≥–ª—É—à–∫–∞', 'stub', 'placeholder']):
                                            logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª-–∑–∞–≥–ª—É—à–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É: {file_path.name}")
                                            continue
                                    except:
                                        pass  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ñ–∞–π–ª
                                    
                                    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞
                                    metadata_file = file_path.with_suffix('.json')
                                    metadata_dict = {}
                                    if metadata_file.exists():
                                        try:
                                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                                api_metadata = json.load(f)
                                                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ API
                                                metadata_dict = {
                                                    "filename": api_metadata.get('file_name', file_path.name),
                                                    "codex_name": api_metadata.get('codex_name'),
                                                    "document_name": api_metadata.get('name'),
                                                    "document_title": api_metadata.get('title'),
                                                    "document_number": api_metadata.get('number'),
                                                    "document_date": api_metadata.get('document_date'),
                                                    "publish_date": api_metadata.get('publish_date'),
                                                    "view_date": api_metadata.get('view_date'),
                                                    "document_type": api_metadata.get('document_type'),
                                                    "pages_count": api_metadata.get('pages_count'),
                                                    "signatory_authorities": api_metadata.get('signatory_authorities', []),
                                                    "eo_number": api_metadata.get('eo_number'),
                                                    "source_url": api_metadata.get('source_url'),
                                                    "source_type": "legal_document",
                                                    "source_path": str(file_path),
                                                    "added_at": api_metadata.get('publish_date') or datetime.fromtimestamp(stat.st_mtime).isoformat(),
                                                    "size": stat.st_size,
                                                    "api_metadata_retrieved_at": api_metadata.get('api_metadata_retrieved_at')
                                                }
                                        except Exception as meta_err:
                                            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è {file_path.name}: {meta_err}")
                                    
                                    # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ + –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–µ–∫—Å –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                                    if not metadata_dict:
                                        # –°–ø–∏—Å–æ–∫ –∫–æ–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ eo_number
                                        codex_names = {
                                            '0001201410140002': '–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001202203030006': '–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140001': '–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140002': '–°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140003': '–ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140004': '–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140005': '–ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                                            '0001201412140006': '–ö–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§'
                                        }
                                        
                                        # –ò–∑–≤–ª–µ–∫–∞–µ–º eo_number –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                                        eo_number = file_path.stem
                                        codex_name = codex_names.get(eo_number, None)
                                        
                                        metadata_dict = {
                                            "filename": file_path.name,
                                            "eo_number": eo_number,
                                            "codex_name": codex_name,
                                            "source_type": "legal_document",
                                            "source_path": str(file_path),
                                            "added_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                                            "size": stat.st_size,
                                            "document_type": "legal_document",
                                            "pages_count": None,
                                            "note": "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ API –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º —Ü–∏–∫–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"
                                        }
                                    
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∞–º–∏
                                    # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                                    content_preview = file_path.read_text(encoding='utf-8', errors='ignore')[:200]
                                    if len(content_preview) < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                                    
                                    # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
                                    content = file_path.read_text(encoding='utf-8', errors='ignore')[:500]
                                    
                                    # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    display_name = metadata_dict.get("codex_name") or metadata_dict.get("document_name") or metadata_dict.get("filename", file_path.name)
                                    
                                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    pdf_size = metadata_dict.get("pdf_file_length")
                                    pages_count = metadata_dict.get("pages_count")
                                    
                                    # –ï—Å–ª–∏ —Ñ–∞–π–ª –º–∞–ª–µ–Ω—å–∫–∏–π, –Ω–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª—å—à–∏–º - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
                                    if pdf_size:
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä PDF –∏–∑ API
                                        display_size = pdf_size
                                        size_warning = None
                                    elif pages_count and pages_count > 10 and stat.st_size < 5000:
                                        # –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –¥–ª—è —Ç–∞–∫–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
                                        display_size = stat.st_size  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                                        size_warning = f"‚ö†Ô∏è –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ HTML-–∑–∞–≥–ª—É—à–∫—É ({stat.st_size} –±–∞–π—Ç), –∞ –Ω–µ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ–¥–µ–∫—Å–∞. –û–∂–∏–¥–∞–µ—Ç—Å—è ~{pages_count*5}KB –¥–ª—è {pages_count} —Å—Ç—Ä–∞–Ω–∏—Ü"
                                    else:
                                        display_size = stat.st_size
                                        size_warning = None
                                    
                                    file_docs.append({
                                        "id": metadata_dict.get("eo_number", str(uuid_lib.uuid4())),
                                        "name": display_name,  # –ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                        "title": metadata_dict.get("document_title") or metadata_dict.get("document_name") or display_name,
                                        "content": content,
                                        "metadata": metadata_dict,
                                        "length": len(content),
                                        "chunks_count": 1,
                                        "total_length": stat.st_size,
                                        "filename": metadata_dict.get("filename", file_path.name),
                                        "codex_name": metadata_dict.get("codex_name"),  # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–¥–µ–∫—Å–∞
                                        "document_type": metadata_dict.get("document_type", "legal_document"),
                                        "publish_date": metadata_dict.get("publish_date"),
                                        "view_date": metadata_dict.get("view_date"),
                                        "pages_count": pages_count,
                                        "size": display_size,
                                        "actual_file_size": stat.st_size,  # –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                                        "expected_pdf_size": pdf_size,  # –û–∂–∏–¥–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä PDF –∏–∑ API
                                        "size_warning": size_warning,  # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ä–∞–∑–º–µ—Ä–µ
                                        "content_preview_length": len(content),  # –î–ª–∏–Ω–∞ preview –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                                        "note": metadata_dict.get("note")  # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                                    })
                                except Exception as file_err:
                                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {file_err}")
                            
                            if file_docs:
                                documents = file_docs[skip:skip + limit]
                                return {
                                    "documents": documents,
                                    "total": len(file_docs),
                                    "skip": skip,
                                    "limit": limit,
                                    "message": "Documents loaded from file system (ChromaDB unavailable)"
                                }
                    except Exception as fs_err:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã: {fs_err}")
                    
                    return {"documents": [], "total": 0, "message": "ChromaDB data corruption detected. Please re-index documents."}
            except Exception as fallback_err:
                logger.error(f"–û—à–∏–±–∫–∞ fallback –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {fallback_err}")
                return {"documents": [], "total": 0, "message": f"Error fetching from vector store: {str(fallback_err)}"}
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ ChromaDB: {e}")
            return {"documents": [], "total": 0, "message": f"Error fetching from vector store: {str(e)}"}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if not results or 'documents' not in results or 'metadatas' not in results:
            logger.warning("ChromaDB –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return {"documents": [], "total": 0, "skip": skip, "limit": limit}
        
        documents_list = results.get('documents', [])
        metadatas_list = results.get('metadatas', [])
        ids_list = results.get('ids', [])
        
        if not documents_list or not metadatas_list:
            return {"documents": [], "total": 0, "skip": skip, "limit": limit}
        
        # –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ä—ã—Ö –∑–∞–≥–ª—É—à–µ–∫ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        old_stub_ids = [
            "0001201412140001", "0001201412140002", "0001201412140003",
            "0001201412140004", "0001201412140005", "0001201412140006",
            "0001201410140002", "0001201905010039", "0001202203030006",
            "198", "289-11"
        ]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ document_id –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents_by_id = {}
        for i in range(min(len(documents_list), len(metadatas_list))):
            try:
                doc = documents_list[i] if isinstance(documents_list[i], str) else str(documents_list[i])
                meta = metadatas_list[i] if isinstance(metadatas_list[i], dict) else {}
                doc_id = str(meta.get('document_id', ids_list[i] if i < len(ids_list) else f"doc_{i}"))
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –ø–æ ID
                if any(old_id in str(doc_id) or old_id in meta.get('filename', '') or old_id in meta.get('source_path', '') for old_id in old_stub_ids):
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã (<= 2000 –±–∞–π—Ç - –∑–∞–≥–ª—É—à–∫–∏ –æ–±—ã—á–Ω–æ 500-1500 –±–∞–π—Ç)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º size –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ–±—É–µ–º file_size, –µ—Å–ª–∏ –Ω–µ—Ç - –¥–ª–∏–Ω—É —á–∞–Ω–∫–∞
                # –ù–û: –µ—Å–ª–∏ —ç—Ç–æ —á–∞–Ω–∫ –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, len(doc) –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–º
                # –ü–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ size/file_size –µ—Å—Ç—å –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                file_size = meta.get('size') or meta.get('file_size')
                if file_size is None:
                    # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–∞ –Ω–µ—Ç –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ —Ä–∞–∑–º–µ—Ä—É
                    # (–¥–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞–Ω–∫–∏)
                    file_size = len(doc) if isinstance(doc, str) else 0
                    # –ù–æ –µ—Å–ª–∏ —á–∞–Ω–∫ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π (< 1000), —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–ª—É—à–∫–∞
                    if file_size < 1000:
                        continue
                elif file_size <= 2000:
                    continue
                
                if doc_id not in documents_by_id:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    display_name = meta.get("codex_name") or meta.get("document_name") or meta.get("filename", f"doc_{doc_id}")
                    
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    documents_by_id[doc_id] = {
                        "id": doc_id,
                        "name": display_name,
                        "title": meta.get("document_title") or meta.get("document_name") or display_name,
                        "content": doc,
                        "metadata": meta,
                        "length": len(doc) if isinstance(doc, str) else 0,
                        "chunks_count": 1,
                        "total_length": len(doc) if isinstance(doc, str) else 0,
                        "filename": meta.get("filename", f"doc_{doc_id}"),
                        "codex_name": meta.get("codex_name"),
                        "document_type": meta.get("document_type", "legal_document"),
                        "publish_date": meta.get("publish_date"),
                        "view_date": meta.get("view_date"),
                        "pages_count": meta.get("pages_count"),
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä PDF –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö API (–µ—Å–ª–∏ –µ—Å—Ç—å), –∏–Ω–∞—á–µ —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                        "size": meta.get("pdf_file_length") or meta.get("size") or (len(doc) if isinstance(doc, str) else 0),
                        "actual_file_size": meta.get("size"),  # –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                        "note": meta.get("note")  # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    }
                else:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                    existing = documents_by_id[doc_id]
                    existing["chunks_count"] += 1
                    doc_len = len(doc) if isinstance(doc, str) else 0
                    existing["total_length"] += doc_len
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —á–∞–Ω–∫
                    if doc_len > existing["length"]:
                        existing["content"] = doc
                        existing["length"] = doc_len
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–µ
                    if meta.get("codex_name") and not existing.get("codex_name"):
                        existing["codex_name"] = meta.get("codex_name")
                        existing["name"] = meta.get("codex_name") or existing.get("name")
                    if meta.get("document_name") and not existing.get("title"):
                        existing["title"] = meta.get("document_name")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {i}: {e}")
                continue
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
        documents = list(documents_by_id.values())
        
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≥–ª—É—à–µ–∫ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        old_stub_ids_final = [
            "0001201412140001", "0001201412140002", "0001201412140003",
            "0001201412140004", "0001201412140005", "0001201412140006",
            "0001201410140002", "0001201905010039", "0001202203030006",
            "198", "289-11"
        ]
        
        documents = [
            doc for doc in documents
            if not any(
                old_id in str(doc.get('id', '')) or 
                old_id in str(doc.get('filename', '')) or
                str(doc.get('size', 0)) == '500' or
                (isinstance(doc.get('size'), int) and doc.get('size') <= 2000) or
                (isinstance(doc.get('actual_file_size'), int) and doc.get('actual_file_size') <= 2000)
                for old_id in old_stub_ids_final
            )
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        documents.sort(key=lambda x: x.get('metadata', {}).get('added_at', ''), reverse=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ source_url –¥–ª—è URL –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ + —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        seen_urls = set()
        unique_documents = []
        for doc in documents:
            # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–ª—É—à–µ–∫
            doc_id = str(doc.get('id', ''))
            filename = str(doc.get('filename', ''))
            size = doc.get('size', 0) or doc.get('actual_file_size', 0)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –ø–æ ID
            if any(old_id in doc_id or old_id in filename for old_id in old_stub_ids_final):
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –∑–∞–≥–ª—É—à–∫–∞ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–ø–∏—Å–∫–µ: {doc_id} ({filename})")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã (<= 2000 –±–∞–π—Ç)
            if size <= 2000:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–ø–∏—Å–∫–µ: {doc_id} ({size} –±–∞–π—Ç)")
                continue
            
            if doc.get('metadata', {}).get('source_type') == 'url':
                source_url = doc.get('metadata', {}).get('source_url', '')
                if source_url not in seen_urls:
                    seen_urls.add(source_url)
                    unique_documents.append(doc)
            else:
                unique_documents.append(doc)
        
        documents = unique_documents
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        total_unique_docs = len(documents)
        documents = documents[skip:skip + limit]
        
        from fastapi.responses import JSONResponse
        response = JSONResponse({
            "documents": documents,
            "total": total_unique_docs,
            "skip": skip,
            "limit": limit
        })
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        return response
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/documents/{document_id}/chunks")
async def get_document_chunks(
    document_id: str,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        if not vector_store_service.is_ready():
            return {"chunks": [], "message": "Vector store not ready"}
        
        collection = vector_store_service.collection
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —Å document_id
        try:
            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –∑–∞–ø–∏—Å–∏ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
            # ChromaDB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –≤ get(), –ø–æ—ç—Ç–æ–º—É –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            all_results = collection.get(
                include=['metadatas', 'documents'],
                limit=100000  # –ë–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏
            )
            
            chunks = []
            if all_results.get('metadatas') and all_results.get('documents'):
                metadatas_list = all_results['metadatas']
                documents_list = all_results['documents']
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ ID —á–µ—Ä–µ–∑ peek –∏–ª–∏ count + get
                try:
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å ID —á–µ—Ä–µ–∑ peek
                    peek_results = collection.peek(limit=100000)
                    ids_list = peek_results.get('ids', [])
                except:
                    # –ï—Å–ª–∏ peek –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–æ–ª—É—á–∞–µ–º —á–µ—Ä–µ–∑ get —Å where (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
                    # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
                    ids_list = []
                    try:
                        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ count –∏ –∑–∞—Ç–µ–º get
                        total_count = collection.count()
                        if total_count > 0:
                            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ ID —á–µ—Ä–µ–∑ get —Å –ø—É—Å—Ç—ã–º where
                            ids_results = collection.get(limit=100000)
                            ids_list = ids_results.get('ids', [])
                    except:
                        pass
                
                for i, meta in enumerate(metadatas_list):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —á–∞–Ω–∫ –Ω—É–∂–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    chunk_doc_id = meta.get('document_id')
                    
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º document_id (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —á–∏—Å–ª–æ–º)
                    if str(chunk_doc_id) == str(document_id):
                        # –ü–æ–ª—É—á–∞–µ–º ID —á–∞–Ω–∫–∞
                        chunk_id = ids_list[i] if i < len(ids_list) else f"chunk_{i}_{document_id}"
                        chunk_content = documents_list[i] if i < len(documents_list) else ""
                        
                        chunks.append({
                            "id": chunk_id,
                            "content": chunk_content,
                            "chunk_index": meta.get('chunk_index', i),
                            "chunk_length": len(chunk_content) if isinstance(chunk_content, str) else 0,
                            "metadata": meta
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_index
            chunks.sort(key=lambda x: x.get('chunk_index', 0))
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è document_id={document_id}: {len(chunks)}")
            
            return {
                "document_id": document_id,
                "chunks": chunks,
                "total": len(chunks)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–ª—è document_id={document_id}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"chunks": [], "message": f"Error fetching chunks: {str(e)}"}
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    description: Optional[str] = Form(None),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤—Ä–µ–º–µ–Ω–Ω–æ
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        safe_filename = os.path.basename(file.filename) if file.filename else "unknown"
        safe_filename = "".join(c for c in safe_filename if c.isalnum() or c in ".-_")[:50]
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{safe_filename}") as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ —á–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            file_extension = os.path.splitext(file.filename)[1].lower() if file.filename else ''
            
            if file_extension == '.docx':
                # –î–ª—è DOCX —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                import docx
                try:
                    doc = docx.Document(tmp_file_path)
                    content = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX —Ñ–∞–π–ª–∞: {e}")
                    # Fallback: —á–∏—Ç–∞–µ–º –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª
                    with open(tmp_file_path, 'rb') as f:
                        content = f.read().decode('utf-8', errors='ignore')
            elif file_extension == '.pdf':
                # –î–ª—è PDF —Ñ–∞–π–ª–æ–≤
                try:
                    import PyPDF2
                    with open(tmp_file_path, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        content = '\n'.join([page.extract_text() for page in reader.pages])
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF —Ñ–∞–π–ª–∞: {e}")
                    content = "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"
            else:
                # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
                with open(tmp_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
            result = await smart_document_processor.process_document(
                file_path=file.filename,
                content=content
            )
            
            if result.get('success'):
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                audit_service = get_audit_service(db)
                audit_service.log_action(
                    user_id=current_admin.id,
                    action=ActionType.ADMIN_ACTION,
                    resource="document",
                    resource_id=file.filename,
                    description=f"Uploaded document with smart processing: {file.filename}",
                    details={
                        "chunks_created": result.get('chunks_count', 0),
                        "articles_found": result.get('articles_count', 0),
                        "document_type": result.get('metadata', {}).get('document_type', 'unknown'),
                        "structure_score": result.get('metadata', {}).get('structure_score', 0)
                    }
                )
                
                return {
                    "success": True,
                    "message": "Document uploaded and analyzed successfully",
                    "details": result,
                    "chunks_created": result.get('chunks_count', 0),
                    "articles_found": result.get('articles_count', 0),
                    "sections_found": result.get('sections_count', 0),
                    "document_type": result.get('metadata', {}).get('document_type', 'unknown'),
                    "structure_score": result.get('metadata', {}).get('structure_score', 0),
                    "processing_time": result.get('processing_time', 0),
                    "filename": file.filename
                }
            else:
                return {
                    "success": False,
                    "message": "Failed to process document with smart analysis",
                    "error": result.get('error', 'Unknown error')
                }
                
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/documents/upload-url")
async def upload_document_from_url(
    request: dict,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ URL"""
    try:
        url = request.get('url')
        if not url:
            raise HTTPException(status_code=400, detail="URL –Ω–µ —É–∫–∞–∑–∞–Ω")
            
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ URL: {url}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
        if not url.startswith(('http://', 'https://')):
            raise HTTPException(status_code=400, detail="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ URL
        result = await document_service.process_url(url)
        
        if result.get('success'):
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞
            audit_service = get_audit_service(db)
            audit_service.log_action(
                user_id=current_admin.id,
                action=ActionType.ADMIN_ACTION,
                resource="document",
                resource_id=result.get('document_id', 'unknown'),
                description=f"Uploaded document from URL: {url}",
                details={"chunks_added": result.get('chunks_added', 0)}
            )
            
            return {
                "success": True,
                "message": "Document uploaded successfully from URL",
                "details": result,
                "chunks_added": result.get('chunks_added', 0),
                "document_type": result.get('document_type', 'unknown'),
                "validation_confidence": result.get('validation_confidence', 0),
                "legal_score": result.get('legal_score', 0),
                "text_length": result.get('text_length', 0),
                "file_hash": result.get('file_hash', ''),
                "document_id": result.get('document_id', ''),
                "source_url": url
            }
        else:
            return {
                "success": False,
                "message": "Failed to upload document from URL",
                "error": result.get('error')
            }
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ URL: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/validation/status")
async def get_validation_status(
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        status = document_service.get_status()
        return {
            "success": True,
            "validation_method": status.get("validation_method", "unknown"),
            "available_methods": ["hybrid", "ai", "rules"],
            "hybrid_stats": status.get("hybrid_stats", {}),
            "ai_validator_stats": ai_document_validator.get_validation_stats()
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/validation/method")
async def set_validation_method(
    method: str = Query(..., description="–ú–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: hybrid, ai, rules, none"),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        document_service.set_validation_method(method)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="validation_method",
            resource_id="system",
            description=f"Changed validation method to: {method}",
            details={"validation_method": method}
        )
        
        method_names = {
            "hybrid": "–ì–∏–±—Ä–∏–¥–Ω—ã–π (–ø—Ä–∞–≤–∏–ª–∞ + AI)",
            "ai": "AI (Vistral-24B)",
            "rules": "–ü—Ä–∞–≤–∏–ª–∞ (–±—ã—Å—Ç—Ä—ã–π)"
        }
        
        return {
            "success": True,
            "message": f"–ú–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {method_names.get(method, method)}",
            "validation_method": method
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/validation/clear-cache")
async def clear_validation_cache(
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    try:
        from ..services.hybrid_document_validator import hybrid_document_validator
        hybrid_document_validator.clear_cache()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="validation_cache",
            resource_id="system",
            description="Cleared validation cache",
            details={}
        )
        
        return {
            "success": True,
            "message": "–ö—ç—à –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—á–∏—â–µ–Ω"
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/documents/versions")
async def get_document_versions(
    document_id: Optional[str] = Query(None, description="ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä—Å–∏–π"),
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        if document_id:
            # –ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä—Å–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            versions = document_versioning_service.get_all_versions(document_id)
            current_version = document_versioning_service.get_current_version(document_id)
            
            return {
                "success": True,
                "document_id": document_id,
                "current_version": current_version.__dict__ if current_version else None,
                "all_versions": [v.__dict__ for v in versions]
            }
        else:
            # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            stats = document_versioning_service.get_version_statistics()
            return {
                "success": True,
                "statistics": stats
            }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä—Å–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/documents/current-versions")
async def get_current_versions(
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–µ –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏—Ö —Ç–µ–∫—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏
        all_documents = {}
        for document_id in document_versioning_service.versions.keys():
            current_version = document_versioning_service.get_current_version(document_id)
            if current_version:
                all_documents[document_id] = current_version.__dict__
        
        return {
            "success": True,
            "current_versions": all_documents
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –≤–µ—Ä—Å–∏–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/documents/{document_id}/archive")
async def archive_document(
    document_id: str,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç (–ø–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π)"""
    try:
        # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        versions = document_versioning_service.get_all_versions(document_id)
        if not versions:
            raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–º–µ—Ç–∏—Ç—å –≤—Å–µ –≤–µ—Ä—Å–∏–∏ –∫–∞–∫ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        for version in versions:
            version.status = "archived"
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="document",
            resource_id=document_id,
            description=f"Archived document {document_id}",
            details={"versions_archived": len(versions)}
        )
        
        return {
            "success": True,
            "message": f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω",
            "archived_versions": len(versions)
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å simple_expert_rag) –∏ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    try:
        if not vector_store_service.is_ready():
            raise HTTPException(status_code=503, detail="Vector store not ready")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º simple_expert_rag
        from ..services.simple_expert_rag import simple_expert_rag
        from pathlib import Path
        import os
        
        # 1. –£–¥–∞–ª—è–µ–º –∏–∑ ChromaDB (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑-–∑–∞ –±–∞–≥–∞ ChromaDB)
        collection = vector_store_service.collection
        matching_ids = []
        filename_to_delete = None
        source_path_to_delete = None
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é –ø–æ ID (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –º–µ—Ç–æ–¥)
        try:
            collection.delete(ids=[document_id])
            matching_ids = [document_id]
            chromadb_chunks_deleted = 1
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∏–∑ ChromaDB –Ω–∞–ø—Ä—è–º—É—é –ø–æ ID: {document_id}")
        except Exception as direct_err:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ ID: {direct_err}")
            chromadb_chunks_deleted = 0
            
            # –ï—Å–ª–∏ –ø—Ä—è–º–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            try:
                # –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ query —Å dummy embedding
                try:
                    from ..services.embeddings_service import embeddings_service
                    dummy_embedding = embeddings_service.get_embedding("test") if hasattr(embeddings_service, 'get_embedding') else [0.0] * 384
                    query_results = collection.query(
                        query_embeddings=[dummy_embedding],
                        n_results=1000,
                        include=['metadatas']
                    )
                    if query_results and query_results.get('ids'):
                        all_ids = query_results.get('ids', [[]])[0] if query_results.get('ids') else []
                        all_metadatas = query_results.get('metadatas', [[]])[0] if query_results.get('metadatas') else []
                        
                        for i, (chunk_id, metadata) in enumerate(zip(all_ids, all_metadatas)):
                            if metadata and (metadata.get('document_id') == document_id or chunk_id == document_id or 
                                           document_id in str(chunk_id) or document_id in str(metadata.get('filename', ''))):
                                matching_ids.append(chunk_id)
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
                                if not filename_to_delete and metadata.get('filename'):
                                    filename_to_delete = metadata.get('filename')
                                if not source_path_to_delete and metadata.get('source_path'):
                                    source_path_to_delete = metadata.get('source_path')
                except Exception as query_err:
                    logger.warning(f"–û—à–∏–±–∫–∞ query() –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {query_err}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ ChromaDB: {e}")
            
            # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            if matching_ids:
                try:
                    collection.delete(ids=matching_ids)
                    chromadb_chunks_deleted = len(matching_ids)
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∏–∑ ChromaDB: {chromadb_chunks_deleted} —á–∞–Ω–∫–æ–≤")
                except Exception as delete_err:
                    logger.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ ChromaDB: {delete_err}")
                    chromadb_chunks_deleted = 0
        
        # 2. –£–¥–∞–ª—è–µ–º –∏–∑ simple_expert_rag
        simple_rag_result = await simple_expert_rag.delete_document(document_id)
        simple_rag_chunks_deleted = simple_rag_result.get('chunks_deleted', 0)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ document_id, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∏–º–µ–Ω–∞–º —Ñ–∞–π–ª–æ–≤
        if simple_rag_chunks_deleted == 0 and filename_to_delete:
            try:
                filename_result = await simple_expert_rag.delete_document(filename_to_delete)
                if filename_result.get('success', False):
                    simple_rag_chunks_deleted += filename_result.get('chunks_deleted', 0)
                    simple_rag_result = filename_result
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ simple_expert_rag –ø–æ filename: {e}")
        
        # 3. –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        files_deleted = []
        possible_paths = [
            "/app/data/codes_downloads",
            "/app/downloaded_codexes",
            "/app/data/downloaded_codexes"
        ]
        
        # –£–¥–∞–ª—è–µ–º –ø–æ source_path –µ—Å–ª–∏ –µ—Å—Ç—å
        if source_path_to_delete:
            try:
                file_path = Path(source_path_to_delete)
                if file_path.exists() and file_path.is_file():
                    file_path.unlink()
                    files_deleted.append(str(file_path))
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª: {file_path}")
                    # –¢–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º JSON –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                    json_path = file_path.with_suffix('.json')
                    if json_path.exists():
                        json_path.unlink()
                        files_deleted.append(str(json_path))
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –ø–æ source_path: {e}")
        
        # –£–¥–∞–ª—è–µ–º –ø–æ filename –µ—Å–ª–∏ –µ—Å—Ç—å
        if filename_to_delete:
            for base_path in possible_paths:
                try:
                    base_dir = Path(base_path)
                    if base_dir.exists():
                        # –ò—â–µ–º —Ñ–∞–π–ª –ø–æ –∏–º–µ–Ω–∏
                        for file_path in base_dir.rglob(filename_to_delete):
                            if file_path.exists() and file_path.is_file():
                                file_path.unlink()
                                files_deleted.append(str(file_path))
                                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª: {file_path}")
                                # –¢–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º JSON –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                                json_path = file_path.with_suffix('.json')
                                if json_path.exists():
                                    json_path.unlink()
                                    files_deleted.append(str(json_path))
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–∞ –≤ {base_path}: {e}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –µ—Å–ª–∏ document_id –ø–æ—Ö–æ–∂ –Ω–∞ –∏–º—è —Ñ–∞–π–ª–∞, –∏—â–µ–º –ø–æ –Ω–µ–º—É
        if not files_deleted and document_id:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç –ª–∏ document_id –±—ã—Ç—å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
            possible_filenames = [
                document_id,
                f"{document_id}.txt",
                f"{document_id}.pdf"
            ]
            for base_path in possible_paths:
                try:
                    base_dir = Path(base_path)
                    if base_dir.exists():
                        for possible_filename in possible_filenames:
                            for file_path in base_dir.rglob(possible_filename):
                                if file_path.exists() and file_path.is_file():
                                    file_path.unlink()
                                    files_deleted.append(str(file_path))
                                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –ø–æ document_id (–∫–∞–∫ –∏–º—è —Ñ–∞–π–ª–∞): {file_path}")
                                    if not filename_to_delete:
                                        filename_to_delete = file_path.name
                                    json_path = file_path.with_suffix('.json')
                                    if json_path.exists():
                                        json_path.unlink()
                                        files_deleted.append(str(json_path))
                                    break
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ document_id –≤ {base_path}: {e}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –∏—â–µ–º —Ñ–∞–π–ª—ã –ø–æ document_id –≤ –∏–º–µ–Ω–∏ (–¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ document_id —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞)
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ document_id = "tk_codex_20251106_103457" –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞
        if not files_deleted and document_id:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω—ã —Å —ç—Ç–∏–º document_id
            for base_path in possible_paths:
                try:
                    base_dir = Path(base_path)
                    if base_dir.exists():
                        # –ò—â–µ–º —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ document_id –≤ –∏–º–µ–Ω–∏ –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                        for file_path in base_dir.rglob("*.txt"):
                            try:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º JSON –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                                json_path = file_path.with_suffix('.json')
                                if json_path.exists():
                                    with open(json_path, 'r', encoding='utf-8') as f:
                                        meta = json.load(f)
                                        if meta.get('document_id') == document_id or meta.get('id') == document_id:
                                            file_path.unlink()
                                            json_path.unlink()
                                            files_deleted.append(str(file_path))
                                            files_deleted.append(str(json_path))
                                            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –ø–æ document_id: {file_path}")
                                            break
                            except Exception:
                                pass
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ document_id –≤ {base_path}: {e}")
        
        # 4. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ document_id
        if chromadb_chunks_deleted == 0 and simple_rag_chunks_deleted == 0 and len(files_deleted) == 0:
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ document_id
            # document_id –º–æ–∂–µ—Ç –±—ã—Ç—å: "–¢—Ä—É–¥–æ–≤–æ–π_–∫–æ–¥–µ–∫—Å_–†–§", "–¢—Ä—É–¥–æ–≤–æ–π_–∫–æ–¥–µ–∫—Å_–†–§.txt", UUID –∏ —Ç.–¥.
            logger.info(f"üîç –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º –º–µ—Ç–æ–¥–∞–º: {document_id}")
            
            # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª, –µ—Å–ª–∏ document_id –ø–æ—Ö–æ–∂ –Ω–∞ –∏–º—è —Ñ–∞–π–ª–∞
            possible_names = [
                document_id,
                f"{document_id}.txt",
                f"{document_id}.pdf",
                document_id.replace("_", " "),
                document_id.replace(" ", "_")
            ]
            
            for possible_name in possible_names:
                for base_path in possible_paths:
                    try:
                        base_dir = Path(base_path)
                        if base_dir.exists():
                            for file_path in base_dir.rglob(possible_name):
                                if file_path.exists() and file_path.is_file():
                                    file_path.unlink()
                                    files_deleted.append(str(file_path))
                                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –ø–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–º—É –ø–æ–∏—Å–∫—É: {file_path}")
                                    if not filename_to_delete:
                                        filename_to_delete = file_path.name
                                    json_path = file_path.with_suffix('.json')
                                    if json_path.exists():
                                        json_path.unlink()
                                        files_deleted.append(str(json_path))
                                    break
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ {base_path}: {e}")
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –Ω–æ —Ñ–∞–π–ª –±—ã–ª —É–¥–∞–ª–µ–Ω - —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º
            if len(files_deleted) > 0:
                logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –∏ —É–¥–∞–ª–µ–Ω —á–µ—Ä–µ–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫")
                chromadb_chunks_deleted = 1  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ
            elif chromadb_chunks_deleted == 0 and simple_rag_chunks_deleted == 0 and len(files_deleted) == 0:
                logger.warning(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ ChromaDB, –Ω–∏ –≤ simple_expert_rag, –Ω–∏ –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ")
                # –ù–ï –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É - –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É–¥–∞–ª–µ–Ω–Ω—ã–π –≤ –∫—ç—à–µ
                logger.info(f"üìù –ü–æ–º–µ—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ —É–¥–∞–ª–µ–Ω–Ω—ã–π –≤ –∫—ç—à–µ (–≤–æ–∑–º–æ–∂–Ω–æ, —É–∂–µ –±—ã–ª —É–¥–∞–ª–µ–Ω)")
        
        # 5. –ü–æ–º–µ—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ —É–¥–∞–ª–µ–Ω–Ω—ã–π –≤ –∫—ç—à–µ (–≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω)
        try:
            from ..services.deleted_documents_cache import deleted_documents_cache
            # –ü–æ–º–µ—á–∞–µ–º –∏ –ø–æ ID, –∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            deleted_documents_cache.mark_deleted(document_id=document_id, filename=filename_to_delete)
            # –¢–∞–∫–∂–µ –ø–æ–º–µ—á–∞–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –µ—Å–ª–∏ —ç—Ç–æ –∫–æ–¥–µ–∫—Å
            if filename_to_delete:
                filename_stem = Path(filename_to_delete).stem
                if filename_stem != filename_to_delete:
                    deleted_documents_cache.mark_deleted(document_id=filename_stem)
            # –ï—Å–ª–∏ document_id –ø–æ—Ö–æ–∂ –Ω–∞ –∏–º—è —Ñ–∞–π–ª–∞, –ø–æ–º–µ—á–∞–µ–º –∏ –µ–≥–æ
            if not filename_to_delete and (document_id.endswith('.txt') or document_id.endswith('.pdf')):
                deleted_documents_cache.mark_deleted(filename=document_id)
                filename_stem = Path(document_id).stem
                deleted_documents_cache.mark_deleted(document_id=filename_stem)
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ —É–¥–∞–ª–µ–Ω–Ω—ã–π –≤ –∫—ç—à–µ: ID={document_id}, filename={filename_to_delete}")
        except Exception as cache_err:
            logger.warning(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫—ç—à —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {cache_err}")
        
        # 6. –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        try:
            audit_service = get_audit_service(db)
            audit_service.log_action(
                user_id=current_admin.id,
                action=ActionType.ADMIN_ACTION,
                resource="document",
                resource_id=document_id,
                description=f"Deleted document from RAG systems (ChromaDB: {chromadb_chunks_deleted}, Simple RAG: {simple_rag_chunks_deleted} chunks, Files: {len(files_deleted)})",
                severity=SeverityLevel.MEDIUM,
                details={
                    "chromadb_chunks_deleted": chromadb_chunks_deleted,
                    "simple_rag_chunks_deleted": simple_rag_chunks_deleted,
                    "files_deleted": files_deleted,
                    "simple_rag_success": simple_rag_result.get('success', False)
                }
            )
        except Exception as audit_err:
            logger.warning(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è: {audit_err}")
        
        # –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—Ö, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –±—ã–ª –Ω–∞–π–¥–µ–Ω (–≤–æ–∑–º–æ–∂–Ω–æ, —É–∂–µ –±—ã–ª —É–¥–∞–ª–µ–Ω)
        # –ì–ª–∞–≤–Ω–æ–µ - –æ–Ω –ø–æ–º–µ—á–µ–Ω –≤ –∫—ç—à–µ –∏ –Ω–µ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è
        return {
            "message": "Document deleted successfully from all RAG systems and file system",
            "chromadb_chunks_deleted": chromadb_chunks_deleted,
            "simple_rag_chunks_deleted": simple_rag_chunks_deleted,
            "total_chunks_deleted": chromadb_chunks_deleted + simple_rag_chunks_deleted,
            "files_deleted": files_deleted,
            "files_deleted_count": len(files_deleted),
            "simple_rag_status": simple_rag_result.get('status', 'unknown'),
            "document_id": document_id,
            "marked_in_cache": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
        logger.error(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
        import traceback
        logger.error(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è: {str(e)}")

@router.post("/documents/clear-all")
async def clear_all_documents(
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å simple_expert_rag)"""
    try:
        if not vector_store_service.is_ready():
            raise HTTPException(status_code=503, detail="Vector store not ready")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º simple_expert_rag
        from ..services.simple_expert_rag import simple_expert_rag
        
        # 1. –û—á–∏—â–∞–µ–º ChromaDB
        collection = vector_store_service.collection
        all_results = collection.get(include=['metadatas'])
        
        chromadb_chunks_deleted = len(all_results['ids'])
        if chromadb_chunks_deleted > 0:
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ ChromaDB
            collection.delete(ids=all_results['ids'])
            logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω–æ ChromaDB: {chromadb_chunks_deleted} —á–∞–Ω–∫–æ–≤")
        
        # 2. –û—á–∏—â–∞–µ–º simple_expert_rag
        simple_rag_result = await simple_expert_rag.clear_all_documents()
        simple_rag_documents_deleted = simple_rag_result.get('documents_deleted', 0)
        simple_rag_chunks_deleted = simple_rag_result.get('chunks_deleted', 0)
        
        if not simple_rag_result.get('success', False):
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ simple_expert_rag: {simple_rag_result.get('error', 'Unknown error')}")
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∏ –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ö–æ—Ç—è –±—ã –≤ –æ–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ
        if chromadb_chunks_deleted == 0 and simple_rag_chunks_deleted == 0:
            return {
                "message": "No documents to delete in any system",
                "chromadb_chunks_deleted": 0,
                "simple_rag_documents_deleted": 0,
                "simple_rag_chunks_deleted": 0,
                "total_chunks_deleted": 0
            }
        
        # 4. –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="documents",
            resource_id="all",
            description=f"Cleared all documents from RAG systems (ChromaDB: {chromadb_chunks_deleted}, Simple RAG: {simple_rag_documents_deleted} docs, {simple_rag_chunks_deleted} chunks)",
            severity=SeverityLevel.HIGH,
            details={
                "chromadb_chunks_deleted": chromadb_chunks_deleted,
                "simple_rag_documents_deleted": simple_rag_documents_deleted,
                "simple_rag_chunks_deleted": simple_rag_chunks_deleted,
                "simple_rag_success": simple_rag_result.get('success', False)
            }
        )
        
        return {
            "message": "All documents deleted successfully from all RAG systems",
            "chromadb_chunks_deleted": chromadb_chunks_deleted,
            "simple_rag_documents_deleted": simple_rag_documents_deleted,
            "simple_rag_chunks_deleted": simple_rag_chunks_deleted,
            "total_chunks_deleted": chromadb_chunks_deleted + simple_rag_chunks_deleted,
            "simple_rag_status": simple_rag_result.get('status', 'unknown')
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {str(e)}")

@router.post("/documents/cleanup-stubs")
async def cleanup_codex_stubs(
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –∫–æ–¥–µ–∫—Å–æ–≤ (–º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã <= 1000 –±–∞–π—Ç)"""
    try:
        if not vector_store_service.is_ready():
            raise HTTPException(status_code=503, detail="Vector store not ready")
        
        # –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ä—ã—Ö –∑–∞–≥–ª—É—à–µ–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        old_codexes = [
            "0001201412140001",  # –¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§
            "0001201412140002",  # –°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§
            "0001201412140003",  # –ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§
            "0001201412140004",  # –ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§
            "0001201412140005",  # –ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§
            "0001201412140006",  # –ö–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§
            "0001201410140002",  # –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§ (—á–∞—Å—Ç—å 1) - –∑–∞–≥–ª—É—à–∫–∞
            "0001201905010039",  # –ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§ (—á–∞—Å—Ç—å 1) - –∑–∞–≥–ª—É—à–∫–∞
            "0001202203030006",  # –£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§ - –∑–∞–≥–ª—É—à–∫–∞
            "198",
            "289-11",
        ]
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ ChromaDB
        collection = vector_store_service.collection
        all_docs = collection.get(include=['metadatas'])
        
        documents_to_delete = []
        deleted_count = 0
        total_chunks_deleted = 0
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º simple_expert_rag –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º
        from ..services.simple_expert_rag import simple_expert_rag
        
        for i, doc_id in enumerate(all_docs['ids']):
            metadata = all_docs['metadatas'][i] if all_docs['metadatas'] else {}
            filename = metadata.get('filename', '') or metadata.get('file_name', '') or str(doc_id)
            source_path = metadata.get('source_path', '')
            file_size = metadata.get('size', 0) or metadata.get('file_size', 0)
            
            should_delete = False
            reason = ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ ID –∏–ª–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            for old_id in old_codexes:
                if old_id in str(doc_id) or old_id in filename or old_id in source_path:
                    should_delete = True
                    reason = f"–°—Ç–∞—Ä–∞—è –∑–∞–≥–ª—É—à–∫–∞ (ID: {old_id})"
                    break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤
            if not should_delete and file_size <= 1000:
                title = metadata.get('title', '').lower() or metadata.get('name', '').lower()
                if any(kw in title for kw in ["–∫–æ–¥–µ–∫—Å", "codex"]):
                    should_delete = True
                    reason = f"–ú–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª –∫–æ–¥–µ–∫—Å–∞ ({file_size} –±–∞–π—Ç)"
            
            if should_delete:
                documents_to_delete.append({
                    "id": doc_id,
                    "filename": filename,
                    "reason": reason
                })
        
        # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc in documents_to_delete:
            try:
                # –£–¥–∞–ª—è–µ–º –∏–∑ ChromaDB
                collection.delete(ids=[doc['id']])
                chromadb_chunks = 1  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                
                # –£–¥–∞–ª—è–µ–º –∏–∑ simple_expert_rag
                simple_rag_result = await simple_expert_rag.delete_document(doc['id'])
                simple_rag_chunks = simple_rag_result.get('chunks_deleted', 0)
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ ID, –ø—Ä–æ–±—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if simple_rag_chunks == 0:
                    filename_result = await simple_expert_rag.delete_document(doc['filename'])
                    simple_rag_chunks = filename_result.get('chunks_deleted', 0)
                
                deleted_count += 1
                total_chunks_deleted += chromadb_chunks + simple_rag_chunks
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc['id']}: {e}")
                continue
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="documents",
            resource_id="cleanup_stubs",
            description=f"Cleaned up {deleted_count} codex stubs",
            severity=SeverityLevel.MEDIUM,
            details={
                "deleted_count": deleted_count,
                "total_chunks_deleted": total_chunks_deleted,
                "documents": [d['filename'] for d in documents_to_delete]
            }
        )
        
        return {
            "message": f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–≥–ª—É—à–µ–∫: {deleted_count}",
            "deleted_count": deleted_count,
            "total_chunks_deleted": total_chunks_deleted,
            "documents": documents_to_delete
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≥–ª—É—à–µ–∫: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {str(e)}")

@router.get("/documents/search")
async def search_documents(
    query: str = Query(..., min_length=1),
    limit: int = Query(10, ge=1, le=100),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º–µ"""
    try:
        if not vector_store_service.is_ready():
            raise HTTPException(status_code=503, detail="Vector store not ready")
        
        collection = vector_store_service.collection
        results = collection.query(
            query_texts=[query],
            n_results=limit,
            include=['documents', 'metadatas', 'distances']
        )
        
        documents = []
        for i, (doc, meta, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            documents.append({
                "id": results['ids'][0][i],
                "content": doc,
                "metadata": meta,
                "similarity": 1 - distance,
                "distance": distance
            })
        
        return {
            "query": query,
            "documents": documents,
            "total_found": len(documents)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== –õ–û–ì–ò –ò –ê–£–î–ò–¢ ====================

@router.get("/logs/audit")
async def get_audit_logs(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    action: Optional[ActionType] = None,
    severity: Optional[SeverityLevel] = None,
    user_id: Optional[int] = None,
    days: int = Query(30, ge=1, le=365),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ –∞—É–¥–∏—Ç–∞"""
    try:
        query = db.query(AuditLog)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        start_date = datetime.utcnow() - timedelta(days=days)
        query = query.filter(AuditLog.created_at >= start_date)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        if action:
            query = query.filter(AuditLog.action == action)
        if severity:
            query = query.filter(AuditLog.severity == severity)
        if user_id:
            query = query.filter(AuditLog.user_id == user_id)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        query = query.order_by(AuditLog.created_at.desc())
        
        logs = query.offset(skip).limit(limit).all()
        
        return {
            "logs": [
                {
                    "id": log.id,
                    "user_id": log.user_id,
                    "action": log.action,
                    "resource": log.resource,
                    "resource_id": log.resource_id,
                    "description": log.description,
                    "severity": log.severity,
                    "ip_address": log.ip_address,
                    "created_at": log.created_at.isoformat(),
                    "details": log.details
                }
                for log in logs
            ],
            "total": query.count(),
            "skip": skip,
            "limit": limit
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ª–æ–≥–æ–≤ –∞—É–¥–∏—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/logs/export")
async def export_audit_logs(
    format: str = Query("json", pattern="^(json|csv)$"),
    days: int = Query(30, ge=1, le=365),
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–≠–∫—Å–ø–æ—Ä—Ç –ª–æ–≥–æ–≤ –∞—É–¥–∏—Ç–∞"""
    try:
        start_date = datetime.utcnow() - timedelta(days=days)
        logs = db.query(AuditLog).filter(
            AuditLog.created_at >= start_date
        ).order_by(AuditLog.created_at.desc()).all()
        
        if format == "json":
            data = [
                {
                    "id": log.id,
                    "user_id": log.user_id,
                    "action": log.action,
                    "resource": log.resource,
                    "resource_id": log.resource_id,
                    "description": log.description,
                    "severity": log.severity,
                    "ip_address": log.ip_address,
                    "created_at": log.created_at.isoformat(),
                    "details": log.details
                }
                for log in logs
            ]
            
            return StreamingResponse(
                io.StringIO(json.dumps(data, indent=2, ensure_ascii=False)),
                media_type="application/json",
                headers={"Content-Disposition": f"attachment; filename=audit_logs_{datetime.now().strftime('%Y%m%d')}.json"}
            )
        
        elif format == "csv":
            import csv
            
            output = io.StringIO()
            writer = csv.writer(output)
            
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            writer.writerow([
                "ID", "User ID", "Action", "Resource", "Resource ID",
                "Description", "Severity", "IP Address", "Created At", "Details"
            ])
            
            # –î–∞–Ω–Ω—ã–µ
            for log in logs:
                writer.writerow([
                    log.id, log.user_id, log.action, log.resource, log.resource_id,
                    log.description, log.severity, log.ip_address,
                    log.created_at.isoformat(), json.dumps(log.details) if log.details else ""
                ])
            
            return StreamingResponse(
                io.BytesIO(output.getvalue().encode('utf-8')),
                media_type="text/csv",
                headers={"Content-Disposition": f"attachment; filename=audit_logs_{datetime.now().strftime('%Y%m%d')}.csv"}
            )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –ª–æ–≥–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== –°–ò–°–¢–ï–ú–ê ====================

@router.get("/system/status")
async def get_system_status(
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã"""
    try:
        return {
            "rag_system": rag_service.get_status(),
            "vector_store": vector_store_service.get_status(),
            "embeddings": embeddings_service.get_status(),
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/system/rag/reinitialize")
async def reinitialize_rag_system(
    current_admin: User = Depends(get_current_admin)
):
    """–ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG —Å–∏—Å—Ç–µ–º—É"""
    try:
        # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        vector_store_service.initialize()
        embeddings_service.load_model()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="system",
            resource_id="rag",
            description="Reinitialized RAG system",
            severity=SeverityLevel.HIGH
        )
        
        return {"message": "RAG system reinitialized successfully"}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/documents/reprocess")
async def reprocess_documents(
    current_admin: User = Depends(get_current_admin),
    db: Session = Depends(get_db)
):
    """–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    try:
        if not vector_store_service.is_ready():
            raise HTTPException(status_code=400, detail="Vector store not ready")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        collection = vector_store_service.collection
        total_docs = collection.count()
        
        if total_docs == 0:
            return {"message": "No documents to reprocess", "processed": 0}
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        results = collection.get(
            limit=total_docs,
            include=['metadatas', 'documents']
        )
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ document_id
        documents_by_id = {}
        for i, (doc, meta) in enumerate(zip(results['documents'], results['metadatas'])):
            doc_id = meta.get('document_id', results['ids'][i])
            if doc_id not in documents_by_id:
                documents_by_id[doc_id] = {
                    "id": doc_id,
                    "metadata": meta,
                    "content": doc
                }
        
        processed_count = 0
        errors = []
        
        # –ü–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for doc_id, doc_data in documents_by_id.items():
            try:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as tmp_file:
                    tmp_file.write(doc_data['content'])
                    tmp_file_path = tmp_file.name
                
                try:
                    # –ü–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –Ω–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                    result = await document_service.process_file(tmp_file_path, doc_data['metadata'])
                    if result.get('success'):
                        processed_count += 1
                    else:
                        errors.append(f"Document {doc_id}: {result.get('error', 'Unknown error')}")
                finally:
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
                        
            except Exception as e:
                errors.append(f"Document {doc_id}: {str(e)}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service(db)
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.ADMIN_ACTION,
            resource="documents",
            resource_id="reprocess",
            description=f"Reprocessed {processed_count} documents",
            severity=SeverityLevel.MEDIUM
        )
        
        return {
            "message": f"Reprocessed {processed_count} documents",
            "processed": processed_count,
            "total": len(documents_by_id),
            "errors": errors
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== –£–ü–†–ê–í–õ–ï–ù–ò–ï –ö–≠–®–ï–ú ====================

@router.get("/cache/status")
async def get_cache_status(
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫—ç—à–∞"""
    try:
        health_status = await cache_service.health_check()
        stats = await cache_service.get_stats()
        
        return {
            "status": "success",
            "cache_health": health_status,
            "cache_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting cache status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/cache/reconnect")
async def reconnect_cache(
    current_admin: User = Depends(get_current_admin)
):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis"""
    try:
        success = await cache_service.reconnect()
        
        if success:
            return {
                "status": "success",
                "message": "Redis reconnection successful",
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "status": "warning",
                "message": "Redis reconnection failed, using in-memory cache",
                "timestamp": datetime.now().isoformat()
            }
    except Exception as e:
        logger.error(f"Error reconnecting cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/cache/clear")
@require_admin_action_validation("clear_cache", "system")
async def clear_cache(
    pattern: Optional[str] = Query(None, description="–ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: user_profile:*)"),
    current_admin: User = Depends(get_current_admin),
    request: Request = None
):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É –∏–ª–∏ –ø–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞"""
    try:
        if pattern:
            cleared_count = await cache_service.clear_pattern(pattern)
            message = f"Cleared {cleared_count} keys matching pattern '{pattern}'"
        else:
            # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            await cache_service.in_memory_cache.clear()
            if cache_service.redis_client and cache_service._initialized:
                await cache_service.redis_client.flushdb()
            message = "All cache cleared"
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        audit_service = get_audit_service()
        audit_service.log_action(
            user_id=current_admin.id,
            action=ActionType.SYSTEM_MAINTENANCE,
            resource="cache",
            resource_id=None,
            description=f"Cache cleared by admin: {message}",
            severity=SeverityLevel.MEDIUM
        )
        
        return {
            "status": "success",
            "message": message,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/cache/keys")
async def get_cache_keys(
    pattern: str = Query("*", description="–ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–π"),
    limit: int = Query(100, ge=1, le=1000, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π"),
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–ª—é—á–µ–π –∫—ç—à–∞"""
    try:
        keys = []
        
        if cache_service.redis_client and cache_service._initialized:
            try:
                redis_keys = await cache_service.redis_client.keys(pattern)
                keys.extend(redis_keys[:limit])
            except Exception as e:
                logger.warning(f"Error getting Redis keys: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–∏ –∏–∑ in-memory –∫—ç—à–∞
        in_memory_keys = list(cache_service.in_memory_cache._cache.keys())
        if pattern != "*":
            # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É –¥–ª—è in-memory –∫—ç—à–∞
            import fnmatch
            in_memory_keys = [k for k in in_memory_keys if fnmatch.fnmatch(k, pattern)]
        
        keys.extend(in_memory_keys[:limit - len(keys)])
        
        return {
            "status": "success",
            "keys": keys[:limit],
            "total_found": len(keys),
            "pattern": pattern,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting cache keys: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/cache/key/{key}")
async def get_cache_value(
    key: str,
    current_admin: User = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á—É –∏–∑ –∫—ç—à–∞"""
    try:
        value = await cache_service.get(key)
        
        return {
            "status": "success",
            "key": key,
            "value": value,
            "exists": value is not None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting cache value for key '{key}': {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/cache/key/{key}")
async def delete_cache_key(
    key: str,
    current_admin: User = Depends(get_current_admin)
):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∫–ª—é—á–∞ –∏–∑ –∫—ç—à–∞"""
    try:
        success = await cache_service.delete(key)
        
        return {
            "status": "success",
            "key": key,
            "deleted": success,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error deleting cache key '{key}': {e}")
        raise HTTPException(status_code=500, detail=str(e))