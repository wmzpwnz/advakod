#!/usr/bin/env python3
"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π
"""

import json
import os
import re
from datetime import datetime
from pathlib import Path

class RAGIntegrationService:
    """–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π"""
    
    def __init__(self, output_dir="/root/advakod/rag_integration"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è RAG
        (self.output_dir / "processed_documents").mkdir(exist_ok=True)
        (self.output_dir / "chunks").mkdir(exist_ok=True)
        (self.output_dir / "embeddings").mkdir(exist_ok=True)
        (self.output_dir / "metadata").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
        self.chunk_size = 1000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        self.chunk_overlap = 200  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        # –°—á–µ—Ç—á–∏–∫–∏
        self.processed_documents = 0
        self.total_chunks = 0
        self.integration_results = []
        
        print(f"‚úÖ RAGIntegrationService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.output_dir}")
    
    def log(self, message, level="INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        log_file = self.output_dir / "logs" / f"integration_{datetime.now().strftime('%Y%m%d')}.log"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + "\n")
    
    def extract_text_from_html(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML"""
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', ' ', html_content)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–æ–≤—ã–µ
        text = re.sub(r'[^\w\s\-\.\,\:\;\(\)\[\]\"\'‚Ññ]', ' ', text)
        
        return text.strip()
    
    def clean_text(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è RAG"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s\-\.\,\:\;\(\)\[\]\"\'‚Ññ]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
        lines = text.split('\n')
        cleaned_lines = [line.strip() for line in lines if len(line.strip()) > 10]
        
        return ' '.join(cleaned_lines)
    
    def create_chunks(self, text, document_id, metadata=None):
        """–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text) < self.chunk_size:
            return [{
                'id': f"{document_id}_chunk_0",
                'text': text,
                'metadata': metadata or {},
                'chunk_index': 0,
                'total_chunks': 1
            }]
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(text):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —á–∞–Ω–∫–∞
            end = start + self.chunk_size
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–Ω—á–∏—Ç—å –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(text):
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                for i in range(end, max(start + self.chunk_size // 2, end - 100), -1):
                    if text[i] in '.!?':
                        end = i + 1
                        break
            
            chunk_text = text[start:end].strip()
            
            if chunk_text:
                chunk = {
                    'id': f"{document_id}_chunk_{chunk_index}",
                    'text': chunk_text,
                    'metadata': {
                        **(metadata or {}),
                        'chunk_index': chunk_index,
                        'start_position': start,
                        'end_position': end,
                        'chunk_length': len(chunk_text)
                    },
                    'chunk_index': chunk_index,
                    'total_chunks': 0  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                }
                chunks.append(chunk)
                chunk_index += 1
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º—Å—è —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            start = end - self.chunk_overlap
            if start >= len(text):
                break
        
        # –û–±–Ω–æ–≤–ª—è–µ–º total_chunks –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        for chunk in chunks:
            chunk['total_chunks'] = len(chunks)
            chunk['metadata']['total_chunks'] = len(chunks)
        
        return chunks
    
    def process_html_document(self, file_path, validation_result=None):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç HTML –¥–æ–∫—É–º–µ–Ω—Ç"""
        self.log(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                html_content = f.read()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = self.extract_text_from_html(html_content)
            text = self.clean_text(text)
            
            if not text or len(text) < 50:
                self.log(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞", "WARNING")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'source': 'pravo.gov.ru',
                'file_type': 'html',
                'file_name': file_path.name,
                'file_path': str(file_path),
                'processing_timestamp': datetime.now().isoformat(),
                'text_length': len(text)
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if validation_result:
                metadata.update({
                    'legal_score': validation_result.get('legal_score', 0),
                    'document_type': validation_result.get('document_type', 'unknown'),
                    'is_valid': validation_result.get('is_valid', False)
                })
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            document_id = f"doc_{self.processed_documents}_{file_path.stem}"
            chunks = self.create_chunks(text, document_id, metadata)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            processed_file = self.output_dir / "processed_documents" / f"{document_id}.json"
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'document_id': document_id,
                    'original_file': str(file_path),
                    'text': text,
                    'metadata': metadata,
                    'chunks': chunks
                }, f, ensure_ascii=False, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
            for chunk in chunks:
                chunk_file = self.output_dir / "chunks" / f"{chunk['id']}.json"
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=2)
            
            self.processed_documents += 1
            self.total_chunks += len(chunks)
            
            self.log(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç {file_path.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            return {
                'document_id': document_id,
                'chunks_count': len(chunks),
                'text_length': len(text),
                'processed_file': str(processed_file)
            }
            
        except Exception as e:
            self.log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path.name}: {e}", "ERROR")
            return None
    
    def process_text_document(self, file_path, validation_result=None):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (.txt)"""
        self.log(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            text = self.clean_text(text)
            
            if not text or len(text) < 50:
                self.log(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞", "WARNING")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'source': 'pravo.gov.ru',
                'file_type': 'txt',
                'file_name': file_path.name,
                'file_path': str(file_path),
                'processing_timestamp': datetime.now().isoformat(),
                'text_length': len(text),
                'source_type': 'file',
                'document_id': f"codex_{file_path.stem}"
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if validation_result:
                metadata.update({
                    'legal_score': validation_result.get('legal_score', 0),
                    'document_type': validation_result.get('document_type', 'unknown'),
                    'is_valid': validation_result.get('is_valid', False)
                })
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            document_id = f"codex_{self.processed_documents}_{file_path.stem}"
            chunks = self.create_chunks(text, document_id, metadata)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            processed_file = self.output_dir / "processed_documents" / f"{document_id}.json"
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'document_id': document_id,
                    'original_file': str(file_path),
                    'text': text,
                    'metadata': metadata,
                    'chunks': chunks
                }, f, ensure_ascii=False, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
            for chunk in chunks:
                chunk_file = self.output_dir / "chunks" / f"{chunk['id']}.json"
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=2)
            
            self.processed_documents += 1
            self.total_chunks += len(chunks)
            
            self.log(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç {file_path.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            return {
                'document_id': document_id,
                'chunks_count': len(chunks),
                'text_length': len(text),
                'processed_file': str(processed_file)
            }
            
        except Exception as e:
            self.log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path.name}: {e}", "ERROR")
            return None
    
    def process_pdf_document(self, file_path, validation_result=None):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –¥–æ–∫—É–º–µ–Ω—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        self.log(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
        
        try:
            # –î–ª—è PDF –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            with open(file_path, 'rb') as f:
                content = f.read()
            
            # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç
            try:
                text = content.decode('utf-8', errors='ignore')
                # –û—á–∏—â–∞–µ–º –æ—Ç –±–∏–Ω–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                text = re.sub(r'[^\x20-\x7E\u0400-\u04FF]', ' ', text)
                text = self.clean_text(text)
            except:
                text = ""
            
            if not text or len(text) < 50:
                self.log(f"‚ö†Ô∏è PDF {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞", "WARNING")
                # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è PDF
                text = f"PDF –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}\n–ò—Å—Ç–æ—á–Ω–∏–∫: pravo.gov.ru\n–¢–∏–ø: –ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç"
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'source': 'pravo.gov.ru',
                'file_type': 'pdf',
                'file_name': file_path.name,
                'file_path': str(file_path),
                'processing_timestamp': datetime.now().isoformat(),
                'text_length': len(text),
                'file_size': file_path.stat().st_size
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if validation_result:
                metadata.update({
                    'legal_score': validation_result.get('legal_score', 0),
                    'document_type': validation_result.get('document_type', 'unknown'),
                    'is_valid': validation_result.get('is_valid', False)
                })
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            document_id = f"pdf_{self.processed_documents}_{file_path.stem}"
            chunks = self.create_chunks(text, document_id, metadata)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            processed_file = self.output_dir / "processed_documents" / f"{document_id}.json"
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'document_id': document_id,
                    'original_file': str(file_path),
                    'text': text,
                    'metadata': metadata,
                    'chunks': chunks
                }, f, ensure_ascii=False, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
            for chunk in chunks:
                chunk_file = self.output_dir / "chunks" / f"{chunk['id']}.json"
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=2)
            
            self.processed_documents += 1
            self.total_chunks += len(chunks)
            
            self.log(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω PDF {file_path.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            return {
                'document_id': document_id,
                'chunks_count': len(chunks),
                'text_length': len(text),
                'processed_file': str(processed_file)
            }
            
        except Exception as e:
            self.log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {file_path.name}: {e}", "ERROR")
            return None
    
    def integrate_documents(self, documents_dir, validation_results=None):
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        documents_dir = Path(documents_dir)
        
        if not documents_dir.exists():
            self.log(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_dir}", "ERROR")
            return []
        
        self.log(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑: {documents_dir}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        validation_dict = {}
        if validation_results:
            for result in validation_results:
                file_path = Path(result['file_path'])
                validation_dict[file_path.name] = result
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
        html_files = list(documents_dir.glob("**/*.html"))
        pdf_files = list(documents_dir.glob("**/*.pdf"))
        txt_files = list(documents_dir.glob("**/*.txt"))  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É .txt —Ñ–∞–π–ª–æ–≤
        
        all_files = html_files + pdf_files + txt_files
        self.log(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {len(all_files)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        for i, file_path in enumerate(all_files, 1):
            self.log(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª {i}/{len(all_files)}: {file_path.name}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            validation_result = validation_dict.get(file_path.name)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
            if file_path.suffix.lower() == '.html':
                result = self.process_html_document(file_path, validation_result)
            elif file_path.suffix.lower() == '.pdf':
                result = self.process_pdf_document(file_path, validation_result)
            elif file_path.suffix.lower() == '.txt':
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .txt —Ñ–∞–π–ª—ã –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                result = self.process_text_document(file_path, validation_result)
            else:
                self.log(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_path.name}", "WARNING")
                continue
            
            if result:
                self.integration_results.append(result)
        
        return self.integration_results
    
    def create_rag_metadata(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        metadata = {
            'integration_timestamp': datetime.now().isoformat(),
            'total_documents': self.processed_documents,
            'total_chunks': self.total_chunks,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'source': 'pravo.gov.ru',
            'integration_results': self.integration_results
        }
        
        metadata_file = self.output_dir / "metadata" / "rag_integration_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        self.log(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ RAG —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_file}")
        return metadata
    
    def save_integration_report(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ–± –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_documents_processed': self.processed_documents,
                'total_chunks_created': self.total_chunks,
                'successful_integrations': len(self.integration_results),
                'failed_integrations': self.processed_documents - len(self.integration_results)
            },
            'integration_results': self.integration_results,
            'output_directory': str(self.output_dir)
        }
        
        report_file = self.output_dir / "metadata" / "integration_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.log(f"üìä –û—Ç—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å RAG")
    print("=" * 50)
    
    integration_service = RAGIntegrationService()
    
    # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    downloaded_dir = "/root/advakod/downloaded_documents"
    if not Path(downloaded_dir).exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {downloaded_dir}")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    validation_results = None
    validation_report_file = "/root/advakod/validation_results/reports/validation_report_20251026_015020.json"
    if Path(validation_report_file).exists():
        with open(validation_report_file, 'r', encoding='utf-8') as f:
            validation_data = json.load(f)
            validation_results = validation_data.get('detailed_results', [])
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(validation_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    results = integration_service.integrate_documents(downloaded_dir, validation_results)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç—á–µ—Ç
    integration_service.create_rag_metadata()
    report = integration_service.save_integration_report()
    
    if report:
        print(f"\nüìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        print(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {report['summary']['total_documents_processed']}")
        print(f"   üîó –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {report['summary']['total_chunks_created']}")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π: {report['summary']['successful_integrations']}")
        print(f"   ‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π: {report['summary']['failed_integrations']}")
        print(f"   üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {report['output_directory']}")
        
        if results:
            print(f"\nüìã –î–ï–¢–ê–õ–ò –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
            for result in results:
                print(f"   ‚Ä¢ {result['document_id']}: {result['chunks_count']} —á–∞–Ω–∫–æ–≤, {result['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print(f"\nüéâ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAG –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()
